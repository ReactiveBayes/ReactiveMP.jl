{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO update nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reactive programming package for Julia\r\n",
    "using Rocket \r\n",
    "# Core package for Constrained Bethe Free Energy minimsation with Factor graphs and message passing\r\n",
    "using ReactiveMP \r\n",
    "# High-level user friendly probabilistic model and constraints specification language package for ReactiveMP\r\n",
    "using GraphPPL\r\n",
    "# Optionally include the Distributions.jl package and the Random package from Base\r\n",
    "using Distributions, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers the fundamentals of the ReactiveMP.jl package. For a more advanced usage we refer the interested reader to the documentation.\n",
    "\n",
    "This tutorial is also available in the [documentation](https://biaslab.github.io/ReactiveMP.jl/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General model specification syntax\r\n",
    "\r\n",
    "We use the `@model` macro from the `GraphPPL.jl` package to create a probabilistic model $p(s, y)$ and we also specify extra constraints on the variational family of distributions $\\mathcal{Q}$, used for approximating intractable posterior distributions.\r\n",
    "Below there is a simple example of the general syntax for model specification. In this tutorial we do not cover all possible ways to create models or advanced features of `GraphPPL.jl`.  Instead we refer the interested reader to the documentation for a more rigorous explanation and illustrative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_model1 (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the `@model` macro accepts a regular Julia function\r\n",
    "@model function test_model1(s_mean, s_precision)\r\n",
    "    \r\n",
    "    # We use the `randomvar` function to create \r\n",
    "    # a random variable in our model\r\n",
    "    s = randomvar()\r\n",
    "    \r\n",
    "    # the `tilde` operator creates a functional dependency\r\n",
    "    # between variables in our model and can be read as \r\n",
    "    # `sampled from` or `is modeled by`\r\n",
    "    s ~ GaussianMeanPrecision(s_mean, s_precision)\r\n",
    "    \r\n",
    "    # We use the `datavar` function to create \r\n",
    "    # observed data variables in our models\r\n",
    "    # We also need to specify the type of our data \r\n",
    "    # In this example it is `Float64`\r\n",
    "    y = datavar(Float64)\r\n",
    "    \r\n",
    "    y ~ GaussianMeanPrecision(s, 1.0)\r\n",
    "    \r\n",
    "    return s, y\r\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@model` macro creates a function with the same name and with the same set of input arguments as the original function (`test_model1(s_mean, s_precision)` in this example). However, the return value is modified in such a way to contain a reference to the model object as the first value and to the user specified variables in the form of a tuple as the second value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, (s, y) = test_model1(0.0, 1.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on we can examine our model structure with the help of some utility functions such as: \n",
    "- `getnodes()`: returns an array of factor nodes in a correposning factor graph\n",
    "- `getrandom()`: returns an array of random variable in the model\n",
    "- `getdata()`: returns an array of data inputs in the model\n",
    "- `getconstant()`: return an array of constant values in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{ReactiveMP.AbstractFactorNode}:\n",
       " FactorNode:\n",
       " form            : NormalMeanPrecision\n",
       " sdtype          : Stochastic()\n",
       " interfaces      : (Interface(out, Marginalisation()), Interface(μ, Marginalisation()), Interface(τ, Marginalisation()))\n",
       " factorisation   : ((1, 2, 3),)\n",
       " local marginals : (:out_μ_τ,)\n",
       " metadata        : nothing\n",
       " pipeline        : FactorNodePipeline(functional_dependencies = DefaultFunctionalDependencies(), extra_stages = EmptyPipelineStage()\n",
       "\n",
       " FactorNode:\n",
       " form            : NormalMeanPrecision\n",
       " sdtype          : Stochastic()\n",
       " interfaces      : (Interface(out, Marginalisation()), Interface(μ, Marginalisation()), Interface(τ, Marginalisation()))\n",
       " factorisation   : ((1, 2, 3),)\n",
       " local marginals : (:out_μ_τ,)\n",
       " metadata        : nothing\n",
       " pipeline        : FactorNodePipeline(functional_dependencies = DefaultFunctionalDependencies(), extra_stages = EmptyPipelineStage()\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getnodes(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Symbol}:\n",
       " :s"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getrandom(model) .|> name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Symbol}:\n",
       " :y"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getdata(model) .|> name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " 0.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getconstant(model) .|> getconst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use control flow statements such as `if` or `for` blocks in the model specification function. In general, any valid snippet of Julia code can be used inside the `@model` block. As an example consider the following (valid!) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_model2 (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function test_model2(n)\r\n",
    "    \r\n",
    "    if n <= 1\r\n",
    "        error(\"`n` argument must be greater than one.\")\r\n",
    "    end\r\n",
    "    \r\n",
    "    # `randomvar(n)` creates a dense sequence of \r\n",
    "    # random variables\r\n",
    "    s = randomvar(n)\r\n",
    "    \r\n",
    "    # `datavar(Float64, n)` creates a dense sequence of \r\n",
    "    # observed data variables of type `Float64`\r\n",
    "    y = datavar(Float64, n)\r\n",
    "    \r\n",
    "    s[1] ~ GaussianMeanPrecision(0.0, 0.1)\r\n",
    "    y[1] ~ GaussianMeanPrecision(s[1], 1.0)\r\n",
    "    \r\n",
    "    for i in 2:n\r\n",
    "        s[i] ~ GaussianMeanPrecision(s[i - 1], 1.0)\r\n",
    "        y[i] ~ GaussianMeanPrecision(s[i], 1.0)\r\n",
    "    end\r\n",
    "    \r\n",
    "    return s, y\r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, (s, y) = test_model2(10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An amount of factor nodes in generated Factor Graph\r\n",
    "getnodes(model) |> length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An amount of random variables\r\n",
    "getrandom(model) |> length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An amount of data inputs\r\n",
    "getdata(model) |> length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An amount of constant values\r\n",
    "getconstant(model) |> length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use complex expression inside the functional dependency expressions\r\n",
    "\r\n",
    "```julia\r\n",
    "y ~ NormalMeanPrecision(2.0 * (s + 1.0), 1.0)\r\n",
    "```\r\n",
    "\r\n",
    "The `~` operator automatically creates a random variable if none was created before with the same name and throws an error if this name already exists\r\n",
    "\r\n",
    "```julia\r\n",
    "# s = randomvar() here is optional\r\n",
    "# `~` creates random variables automatically\r\n",
    "s ~ NormalMeanPrecision(0.0, 1.0)\r\n",
    "```\r\n",
    "\r\n",
    "An example model which will throw an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: Invalid name 's' for new random variable. 's' was already initialized with '=' operator before.\nin expression starting at /Users/bvdmitri/.julia/dev/GraphPPL/src/GraphPPL.jl:161",
     "output_type": "error",
     "traceback": [
      "LoadError: Invalid name 's' for new random variable. 's' was already initialized with '=' operator before.\nin expression starting at /Users/bvdmitri/.julia/dev/GraphPPL/src/GraphPPL.jl:161",
      "",
      "Stacktrace:",
      "  [1] error(s::String)",
      "    @ Base ./error.jl:33",
      "  [2] (::GraphPPL.var\"#41#54\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol})(expression::Expr)",
      "    @ GraphPPL ~/.julia/dev/GraphPPL/src/GraphPPL.jl:302",
      "  [3] walk",
      "    @ ~/.julia/packages/MacroTools/HDyRe/src/utils.jl:112 [inlined]",
      "  [4] postwalk",
      "    @ ~/.julia/packages/MacroTools/HDyRe/src/utils.jl:122 [inlined]",
      "  [5] (::MacroTools.var\"#19#20\"{GraphPPL.var\"#41#54\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol}})(x::Expr)",
      "    @ MacroTools ~/.julia/packages/MacroTools/HDyRe/src/utils.jl:122",
      "  [6] iterate",
      "    @ ./generator.jl:47 [inlined]",
      "  [7] collect_to!(dest::Vector{Any}, itr::Base.Generator{Vector{Any}, MacroTools.var\"#19#20\"{GraphPPL.var\"#41#54\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol}}}, offs::Int64, st::Int64)",
      "    @ Base ./array.jl:728",
      "  [8] collect_to!(dest::Vector{LineNumberNode}, itr::Base.Generator{Vector{Any}, MacroTools.var\"#19#20\"{GraphPPL.var\"#41#54\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol}}}, offs::Int64, st::Int64)",
      "    @ Base ./array.jl:736",
      "  [9] collect_to_with_first!(dest::Vector{LineNumberNode}, v1::LineNumberNode, itr::Base.Generator{Vector{Any}, MacroTools.var\"#19#20\"{GraphPPL.var\"#41#54\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol}}}, st::Int64)",
      "    @ Base ./array.jl:706",
      " [10] _collect(c::Vector{Any}, itr::Base.Generator{Vector{Any}, MacroTools.var\"#19#20\"{GraphPPL.var\"#41#54\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol}}}, #unused#::Base.EltypeUnknown, isz::Base.HasShape{1})",
      "    @ Base ./array.jl:700",
      " [11] collect_similar(cont::Vector{Any}, itr::Base.Generator{Vector{Any}, MacroTools.var\"#19#20\"{GraphPPL.var\"#41#54\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol}}})",
      "    @ Base ./array.jl:606",
      " [12] map(f::Function, A::Vector{Any})",
      "    @ Base ./abstractarray.jl:2294",
      " [13] walk(x::Expr, inner::Function, outer::GraphPPL.var\"#41#54\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol})",
      "    @ MacroTools ~/.julia/packages/MacroTools/HDyRe/src/utils.jl:112",
      " [14] postwalk(f::Function, x::Expr)",
      "    @ MacroTools ~/.julia/packages/MacroTools/HDyRe/src/utils.jl:122",
      " [15] generate_model_expression(backend::ReactiveMPBackend, model_options::Expr, model_specification::Expr)",
      "    @ GraphPPL ~/.julia/dev/GraphPPL/src/GraphPPL.jl:272",
      " [16] var\"@model\"(__source__::LineNumberNode, __module__::Module, model_options::Any, model_specification::Any)",
      "    @ GraphPPL ~/.julia/dev/GraphPPL/src/GraphPPL.jl:165",
      " [17] eval",
      "    @ ./boot.jl:360 [inlined]",
      " [18] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "@model function error_model1()\r\n",
    "    s = 1.0\r\n",
    "    s ~ NormalMeanPrecision(0.0, 1.0)\r\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the `GraphPPL.jl` package creates new references for constants (literals like `0.0` or `1.0`) in a model. In some situations this may not be efficient, especially when these constants represent large matrices. `GraphPPL.jl` will by default create new copies of some constant (e.g. matrix) in a model every time it uses it. However it is possible to use `constvar()` function to create and reuse similar constants in the model specification syntax as\r\n",
    "\r\n",
    "```julia\r\n",
    "# Creates constant reference in a model with a prespecified value\r\n",
    "c = constvar(0.0)\r\n",
    "```\r\n",
    "\r\n",
    "An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_model5 (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function test_model5(dim::Int, n::Int, A::Matrix, P::Matrix, Q::Matrix)\r\n",
    "    \r\n",
    "    s = randomvar(n)\r\n",
    "    \r\n",
    "    y = datavar(Vector{Float64}, n)\r\n",
    "    \r\n",
    "    # Here we create constant references\r\n",
    "    # for constant matrices in our model \r\n",
    "    # to make inference more memory efficient\r\n",
    "    cA = constvar(A)\r\n",
    "    cP = constvar(P)\r\n",
    "    cQ = constvar(Q)\r\n",
    "    \r\n",
    "    s[1] ~ MvGaussianMeanCovariance(zeros(dim), cP)\r\n",
    "    y[1] ~ MvGaussianMeanCovariance(s[1], cQ)\r\n",
    "    \r\n",
    "    for i in 2:n\r\n",
    "        s[i] ~ MvGaussianMeanCovariance(cA * s[i - 1], cP)\r\n",
    "        y[i] ~ MvGaussianMeanCovariance(s[i], cQ)\r\n",
    "    end\r\n",
    "    \r\n",
    "    return s, y\r\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `~` expression can also return a reference to a newly created node in a corresponding factor graph for convenience in later usage:\r\n",
    "\r\n",
    "```julia\r\n",
    "@model function test_model()\r\n",
    "\r\n",
    "    # In this example `ynode` refers to the corresponding \r\n",
    "    # `GaussianMeanVariance` node created in the factor graph\r\n",
    "    ynode, y ~ GaussianMeanVariance(0.0, 1.0)\r\n",
    "    \r\n",
    "    return ynode, y\r\n",
    "end\r\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic inference in ReactiveMP.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ReactiveMP.jl` uses the `Rocket.jl` package API for inference routines. `Rocket.jl` is a reactive programming extension for Julia that is higly inspired by `RxJS` and similar libraries from the `Rx` ecosystem. It consists of **observables**, **actors**, **subscriptions** and **operators**. For more infromation and rigorous examples see [Rocket.jl github page](https://github.com/biaslab/Rocket.jl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observables\r\n",
    "Observables are lazy push-based collections and they deliver their values over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimerObservable(1000, 1000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Timer that emits a new value every second and has an initial one second delay \r\n",
    "observable = timer(1000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subscription allows us to subscribe on future values of some observable, and actors specify what to do with these new values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimerSubscription()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor = (value) -> println(value)\r\n",
    "subscription1 = subscribe!(observable, actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We always need to unsubscribe from some observables\r\n",
    "unsubscribe!(subscription1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProxyObservable(Int64, MapProxy(Int64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can modify our observables\r\n",
    "modified = observable |> filter(d -> rem(d, 2) === 1) |> map(Int, d -> d ^ 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimerSubscription()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subscription2 = subscribe!(modified, (value) -> println(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsubscribe!(subscription2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ReactiveMP.jl` package returns posterior marginal distributions in our specified model in the form of an observable. It is possible to subscribe on its future updates, but for convenience `ReactiveMP.jl` only caches the last obtained values of all marginals in a model. To get a reference for the posterior marginal of some random variable in a model `ReactiveMP.jl` exports two functions: \r\n",
    "- `getmarginal(x)`: for a single random variable `x`\r\n",
    "- `getmarginals(xs)`: for a dense sequence of random variables `sx`\r\n",
    "\r\n",
    "Lets see how it works in practice. Here we create a simple coin toss model. We assume that observations are governed by the `Bernoulli` distribution with unknown bias parameter `θ`. To have a fully Bayesian treatment of this problem we endow `θ` with the `Beta` prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coin_toss_model (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function coin_toss_model(n)\r\n",
    "\r\n",
    "    # `datavar` creates data 'inputs' in our model\r\n",
    "    # We will pass data later on to these inputs\r\n",
    "    # In this example we create a sequence of inputs that accepts Float64\r\n",
    "    y = datavar(Float64, n)\r\n",
    "    \r\n",
    "    # We endow θ parameter of our model with some prior\r\n",
    "    θ ~ Beta(2.0, 7.0)\r\n",
    "    \r\n",
    "    # We assume that the outcome of each coin flip \r\n",
    "    # is modeled by a Bernoulli distribution\r\n",
    "    for i in 1:n\r\n",
    "        y[i] ~ Bernoulli(θ)\r\n",
    "    end\r\n",
    "    \r\n",
    "    # We return references to our data inputs and θ parameter\r\n",
    "    # We will use these references later on during the inference step\r\n",
    "    return y, θ\r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (y, θ) = coin_toss_model(500);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As soon as we have a new value for the marginal posterior over the `θ` variable\r\n",
    "# we simply print the first two statistics of it\r\n",
    "θ_subscription = subscribe!(getmarginal(θ), (marginal) -> println(\"New update: mean(θ) = \", mean(marginal), \", std(θ) = \", std(marginal)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets define our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.75 # Bias of a coin\r\n",
    "\r\n",
    "dataset = float.(rand(Bernoulli(p), 500));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass data to our model we use `update!` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New update: mean(θ) = 0.793713163064833, std(θ) = 0.01791770828759402\n"
     ]
    }
   ],
   "source": [
    "update!(y, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is necessary to always unsubscribe from running observables\r\n",
    "unsubscribe!(θ_subscription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ReactiveMP.jl inference backend is lazy and does not compute posterior marginals if no-one is listening for them\r\n",
    "# At this moment we have already unsubscribed from the new posterior updates so this `update!` does nothing\r\n",
    "update!(y, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Rocket.jl` provides some useful built-in actors for obtaining posterior marginals especially with static datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeepActor{Marginal}(Marginal[])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the `keep` actor simply keeps all incoming updates in an internal storage, ordered\r\n",
    "θvalues = keep(Marginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `getmarginal` always emits last cached value as its first value\r\n",
    "subscribe!(getmarginal(θ) |> take(1), θvalues);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Marginal}:\n",
       " Marginal(Beta{Float64}(α=404.0, β=105.0))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(θvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscribe!(getmarginal(θ) |> take(1), θvalues);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Marginal}:\n",
       " Marginal(Beta{Float64}(α=404.0, β=105.0))\n",
       " Marginal(Beta{Float64}(α=404.0, β=105.0))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(θvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BufferActor{Marginal, Vector{Marginal}}(Marginal[#undef])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the `buffer` actor keeps very last incoming update in an internal storage and can also store \r\n",
    "# an array of updates for a sequence of random variables\r\n",
    "θbuffer = buffer(Marginal, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscribe!(getmarginals([ θ ]) |> take(1), θbuffer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Marginal}:\n",
       " Marginal(Beta{Float64}(α=404.0, β=105.0))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(θbuffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscribe!(getmarginals([ θ ]) |> take(1), θbuffer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Marginal}:\n",
       " Marginal(Beta{Float64}(α=404.0, β=105.0))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(θbuffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reactive Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReactiveMP.jl naturally supports reactive streams of data and it is possible to run reactive inference with some external datasource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "online_coin_toss_model (generic function with 1 method)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function online_coin_toss_model()\n",
    "    \n",
    "    # We create datavars for the prior \n",
    "    # over `θ` variable\n",
    "    θ_a = datavar(Float64)\n",
    "    θ_b = datavar(Float64)\n",
    "    \n",
    "    θ ~ Beta(θ_a, θ_b)\n",
    "    \n",
    "    y = datavar(Float64)\n",
    "    y ~ Bernoulli(θ)\n",
    "\n",
    "    return θ_a, θ_b, θ, y\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (θ_a, θ_b, θ, y) = online_coin_toss_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example we subscribe on posterior marginal of θ variable and use it as a prior for our next observation\n",
    "# We also print into stdout for convenience\n",
    "θ_subscription = subscribe!(getmarginal(θ), (m) -> begin \n",
    "    m_a, m_b = params(m)\n",
    "    update!(θ_a, m_a)\n",
    "    update!(θ_b, m_b)\n",
    "    println(\"New posterior for θ: mean = \", mean(m), \", std = \", std(m))\n",
    "end);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial priors\n",
    "update!(θ_a, 10.0 * rand())\n",
    "update!(θ_b, 10.0 * rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = timer(500, 500) |> map(Float64, (_) -> float(rand(Bernoulli(0.75)))) |> tap((v) -> println(\"New observation: \", v));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimerSubscription()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_subscription = subscribe!(data_source, (data) -> update!(y, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is important to unsubscribe from running observables to release computer resources\n",
    "unsubscribe!(data_subscription)\n",
    "unsubscribe!(θ_subscription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was an example of exact Bayesian inference with Sum-Product (or Belief Propagation) algorithm. However, `ReactiveMP.jl` is not limited to only the sum-product algorithm but it also supports variational message passing with [Constrained Bethe Free Energy Minimisation](https://www.mdpi.com/1099-4300/23/7/807)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a very high-level, ReactiveMP.jl is aimed to solve the Constrained Bethe Free Energy minimisation problem. For this task we approximate our exact posterior marginal distribution by some family of distributions $q \\in \\mathcal{Q}$. Often this involves assuming some factorization over $q$. For this purpose the `@model` macro supports optional `where { ... }` clauses for every `~` expression in a model specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_model6 (generic function with 1 method)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function test_model6(n)\r\n",
    "    τ ~ GammaShapeRate(1.0, 1.0) \r\n",
    "    μ ~ NormalMeanVariance(0.0, 100.0)\r\n",
    "    \r\n",
    "    y = datavar(Float64, n)\r\n",
    "    \r\n",
    "    for i in 1:n\r\n",
    "        # Here we assume a mean-field assumption on our \r\n",
    "        # variational family of distributions locally for the current node\r\n",
    "        y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) }\r\n",
    "    end\r\n",
    "    \r\n",
    "    return μ, τ, y\r\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we specified an extra constraints for $q_a$ for Bethe factorisation:\n",
    "\n",
    "$$\n",
    "q(s) = \\prod_{a \\in \\mathcal{V}} q_a(s_a) \\prod_{i \\in \\mathcal{E}} q_i^{-1}(s_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several options to specify the mean-field factorisation constraint. \n",
    "\n",
    "```julia\n",
    "y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) } # With names from model specification\n",
    "y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(out)q(mean)q(precision) } # With names from node specification\n",
    "y[i] ~ NormalMeanPrecision(μ, τ) where { q = MeanField() } # With alias name\n",
    "```\n",
    "\n",
    "It is also possible to use local structured factorisation:\n",
    "\n",
    "```julia\n",
    "y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i], μ)q(τ) } # With names from model specification\n",
    "y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(out, mean)q(precision) } # With names from node specification\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an option the `@model` macro accepts optional arguments for model specification, one of which is `default_factorisation` that accepts `MeanField()` as its argument for better convenience\r\n",
    "\r\n",
    "```julia\r\n",
    "@model [ default_factorisation = MeanField() ] function test_model(...)\r\n",
    "    ...\r\n",
    "end\r\n",
    "```\r\n",
    "This will autatically impose a mean field factorization constraint over all marginal distributions in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run inference in this model we again need to create a synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rand(Normal(-3.0, inv(sqrt(5.0))), 1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, (μ, τ, y) = test_model6(length(dataset));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For variational inference we also usually need to set initial marginals for our inference procedure. For that purpose `ReactiveMP.jl` export the `setmarginal!` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "setmarginal!(μ, vague(NormalMeanPrecision))\r\n",
    "setmarginal!(τ, vague(GammaShapeRate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "μ_values = keep(Marginal)\r\n",
    "τ_values = keep(Marginal)\r\n",
    "\r\n",
    "μ_subscription = subscribe!(getmarginal(μ), μ_values)\r\n",
    "τ_subscription = subscribe!(getmarginal(τ), τ_values)\r\n",
    "\r\n",
    "for i in 1:10\r\n",
    "    update!(y, dataset)\r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Marginal}:\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-3.0212974270295243e-9, w=0.010000001002000566))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-27.644279686124342, w=9.178103741600976))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-9774.492725115699, w=3241.6775110680173))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-15073.107906957546, w=4998.939925770426))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-15081.266894092054, w=5001.645817959491))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-15081.275041176204, w=5001.648519904173))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-15081.275049307018, w=5001.648522600755))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-15081.275049315067, w=5001.648522603477))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-15081.275049315103, w=5001.648522603478))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-15081.275049315096, w=5001.648522603478))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(μ_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Marginal}:\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=5.0000000000464625e14))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=54645.97850552969))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=154.5500882769262))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=100.22144887793893))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=100.16722892959258))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=100.16717481806423))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=100.16717476406079))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=100.16717476400719))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=100.16717476400692))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=100.16717476400699))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(τ_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "μ: mean = -3.0152608647248424, std = 0.01413980483704458\n"
     ]
    }
   ],
   "source": [
    "println(\"μ: mean = \", mean(last(μ_values)), \", std = \", std(last(μ_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "τ: mean = 5.001638522603355, std = 0.22345672959563467\n"
     ]
    }
   ],
   "source": [
    "println(\"τ: mean = \", mean(last(τ_values)), \", std = \", std(last(τ_values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form constraints\r\n",
    "\r\n",
    "In order to support form constraints, the `randomvar()` function also supports a `where { ... }` clause with some optional arguments. One of these arguments is `form_constraint` that allows us to specify a form constraint to the random variables in our model. Another one is `prod_constraint` that allows to specify an additional constraints during computation of product of two colliding messages. For example we can perform the EM algorithm if we assign a point mass contraint on some variables in our model.\r\n",
    "\r\n",
    "<img style=\"display: block;\r\n",
    "  margin-left: auto;\r\n",
    "  margin-right: auto;\r\n",
    "  width: 50%;\" src=\"./pics/posterior.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_model7 (generic function with 1 method)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function test_model7(n)\r\n",
    "    τ ~ GammaShapeRate(1.0, 1.0) \r\n",
    "    \r\n",
    "    # In case of form constraints `randomvar()` call is necessary\r\n",
    "    μ = randomvar() where { form_constraint = PointMassFormConstraint() }\r\n",
    "    μ ~ NormalMeanVariance(0.0, 100.0)\r\n",
    "    \r\n",
    "    y = datavar(Float64, n)\r\n",
    "    \r\n",
    "    for i in 1:n\r\n",
    "        y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) }\r\n",
    "    end\r\n",
    "    \r\n",
    "    return μ, τ, y\r\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we specified an extra constraints for $q_i$ for Bethe factorisation:\n",
    "\n",
    "$$\n",
    "q(s) = \\prod_{a \\in \\mathcal{V}} q_a(s_a) \\prod_{i \\in \\mathcal{E}} q_i^{-1}(s_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, (μ, τ, y) = test_model7(length(dataset));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "setmarginal!(μ, vague(NormalMeanPrecision))\r\n",
    "setmarginal!(τ, PointMass(1.0))\r\n",
    "\r\n",
    "μ_values = keep(Marginal)\r\n",
    "τ_values = keep(Marginal)\r\n",
    "\r\n",
    "μ_subscription = subscribe!(getmarginal(μ), μ_values)\r\n",
    "τ_subscription = subscribe!(getmarginal(τ), τ_values)\r\n",
    "\r\n",
    "for i in 1:10\r\n",
    "    update!(y, dataset)\r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Marginal(PointMass{Float64}(-3.0152608707414226))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(μ_values) |> last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Marginal(GammaShapeRate{Float64}(a=501.0, b=100.0672077235559))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(τ_values) |> last "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `ReactiveMP.jl` tries to compute an analytical product of two colliding messages and throws an error if no analytical solution is known. However, it is possible to fall back to a generic product that does not require an analytical solution to be known. In this case the inference backend will simply propagate the product of two message in a form of a tuple. It is not possible to use such a tuple-product during an inference and in this case it is mandatory to use some form constraint to approximate this product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```julia\n",
    "μ = randomvar() where { \n",
    "    prod_constraint = ProdGeneric(),\n",
    "    form_constraint = SampleListFormConstraint() \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is useful to preserve a specific parametrisation of the resulting product later on in an inference procedure. `ReactiveMP.jl` exports a special `prod_constraint` called `ProdPreserveType` especially for that purpose:\r\n",
    "\r\n",
    "```julia\r\n",
    "μ = randomvar() where { prod_constraint = ProdPreserveType(NormalWeightedMeanPrecision) }\r\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free Energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During variational inference `ReactiveMP.jl` optimises a special functional called the Bethe Free Energy functional. It is possible to obtain its values for all VMP iterations with the `score` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, (μ, τ, y) = test_model6(length(dataset));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProxyObservable(Real, MapProxy(Tuple{ReactiveMP.InfCountingReal, ReactiveMP.InfCountingReal}))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfe_observable = score(BetheFreeEnergy(), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfe_subscription = subscribe!(bfe_observable, (fe) -> println(\"Current BFE value: \", fe));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current BFE value: 14763.268311193482\n",
      "Current BFE value: 3276.2059895046787\n",
      "Current BFE value: 662.6822962461524\n",
      "Current BFE value: 621.7420641096833\n",
      "Current BFE value: 621.7419906930281\n",
      "Current BFE value: 621.7419906929467\n",
      "Current BFE value: 621.7419906929499\n",
      "Current BFE value: 621.7419906929513\n",
      "Current BFE value: 621.7419906929531\n",
      "Current BFE value: 621.7419906929522\n"
     ]
    }
   ],
   "source": [
    "# Reset the model with vague marginals\r\n",
    "setmarginal!(μ, vague(NormalMeanPrecision))\r\n",
    "setmarginal!(τ, vague(GammaShapeRate))\r\n",
    "\r\n",
    "for i in 1:10\r\n",
    "    update!(y, dataset)\r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It always necessary to unsubscribe and release computer resources\r\n",
    "unsubscribe!([ μ_subscription, τ_subscription, bfe_subscription ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta data specification\r\n",
    "\r\n",
    "During model specification some functional dependencies may accept an optional `meta` object in the `where { ... }` clause. The purpose of the `meta` object is to adjust, modify or supply some extra information to the inference backend during the computations of the messages. The `meta` object for example may contain an approximation method that needs to be used during various approximations or it may specify the tradeoff between accuracy and performance:\r\n",
    "\r\n",
    "```julia\r\n",
    "# In this example the `meta` object for the autoregressive `AR` node specifies the variate type of \r\n",
    "# the autoregressive process and its order. In addition it specifies that the message computation rules should\r\n",
    "# respect accuracy over speed with the `ARsafe()` strategy. In contrast, `ARunsafe()` strategy tries to speedup computations\r\n",
    "# by cost of possible numerical instabilities during an inference procedure\r\n",
    "s[i] ~ AR(s[i - 1], θ, γ) where { q = q(s[i - 1], s[i])q(θ)q(γ), meta = ARMeta(Multivariate, order, ARsafe()) }\r\n",
    "...\r\n",
    "s[i] ~ AR(s[i - 1], θ, γ) where { q = q(s[i - 1], s[i])q(θ)q(γ), meta = ARMeta(Univariate, order, ARunsafe()) }\r\n",
    "```\r\n",
    "\r\n",
    "Another example with `GaussianControlledVariance`, or simply `GCV` [see Hierarchical Gaussian Filter], node:\r\n",
    "\r\n",
    "```julia\r\n",
    "# In this example we specify structured factorisation and flag meta with `GaussHermiteCubature` \r\n",
    "# method with `21` sigma points for approximation of non-lineariety between hierarchy layers\r\n",
    "xt ~ GCV(xt_min, zt, real_k, real_w) where { q = q(xt, xt_min)q(zt)q(κ)q(ω), meta = GCVMetadata(GaussHermiteCubature(21)) }\r\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Meta object is useful to pass any extra information to a node that is not a random variable or constant model variable. It may include extra approximation methods, differentiation methods, optional non-linear functions, extra inference parameters etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom nodes and message computation rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom nodes\n",
    "\n",
    "To create a custom functional form and to make it available during model specification `ReactiveMP.jl` exports the `@node` macro:\n",
    "\n",
    "```julia\n",
    "# `@node` macro accepts a name of the functional form, its type, either `Stochastic` or `Deterministic` and an array of interfaces:\n",
    "@node NormalMeanVariance Stochastic [ out, μ, v ]\n",
    "\n",
    "# Interfaces may have aliases for their names that might be convenient for factorisation constraints specification\n",
    "@node NormalMeanVariance Stochastic [ out, (μ, aliases = [ mean ]), (v, aliases = [ var ]) ]\n",
    "\n",
    "# `NormalMeanVariance` structure declaration must exist, otherwise `@node` macro will throw an error\n",
    "struct NormalMeanVariance end \n",
    "\n",
    "@node NormalMeanVariance Stochastic [ out, μ, v ]\n",
    "\n",
    "# It is also possible to use function objects as a node functional form\n",
    "function dot end\n",
    "\n",
    "# Syntax for functions is a bit differet, as it is necesssary to use `typeof(...)` function for them \n",
    "# out = dot(x, a)\n",
    "@node typeof(dot) Deterministic [ out, x, a ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Deterministic nodes do not support factorisation constraints with the `where { q = ... }` clause.\r\n",
    "\r\n",
    "After that it is possible to use the newly created node during model specification:\r\n",
    "\r\n",
    "```julia\r\n",
    "@model function test_model()\r\n",
    "    ...\r\n",
    "    y ~ dot(x, a)\r\n",
    "    ...\r\n",
    "end\r\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom messages computation rules\r\n",
    "\r\n",
    "`ReactiveMP.jl` exports the `@rule` macro to create custom message computation rules. For example let us create a simple `+` node to be available for usage in the model specification usage. We refer to *A Factor Graph Approach to Signal Modelling , System Identification and Filtering* [ Sascha Korl, 2005, page 32 ] for a rigorous explanation of the `+` node in factor graphs. According to Korl, assuming that inputs are Gaussian Sum-Product message computation rule for `+` node is the following:\r\n",
    "\r\n",
    "$$\r\n",
    "\\mu_z = \\mu_x + \\mu_y \\\\\r\n",
    "V_z = V_x + V_y\r\n",
    "$$\r\n",
    "\r\n",
    "To specify this in `ReactiveMP.jl` we use the `@node` and `@rule` macros:\r\n",
    " \r\n",
    "```julia\r\n",
    "@node typeof(+) Deterministic  [ z, x, y ]\r\n",
    "\r\n",
    "@rule typeof(+)(:z, Marginalisation) (m_x::UnivariateNormalDistributionsFamily, m_y::UnivariateNormalDistributionsFamily) = begin\r\n",
    "    x_mean, x_var = mean_var(m_x)\r\n",
    "    y_mean, y_var = mean_var(m_y)\r\n",
    "    return NormalMeanVariance(x_mean + y_mean, x_var + y_var)\r\n",
    "end\r\n",
    "```\r\n",
    "\r\n",
    "In this example, for the `@rule` macro, we specify a type of our functional form: `typeof(+)`. Next, we specify an edge we are going to compute an outbound message for. `Marginalisation` indicates that the corresponding message respects the marginalisation constraint for posterior over corresponding edge:\r\n",
    "\r\n",
    "$$\r\n",
    "q(z) = \\int q(z, x, y) \\mathrm{d}x\\mathrm{d}y\r\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look on difference between sum-product rules and variational rules with mean-field assumption we notice that they require different local information to compute an outgoing message:\n",
    "\n",
    "<div style=\"width:100%\">\n",
    "<div style=\"width:50%; left:25%; position: relative; padding-top: 50px; padding-bottom: 50px\">\n",
    "<img style=\"display: block;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "  width: 50%;\" src=\"./pics/sp.png\" align=\"left\"/>\n",
    "\n",
    "<img style=\"display: block;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "  width: 50%;\" src=\"./pics/vmp.png\" align=\"left\" />\n",
    "</div>\n",
    "<div style=\"width:50%\">&nbsp;&nbsp;&nbsp;&nbsp;</div>\n",
    "</div\n",
    "\n",
    "<div style=\"width:100%\">\n",
    "$$\n",
    "\\mu(z) = \\int f(x, y, z)\\mu(x)\\mu(y)\\mathrm{d}x\\mathrm{d}y\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nu(z) = \\exp{ \\int \\log f(x, y, z)q(x)q(y)\\mathrm{d}x\\mathrm{d}y }\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@rule` macro supports both cases with special prefixes during rule specification:\r\n",
    "- `m_` prefix corresponds to the incoming message on a specific edge\r\n",
    "- `q_` prefix corresponds to the posterior marginal of a specific edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a Sum-Product rule with `m_` messages used:\n",
    "\n",
    "```julia\n",
    "@rule NormalMeanPrecision(:μ, Marginalisation) (m_out::UnivariateNormalDistributionsFamily, m_τ::PointMass) = begin \n",
    "    m_out_mean, m_out_cov = mean_cov(m_out)\n",
    "    return NormalMeanPrecision(m_out_mean, inv(m_out_cov + inv(mean(m_τ))))\n",
    "end\n",
    "```\n",
    "\n",
    "Example of a Variational rule with Mean-Field assumption with `q_` posteriors used:\n",
    "\n",
    "```julia\n",
    "@rule NormalMeanPrecision(:μ, Marginalisation) (q_out::Any, q_τ::Any) = begin \n",
    "    return NormalMeanPrecision(mean(q_out), mean(q_τ))\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ReactiveMP.jl` also supports structured rules. It is possible to obtain joint marginal over a set of edges:\n",
    "\n",
    "```julia\n",
    "@rule NormalMeanPrecision(:τ, Marginalisation) (q_out_μ::Any, ) = begin\n",
    "    m, V = mean_cov(q_out_μ)\n",
    "    θ = 2 / (V[1,1] - V[1,2] - V[2,1] + V[2,2] + abs2(m[1] - m[2]))\n",
    "    α = convert(typeof(θ), 1.5)\n",
    "    return Gamma(α, θ)\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: In the `@rule` specification the messages or marginals arguments **must** be in order with interfaces specification from `@node` macro:\r\n",
    "\r\n",
    "```julia\r\n",
    "# Inference backend expects arguments in `@rule` macro to be in the same order\r\n",
    "@node NormalMeanPrecision Stochastic [ out, μ, τ ]\r\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any rule always has access to the meta information with hidden the `meta::Any` variable:\r\n",
    "\r\n",
    "```julia\r\n",
    "@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any) = begin \r\n",
    "    ...\r\n",
    "    println(meta)\r\n",
    "    ...\r\n",
    "end\r\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to dispatch on a specific type of a meta object:\n",
    "\n",
    "```julia\n",
    "@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::LaplaceApproximation) = begin \n",
    "    ...\n",
    "end\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```julia\n",
    "@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::GaussHermiteCubature) = begin \n",
    "    ...\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing messages computational pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In certain situations it might be convenient to customize the default message computational pipeline. `GrahpPPL.jl` supports the `pipeline` keyword in the `where { ... }` clause to add some extra steps after a message has been computed. A use case might be an extra approximation method to preserve conjugacy in the model, debugging or simple printing.\r\n",
    "\r\n",
    "<img style=\"display: block;\r\n",
    "  margin-left: auto;\r\n",
    "  margin-right: auto;\r\n",
    "  width: 30%;\" src=\"./pics/pipeline.png\" width=\"20%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```julia\n",
    "# Logs all outbound messages\n",
    "y[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = LoggerPipelineStage() }\n",
    "# Initialise messages to be vague\n",
    "y[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = InitVaguePipelineStage() }\n",
    "# In principle, it is possible to approximate outbound messages with Laplace Approximation\n",
    "y[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = LaplaceApproximation() }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us return to the coin toss model, but this time we want to print flowing messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coin_toss_model_log (generic function with 1 method)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function coin_toss_model_log(n)\r\n",
    "\r\n",
    "    y = datavar(Float64, n)\r\n",
    "\r\n",
    "    θ ~ Beta(2.0, 7.0) where { pipeline = LoggerPipelineStage(\"θ\") }\r\n",
    "\r\n",
    "    for i in 1:n\r\n",
    "        y[i] ~ Bernoulli(θ)  where { pipeline = LoggerPipelineStage(\"y[$i]\") }\r\n",
    "    end\r\n",
    "    \r\n",
    "    return y, θ\r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (y, θ) = coin_toss_model_log(5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[θ][Beta][out]: Message(Beta{Float64}(α=2.0, β=7.0))\n"
     ]
    }
   ],
   "source": [
    "θ_subscription = subscribe!(getmarginal(θ), (value) -> println(\"New posterior marginal for θ: \", value));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "coinflips = float.(rand(Bernoulli(0.5), 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[y[1]][Bernoulli][p]: Message(Beta{Float64}(α=1.0, β=2.0))\n",
      "[y[2]][Bernoulli][p]: Message(Beta{Float64}(α=2.0, β=1.0))\n",
      "[y[3]][Bernoulli][p]: Message(Beta{Float64}(α=2.0, β=1.0))\n",
      "[y[4]][Bernoulli][p]: Message(Beta{Float64}(α=1.0, β=2.0))\n",
      "[y[5]][Bernoulli][p]: Message(Beta{Float64}(α=1.0, β=2.0))\n",
      "New posterior marginal for θ: Marginal(Beta{Float64}(α=4.0, β=10.0))\n"
     ]
    }
   ],
   "source": [
    "update!(y, coinflips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsubscribe!(θ_subscription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference is lazy and does not send messages if no one is listening for them\r\n",
    "update!(y, coinflips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
