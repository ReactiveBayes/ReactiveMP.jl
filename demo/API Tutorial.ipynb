{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling ReactiveMP [a194aa59-28ba-4574-a09c-4a745416d6e3]\n",
      "└ @ Base loading.jl:1423\n",
      "┌ Info: Precompiling GraphPPL [b3f8163a-e979-4e85-b43e-1f63d8c8b42c]\n",
      "└ @ Base loading.jl:1423\n"
     ]
    }
   ],
   "source": [
    "# Reactive programming package for Julia\n",
    "using Rocket \n",
    "# Core package for Constrained Bethe Free Energy minimsation with Factor graphs and message passing\n",
    "using ReactiveMP \n",
    "# High-level user friendly probabilistic model and constraints specification language package for ReactiveMP\n",
    "using GraphPPL\n",
    "# Optionally include the Distributions.jl package and the Random package from Base\n",
    "using Distributions, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers the fundamentals of the ReactiveMP.jl package. For a more advanced usage we refer the interested reader to the documentation.\n",
    "\n",
    "This tutorial is also available in the [documentation](https://biaslab.github.io/ReactiveMP.jl/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General model specification syntax\n",
    "\n",
    "We use the `@model` macro from the `GraphPPL.jl` package to create a probabilistic model $p(s, y)$ and we also specify extra constraints on the variational family of distributions $\\mathcal{Q}$, used for approximating intractable posterior distributions.\n",
    "Below there is a simple example of the general syntax for model specification. In this tutorial we do not cover all possible ways to create models or advanced features of `GraphPPL.jl`.  Instead we refer the interested reader to the documentation for a more rigorous explanation and illustrative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the `@model` macro accepts a regular Julia function\n",
    "@model function test_model1(s_mean, s_precision)\n",
    "    \n",
    "    # We use the `randomvar` function to create \n",
    "    # a random variable in our model\n",
    "    s = randomvar()\n",
    "    \n",
    "    # the `tilde` operator creates a functional dependency\n",
    "    # between variables in our model and can be read as \n",
    "    # `sampled from` or `is modeled by`\n",
    "    s ~ GaussianMeanPrecision(s_mean, s_precision)\n",
    "    \n",
    "    # We use the `datavar` function to create \n",
    "    # observed data variables in our models\n",
    "    # We also need to specify the type of our data \n",
    "    # In this example it is `Float64`\n",
    "    y = datavar(Float64)\n",
    "    \n",
    "    y ~ GaussianMeanPrecision(s, 1.0)\n",
    "    \n",
    "    # In general `@model` macro returns a variable of interests\n",
    "    # However it is also possible to obtain all variable in the model \n",
    "    # with the `ReactiveMP.getvardict(model)` function call\n",
    "    return s, y \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@model` macro creates a function with the same name and with the same set of input arguments as the original function (`test_model1(s_mean, s_precision)` in this example). However, the return value is modified in such a way to contain a reference to the model object as the first value and to the user specified variables in the form of a tuple as the second value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FactorGraphModel(), (RandomVariable(s), DataVariable(y)))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, (s, y) = test_model1(0.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of creating the model is to use the `Model` function that returns an instance of `ModelGenerator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FactorGraphModel(), (RandomVariable(s), DataVariable(y)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelgenerator = Model(test_model1, 0.0, 1.0)\n",
    "\n",
    "model, (s, y) = ReactiveMP.create_model(modelgenerator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefits of using model generator as a way to create a model is that it allows to change inference constraints and meta specification for nodes. We will talk about factorisation and form constraints and meta specification later on in this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GraphPPL.jl` returns a factor graph-based representation of a model. We can examine this factor graph structure with the help of some utility functions such as: \n",
    "- `getnodes()`: returns an array of factor nodes in a correposning factor graph\n",
    "- `getrandom()`: returns an array of random variable in the model\n",
    "- `getdata()`: returns an array of data inputs in the model\n",
    "- `getconstant()`: return an array of constant values in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{ReactiveMP.AbstractFactorNode}:\n",
       " FactorNode:\n",
       " form            : NormalMeanPrecision\n",
       " sdtype          : Stochastic()\n",
       " interfaces      : (Interface(out, Marginalisation()), Interface(μ, Marginalisation()), Interface(τ, Marginalisation()))\n",
       " factorisation   : ((1,), (2,), (3,))\n",
       " local marginals : (:out, :μ, :τ)\n",
       " metadata        : nothing\n",
       " pipeline        : FactorNodePipeline(functional_dependencies = DefaultFunctionalDependencies(), extra_stages = EmptyPipelineStage()\n",
       "\n",
       " FactorNode:\n",
       " form            : NormalMeanPrecision\n",
       " sdtype          : Stochastic()\n",
       " interfaces      : (Interface(out, Marginalisation()), Interface(μ, Marginalisation()), Interface(τ, Marginalisation()))\n",
       " factorisation   : ((1,), (2,), (3,))\n",
       " local marginals : (:out, :μ, :τ)\n",
       " metadata        : nothing\n",
       " pipeline        : FactorNodePipeline(functional_dependencies = DefaultFunctionalDependencies(), extra_stages = EmptyPipelineStage()\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getnodes(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Symbol}:\n",
       " :s"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getrandom(model) .|> name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Symbol}:\n",
       " :y"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getdata(model) .|> name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " 0.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getconstant(model) .|> getconst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use control flow statements such as `if` or `for` blocks in the model specification function. In general, any valid snippet of Julia code can be used inside the `@model` block. As an example consider the following (valid!) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function test_model2(n)\n",
    "    \n",
    "    if n <= 1\n",
    "        error(\"`n` argument must be greater than one.\")\n",
    "    end\n",
    "    \n",
    "    # `randomvar(n)` creates a dense sequence of \n",
    "    # random variables\n",
    "    s = randomvar(n)\n",
    "    \n",
    "    # `datavar(Float64, n)` creates a dense sequence of \n",
    "    # observed data variables of type `Float64`\n",
    "    y = datavar(Float64, n)\n",
    "    \n",
    "    s[1] ~ GaussianMeanPrecision(0.0, 0.1)\n",
    "    y[1] ~ GaussianMeanPrecision(s[1], 1.0)\n",
    "    \n",
    "    for i in 2:n\n",
    "        s[i] ~ GaussianMeanPrecision(s[i - 1], 1.0)\n",
    "        y[i] ~ GaussianMeanPrecision(s[i], 1.0)\n",
    "    end\n",
    "    \n",
    "    return s, y\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some limitations though regarding using `if`-blocks to create random variables. It is advised to create random variables in advance before `if` block, e.g instead of \n",
    "\n",
    "```julia\n",
    "if some_condition\n",
    "    x ~ Normal(0.0, 1.0)\n",
    "else\n",
    "    x ~ Normal(0.0, 100.0)\n",
    "end\n",
    "```\n",
    "\n",
    "some needs to write:\n",
    "\n",
    "```julia\n",
    "x = randomvar()\n",
    "\n",
    "if some_condition\n",
    "    x ~ Normal(0.0, 1.0)\n",
    "else\n",
    "    x ~ Normal(0.0, 100.0)\n",
    "end\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, (s, y) = test_model2(10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An amount of factor nodes in generated Factor Graph\n",
    "getnodes(model) |> length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An amount of random variables\n",
    "getrandom(model) |> length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An amount of data inputs\n",
    "getdata(model) |> length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An amount of constant values\n",
    "getconstant(model) |> length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use complex expression inside the functional dependency expressions\n",
    "\n",
    "```julia\n",
    "y ~ NormalMeanPrecision(2.0 * (s + 1.0), 1.0)\n",
    "```\n",
    "\n",
    "The `~` operator automatically creates a random variable if none was created before with the same name and throws an error if this name already exists\n",
    "\n",
    "```julia\n",
    "# s = randomvar() here is optional\n",
    "# `~` creates random variables automatically\n",
    "s ~ NormalMeanPrecision(0.0, 1.0)\n",
    "```\n",
    "\n",
    "An example model which will throw an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: Invalid name 's' for new random variable. 's' was already initialized with '=' operator before.\nin expression starting at /Users/bvdmitri/.julia/dev/GraphPPL.jl/src/model.jl:173",
     "output_type": "error",
     "traceback": [
      "LoadError: Invalid name 's' for new random variable. 's' was already initialized with '=' operator before.\nin expression starting at /Users/bvdmitri/.julia/dev/GraphPPL.jl/src/model.jl:173",
      "",
      "Stacktrace:",
      "  [1] error(s::String)",
      "    @ Base ./error.jl:33",
      "  [2] (::GraphPPL.var\"#39#52\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol})(expression::Expr)",
      "    @ GraphPPL ~/.julia/dev/GraphPPL.jl/src/model.jl:320",
      "  [3] walk",
      "    @ ~/.julia/packages/MacroTools/PP9IQ/src/utils.jl:112 [inlined]",
      "  [4] postwalk",
      "    @ ~/.julia/packages/MacroTools/PP9IQ/src/utils.jl:122 [inlined]",
      "  [5] (::MacroTools.var\"#21#22\"{GraphPPL.var\"#39#52\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol}})(x::Expr)",
      "    @ MacroTools ~/.julia/packages/MacroTools/PP9IQ/src/utils.jl:122",
      "  [6] iterate",
      "    @ ./generator.jl:47 [inlined]",
      "  [7] collect_to!(dest::Vector{Any}, itr::Base.Generator{Vector{Any}, MacroTools.var\"#21#22\"{GraphPPL.var\"#39#52\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol}}}, offs::Int64, st::Int64)",
      "    @ Base ./array.jl:782",
      "  [8] collect_to!(dest::Vector{LineNumberNode}, itr::Base.Generator{Vector{Any}, MacroTools.var\"#21#22\"{GraphPPL.var\"#39#52\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol}}}, offs::Int64, st::Int64)",
      "    @ Base ./array.jl:790",
      "  [9] collect_to_with_first!(dest::Vector{LineNumberNode}, v1::LineNumberNode, itr::Base.Generator{Vector{Any}, MacroTools.var\"#21#22\"{GraphPPL.var\"#39#52\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol}}}, st::Int64)",
      "    @ Base ./array.jl:760",
      " [10] _collect(c::Vector{Any}, itr::Base.Generator{Vector{Any}, MacroTools.var\"#21#22\"{GraphPPL.var\"#39#52\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol}}}, #unused#::Base.EltypeUnknown, isz::Base.HasShape{1})",
      "    @ Base ./array.jl:754",
      " [11] collect_similar(cont::Vector{Any}, itr::Base.Generator{Vector{Any}, MacroTools.var\"#21#22\"{GraphPPL.var\"#39#52\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol}}})",
      "    @ Base ./array.jl:653",
      " [12] map(f::Function, A::Vector{Any})",
      "    @ Base ./abstractarray.jl:2849",
      " [13] walk(x::Expr, inner::Function, outer::GraphPPL.var\"#39#52\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol})",
      "    @ MacroTools ~/.julia/packages/MacroTools/PP9IQ/src/utils.jl:112",
      " [14] postwalk(f::Function, x::Expr)",
      "    @ MacroTools ~/.julia/packages/MacroTools/PP9IQ/src/utils.jl:122",
      " [15] generate_model_expression(backend::ReactiveMPBackend, model_options::Expr, model_specification::Expr)",
      "    @ GraphPPL ~/.julia/dev/GraphPPL.jl/src/model.jl:287",
      " [16] var\"@model\"(__source__::LineNumberNode, __module__::Module, model_options::Any, model_specification::Any)",
      "    @ GraphPPL ~/.julia/dev/GraphPPL.jl/src/model.jl:177",
      " [17] eval",
      "    @ ./boot.jl:373 [inlined]",
      " [18] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "@model function error_model1()\n",
    "    s = 1.0\n",
    "    s ~ NormalMeanPrecision(0.0, 1.0)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the `GraphPPL.jl` package creates new references for constants (literals like `0.0` or `1.0`) in a model. In some situations this may not be efficient, especially when these constants represent large matrices. `GraphPPL.jl` will by default create new copies of some constant (e.g. matrix) in a model every time it uses it. However it is possible to use `constvar()` function to create and reuse similar constants in the model specification syntax as\n",
    "\n",
    "```julia\n",
    "# Creates constant reference in a model with a prespecified value\n",
    "c = constvar(0.0)\n",
    "```\n",
    "\n",
    "An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function test_model5(dim::Int, n::Int, A::Matrix, P::Matrix, Q::Matrix)\n",
    "    \n",
    "    s = randomvar(n)\n",
    "    \n",
    "    y = datavar(Vector{Float64}, n)\n",
    "    \n",
    "    # Here we create constant references\n",
    "    # for constant matrices in our model \n",
    "    # to make inference more memory efficient\n",
    "    cA = constvar(A)\n",
    "    cP = constvar(P)\n",
    "    cQ = constvar(Q)\n",
    "    \n",
    "    s[1] ~ MvGaussianMeanCovariance(zeros(dim), cP)\n",
    "    y[1] ~ MvGaussianMeanCovariance(s[1], cQ)\n",
    "    \n",
    "    for i in 2:n\n",
    "        s[i] ~ MvGaussianMeanCovariance(cA * s[i - 1], cP)\n",
    "        y[i] ~ MvGaussianMeanCovariance(s[i], cQ)\n",
    "    end\n",
    "    \n",
    "    return s, y\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `~` expression can also return a reference to a newly created node in a corresponding factor graph for convenience in later usage:\n",
    "\n",
    "```julia\n",
    "@model function test_model()\n",
    "\n",
    "    # In this example `ynode` refers to the corresponding \n",
    "    # `GaussianMeanVariance` node created in the factor graph\n",
    "    ynode, y ~ GaussianMeanVariance(0.0, 1.0)\n",
    "    \n",
    "    return ynode, y\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic inference in ReactiveMP.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ReactiveMP.jl` uses the `Rocket.jl` package API for inference routines. `Rocket.jl` is a reactive programming extension for Julia that is higly inspired by `RxJS` and similar libraries from the `Rx` ecosystem. It consists of **observables**, **actors**, **subscriptions** and **operators**. For more infromation and rigorous examples see [Rocket.jl github page](https://github.com/biaslab/Rocket.jl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observables\n",
    "Observables are lazy push-based collections and they deliver their values over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimerObservable(1000, 1000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Timer that emits a new value every second and has an initial one second delay \n",
    "observable = timer(1000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subscription allows us to subscribe on future values of some observable, and actors specify what to do with these new values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimerSubscription()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor = (value) -> println(value)\n",
    "subscription1 = subscribe!(observable, actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We always need to unsubscribe from some observables\n",
    "unsubscribe!(subscription1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProxyObservable(Int64, MapProxy(Int64))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can modify our observables\n",
    "modified = observable |> filter(d -> rem(d, 2) === 1) |> map(Int, d -> d ^ 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimerSubscription()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subscription2 = subscribe!(modified, (value) -> println(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsubscribe!(subscription2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ReactiveMP.jl` package returns posterior marginal distributions in our specified model in the form of an observable. It is possible to subscribe on its future updates, but for convenience `ReactiveMP.jl` only caches the last obtained values of all marginals in a model. To get a reference for the posterior marginal of some random variable in a model `ReactiveMP.jl` exports two functions: \n",
    "- `getmarginal(x)`: for a single random variable `x`\n",
    "- `getmarginals(xs)`: for a dense sequence of random variables `sx`\n",
    "\n",
    "Lets see how it works in practice. Here we create a simple coin toss model. We assume that observations are governed by the `Bernoulli` distribution with unknown bias parameter `θ`. To have a fully Bayesian treatment of this problem we endow `θ` with the `Beta` prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function coin_toss_model(n)\n",
    "\n",
    "    # `datavar` creates data 'inputs' in our model\n",
    "    # We will pass data later on to these inputs\n",
    "    # In this example we create a sequence of inputs that accepts Float64\n",
    "    y = datavar(Float64, n)\n",
    "    \n",
    "    # We endow θ parameter of our model with some prior\n",
    "    θ ~ Beta(2.0, 7.0)\n",
    "    \n",
    "    # We assume that the outcome of each coin flip \n",
    "    # is modeled by a Bernoulli distribution\n",
    "    for i in 1:n\n",
    "        y[i] ~ Bernoulli(θ)\n",
    "    end\n",
    "    \n",
    "    # We return references to our data inputs and θ parameter\n",
    "    # We will use these references later on during the inference step\n",
    "    return y, θ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (y, θ) = coin_toss_model(500);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As soon as we have a new value for the marginal posterior over the `θ` variable\n",
    "# we simply print the first two statistics of it\n",
    "θ_subscription = subscribe!(getmarginal(θ), (marginal) -> println(\"New update: mean(θ) = \", mean(marginal), \", std(θ) = \", std(marginal)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets define our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.75 # Bias of a coin\n",
    "\n",
    "dataset = float.(rand(Bernoulli(p), 500));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass data to our model we use `update!` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New update: mean(θ) = 0.7426326129666012, std(θ) = 0.019358810889841\n"
     ]
    }
   ],
   "source": [
    "update!(y, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is necessary to always unsubscribe from running observables\n",
    "unsubscribe!(θ_subscription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ReactiveMP.jl inference backend is lazy and does not compute posterior marginals if no-one is listening for them\n",
    "# At this moment we have already unsubscribed from the new posterior updates so this `update!` does nothing\n",
    "update!(y, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Rocket.jl` provides some useful built-in actors for obtaining posterior marginals especially with static datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeepActor{Marginal}(Marginal[])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the `keep` actor simply keeps all incoming updates in an internal storage, ordered\n",
    "θvalues = keep(Marginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `getmarginal` always emits last cached value as its first value\n",
    "subscribe!(getmarginal(θ) |> take(1), θvalues);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Marginal}:\n",
       " Marginal(Beta{Float64}(α=378.0, β=131.0))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(θvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscribe!(getmarginal(θ) |> take(1), θvalues);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Marginal}:\n",
       " Marginal(Beta{Float64}(α=378.0, β=131.0))\n",
       " Marginal(Beta{Float64}(α=378.0, β=131.0))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(θvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BufferActor{Marginal, Vector{Marginal}}(Marginal[#undef])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the `buffer` actor keeps very last incoming update in an internal storage and can also store \n",
    "# an array of updates for a sequence of random variables\n",
    "θbuffer = buffer(Marginal, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscribe!(getmarginals([ θ ]) |> take(1), θbuffer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Marginal}:\n",
       " Marginal(Beta{Float64}(α=378.0, β=131.0))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(θbuffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscribe!(getmarginals([ θ ]) |> take(1), θbuffer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Marginal}:\n",
       " Marginal(Beta{Float64}(α=378.0, β=131.0))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(θbuffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reactive Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReactiveMP.jl naturally supports reactive streams of data and it is possible to run reactive inference with some external datasource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function online_coin_toss_model()\n",
    "    \n",
    "    # We create datavars for the prior \n",
    "    # over `θ` variable\n",
    "    θ_a = datavar(Float64)\n",
    "    θ_b = datavar(Float64)\n",
    "    \n",
    "    θ ~ Beta(θ_a, θ_b)\n",
    "    \n",
    "    y = datavar(Float64)\n",
    "    y ~ Bernoulli(θ)\n",
    "\n",
    "    return θ_a, θ_b, θ, y\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (θ_a, θ_b, θ, y) = online_coin_toss_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example we subscribe on posterior marginal of θ variable and use it as a prior for our next observation\n",
    "# We also print into stdout for convenience\n",
    "θ_subscription = subscribe!(getmarginal(θ), (m) -> begin \n",
    "    m_a, m_b = params(m)\n",
    "    update!(θ_a, m_a)\n",
    "    update!(θ_b, m_b)\n",
    "    println(\"New posterior for θ: mean = \", mean(m), \", std = \", std(m))\n",
    "end);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial priors\n",
    "update!(θ_a, 10.0 * rand())\n",
    "update!(θ_b, 10.0 * rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = timer(500, 500) |> map(Float64, (_) -> float(rand(Bernoulli(0.75)))) |> tap((v) -> println(\"New observation: \", v));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimerSubscription()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_subscription = subscribe!(data_source, (data) -> update!(y, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is important to unsubscribe from running observables to release computer resources\n",
    "unsubscribe!(data_subscription)\n",
    "unsubscribe!(θ_subscription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was an example of exact Bayesian inference with Sum-Product (or Belief Propagation) algorithm. However, `ReactiveMP.jl` is not limited to only the sum-product algorithm but it also supports variational message passing with [Constrained Bethe Free Energy Minimisation](https://www.mdpi.com/1099-4300/23/7/807)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a very high-level, ReactiveMP.jl is aimed to solve the Constrained Bethe Free Energy minimisation problem. For this task we approximate our exact posterior marginal distribution by some family of distributions $q \\in \\mathcal{Q}$. Often this involves assuming some factorization over $q$. For this purpose the `@model` macro supports optional `where { ... }` clauses for every `~` expression in a model specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function test_model6_with_manual_constraints(n)\n",
    "    τ ~ GammaShapeRate(1.0, 1.0) \n",
    "    μ ~ NormalMeanVariance(0.0, 100.0)\n",
    "    \n",
    "    y = datavar(Float64, n)\n",
    "    \n",
    "    for i in 1:n\n",
    "        # Here we assume a mean-field assumption on our \n",
    "        # variational family of distributions locally for the current node\n",
    "        y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) }\n",
    "    end\n",
    "    \n",
    "    return μ, τ, y\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we specified an extra constraints for $q_a$ for Bethe factorisation:\n",
    "\n",
    "$$\n",
    "q(s) = \\prod_{a \\in \\mathcal{V}} q_a(s_a) \\prod_{i \\in \\mathcal{E}} q_i^{-1}(s_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several options to specify the mean-field factorisation constraint. \n",
    "\n",
    "```julia\n",
    "y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) } # With names from model specification\n",
    "y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(out)q(mean)q(precision) } # With names from node specification\n",
    "y[i] ~ NormalMeanPrecision(μ, τ) where { q = MeanField() } # With alias name\n",
    "```\n",
    "\n",
    "It is also possible to use local structured factorisation:\n",
    "\n",
    "```julia\n",
    "y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i], μ)q(τ) } # With names from model specification\n",
    "y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(out, mean)q(precision) } # With names from node specification\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an option the `@model` macro accepts optional arguments for model specification, one of which is `default_factorisation` that accepts `MeanField()` as its argument for better convenience\n",
    "\n",
    "```julia\n",
    "@model [ default_factorisation = MeanField() ] function test_model(...)\n",
    "    ...\n",
    "end\n",
    "```\n",
    "This will autatically impose a mean field factorization constraint over all marginal distributions in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphPPL.jl constraints macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GraphPPL.jl` package exports `@constraints` macro to simplify factorisation and form constraints specification. Read more about `@constraints` macro in the corresponding documentation section, here we show a simple example of the same factorisation constraints specification, but with `@constraints` macro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Constraints:\n",
       "\tmarginals form:\n",
       "\tmessages form:\n",
       "\tfactorisation:\n",
       "\t\tq(μ, τ) = q(μ)q(τ)\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints6 = @constraints begin\n",
    "     q(μ, τ) = q(μ)q(τ) # Mean-Field over `μ` and `τ`\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: `where` blocks have higher priority over constraints specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function test_model6(n)\n",
    "    τ ~ GammaShapeRate(1.0, 1.0) \n",
    "    μ ~ NormalMeanVariance(0.0, 100.0)\n",
    "    \n",
    "    y = datavar(Float64, n)\n",
    "    \n",
    "    for i in 1:n\n",
    "        # Here we assume a mean-field assumption on our \n",
    "        # variational family of distributions locally for the current node\n",
    "        y[i] ~ NormalMeanPrecision(μ, τ)\n",
    "    end\n",
    "    \n",
    "    return μ, τ, y\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run inference in this model we again need to create a synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rand(Normal(-3.0, inv(sqrt(5.0))), 1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `inference` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to simplify model and inference testing, `ReactiveMP.jl` exports pre-written inference function, that is aimed for simple use cases with static datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1me\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "inference(\n",
       "    # `model`: specifies a model generator, with the help of the `Model` function\n",
       "    model::ModelGenerator; \n",
       "    # NamedTuple or Dict with data, required\n",
       "    data,\n",
       "    # NamedTuple with initial marginals, optional, defaults to empty\n",
       "    initmarginals = nothing,\n",
       "    # NamedTuple with initial messages, optional, defaults to empty\n",
       "    initmessages = nothing,  # optional\n",
       "    # Constraints specification object\n",
       "    constraints = nothing,\n",
       "    # Meta specification object\n",
       "    meta  = nothing,\n",
       "    # Model creation options\n",
       "    options = (;),\n",
       "    # Return structure info, optional, defaults to return everything at each iteration\n",
       "    returnvars = nothing, \n",
       "    # Number of iterations, defaults to 1, we do not distinguish between VMP or Loopy belief or EP iterations\n",
       "    iterations = 1,\n",
       "    # Do we compute FE, optional, defaults to false\n",
       "    free_energy = false,\n",
       "    # Show progress module, optional, defaults to false\n",
       "    showprogress = false,\n",
       ")\n",
       "\\end{verbatim}\n",
       "This function provides generic (but somewhat limited) way to run inference in ReactiveMP.jl. \n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "inference(\n",
       "    # `model`: specifies a model generator, with the help of the `Model` function\n",
       "    model::ModelGenerator; \n",
       "    # NamedTuple or Dict with data, required\n",
       "    data,\n",
       "    # NamedTuple with initial marginals, optional, defaults to empty\n",
       "    initmarginals = nothing,\n",
       "    # NamedTuple with initial messages, optional, defaults to empty\n",
       "    initmessages = nothing,  # optional\n",
       "    # Constraints specification object\n",
       "    constraints = nothing,\n",
       "    # Meta specification object\n",
       "    meta  = nothing,\n",
       "    # Model creation options\n",
       "    options = (;),\n",
       "    # Return structure info, optional, defaults to return everything at each iteration\n",
       "    returnvars = nothing, \n",
       "    # Number of iterations, defaults to 1, we do not distinguish between VMP or Loopy belief or EP iterations\n",
       "    iterations = 1,\n",
       "    # Do we compute FE, optional, defaults to false\n",
       "    free_energy = false,\n",
       "    # Show progress module, optional, defaults to false\n",
       "    showprogress = false,\n",
       ")\n",
       "```\n",
       "\n",
       "This function provides generic (but somewhat limited) way to run inference in ReactiveMP.jl. \n"
      ],
      "text/plain": [
       "\u001b[36m  inference(\u001b[39m\n",
       "\u001b[36m      # `model`: specifies a model generator, with the help of the `Model` function\u001b[39m\n",
       "\u001b[36m      model::ModelGenerator; \u001b[39m\n",
       "\u001b[36m      # NamedTuple or Dict with data, required\u001b[39m\n",
       "\u001b[36m      data,\u001b[39m\n",
       "\u001b[36m      # NamedTuple with initial marginals, optional, defaults to empty\u001b[39m\n",
       "\u001b[36m      initmarginals = nothing,\u001b[39m\n",
       "\u001b[36m      # NamedTuple with initial messages, optional, defaults to empty\u001b[39m\n",
       "\u001b[36m      initmessages = nothing,  # optional\u001b[39m\n",
       "\u001b[36m      # Constraints specification object\u001b[39m\n",
       "\u001b[36m      constraints = nothing,\u001b[39m\n",
       "\u001b[36m      # Meta specification object\u001b[39m\n",
       "\u001b[36m      meta  = nothing,\u001b[39m\n",
       "\u001b[36m      # Model creation options\u001b[39m\n",
       "\u001b[36m      options = (;),\u001b[39m\n",
       "\u001b[36m      # Return structure info, optional, defaults to return everything at each iteration\u001b[39m\n",
       "\u001b[36m      returnvars = nothing, \u001b[39m\n",
       "\u001b[36m      # Number of iterations, defaults to 1, we do not distinguish between VMP or Loopy belief or EP iterations\u001b[39m\n",
       "\u001b[36m      iterations = 1,\u001b[39m\n",
       "\u001b[36m      # Do we compute FE, optional, defaults to false\u001b[39m\n",
       "\u001b[36m      free_energy = false,\u001b[39m\n",
       "\u001b[36m      # Show progress module, optional, defaults to false\u001b[39m\n",
       "\u001b[36m      showprogress = false,\u001b[39m\n",
       "\u001b[36m  )\u001b[39m\n",
       "\n",
       "  This function provides generic (but somewhat limited) way to run inference\n",
       "  in ReactiveMP.jl."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:01\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Inference results:\n",
       "-----------------------------------------\n",
       "Free Energy: Real[14763.3, 3276.07, 645.285, 601.821, 601.821, 601.821, 601.821, 601.821, 601.821, 601.821]\n",
       "-----------------------------------------\n",
       "μ = Marginal(NormalWeightedMeanPrecision{Float64}(xi=-15682.23170282978, w=5204.7373...\n",
       "τ = Marginal(GammaShapeRate{Float64}(a=501.0, b=96.25864505252237))\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = inference(\n",
    "    model         = Model(test_model6, length(dataset)),\n",
    "    data          = (y = dataset, ),\n",
    "    constraints   = constraints6, \n",
    "    initmarginals = (μ = vague(NormalMeanPrecision), τ = vague(GammaShapeRate)),\n",
    "    returnvars    = (μ = KeepLast(), τ = KeepLast()),\n",
    "    iterations    = 10,\n",
    "    free_energy   = true,\n",
    "    showprogress  = true\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "μ: mean = -3.0130688119259657, std = 0.013861192405657346\n"
     ]
    }
   ],
   "source": [
    "println(\"μ: mean = \", mean(result.posteriors[:μ]), \", std = \", std(result.posteriors[:μ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "τ: mean = 5.204727323209624, std = 0.23253006806179705\n"
     ]
    }
   ],
   "source": [
    "println(\"τ: mean = \", mean(result.posteriors[:τ]), \", std = \", std(result.posteriors[:τ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For advanced use cases it is advised to write inference functions manually as it provides more flexibility, here is an example of manual inference specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, (μ, τ, y) = test_model6(constraints6, length(dataset));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For variational inference we also usually need to set initial marginals for our inference procedure. For that purpose `ReactiveMP.jl` export the `setmarginal!` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "setmarginal!(μ, vague(NormalMeanPrecision))\n",
    "setmarginal!(τ, vague(GammaShapeRate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "μ_values = keep(Marginal)\n",
    "τ_values = keep(Marginal)\n",
    "\n",
    "μ_subscription = subscribe!(getmarginal(μ), μ_values)\n",
    "τ_subscription = subscribe!(getmarginal(τ), τ_values)\n",
    "\n",
    "for i in 1:10\n",
    "    update!(y, dataset)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Marginal}:\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-3.019100750200221e-9, w=0.010000001002000566))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-27.629495773280823, w=9.179867803429282))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-10021.247182136565, w=3325.930698652331))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-15673.395541852733, w=5201.80471710224))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-15682.22287937003, w=5204.734394819007))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-15682.231694023943, w=5204.737320287038))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-15682.231702820985, w=5204.737323206587))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-15682.231702829797, w=5204.737323209579))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-15682.231702829771, w=5204.737323209567))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-15682.23170282978, w=5204.737323209567))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(μ_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Marginal}:\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=5.0000000000463575e14))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=54635.465934702996))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=150.63498062446408))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=96.31291260934161))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=96.25869921157143))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=96.25864510657314))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=96.25864505257626))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=96.25864505252225))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=96.25864505252243))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=96.25864505252237))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(τ_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "μ: mean = -3.0130688119259657, std = 0.013861192405657346\n"
     ]
    }
   ],
   "source": [
    "println(\"μ: mean = \", mean(last(μ_values)), \", std = \", std(last(μ_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "τ: mean = 5.204727323209624, std = 0.23253006806179705\n"
     ]
    }
   ],
   "source": [
    "println(\"τ: mean = \", mean(last(τ_values)), \", std = \", std(last(τ_values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form constraints\n",
    "\n",
    "In order to support form constraints, the `randomvar()` function also supports a `where { ... }` clause with some optional arguments. One of these arguments is `form_constraint` that allows us to specify a form constraint to the random variables in our model. Another one is `prod_constraint` that allows to specify an additional constraints during computation of product of two colliding messages. For example we can perform the EM algorithm if we assign a point mass contraint on some variables in our model.\n",
    "\n",
    "<img style=\"display: block;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "  width: 50%;\" src=\"./pics/posterior.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function test_model7_with_manual_constraints(n)\n",
    "    τ ~ GammaShapeRate(1.0, 1.0) \n",
    "    \n",
    "    # In case of form constraints `randomvar()` call is necessary\n",
    "    μ = randomvar() where { marginal_form_constraint = PointMassFormConstraint() }\n",
    "    μ ~ NormalMeanVariance(0.0, 100.0)\n",
    "    \n",
    "    y = datavar(Float64, n)\n",
    "    \n",
    "    for i in 1:n\n",
    "        y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) }\n",
    "    end\n",
    "    \n",
    "    return μ, τ, y\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous example we can use `@constraints` macro to achieve the same goal with a nicer syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Constraints:\n",
       "\tmarginals form:\n",
       "\t\tq(μ) :: PointMassFormConstraint() [ prod_constraint = ProdGeneric(ProdAnalytical())]\n",
       "\tmessages form:\n",
       "\tfactorisation:\n",
       "\t\tq(μ, τ) = q(μ)q(τ)\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints7 = @constraints begin \n",
    "    q(μ) :: PointMass\n",
    "    \n",
    "    q(μ, τ) = q(μ)q(τ) # Mean-Field over `μ` and `τ`\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we specified an extra constraints for $q_i$ for Bethe factorisation:\n",
    "\n",
    "$$\n",
    "q(s) = \\prod_{a \\in \\mathcal{V}} q_a(s_a) \\prod_{i \\in \\mathcal{E}} q_i^{-1}(s_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function test_model7(n)\n",
    "    τ ~ GammaShapeRate(1.0, 1.0) \n",
    "    \n",
    "    # In case of form constraints `randomvar()` call is necessary\n",
    "    μ = randomvar()\n",
    "    μ ~ NormalMeanVariance(0.0, 100.0)\n",
    "    \n",
    "    y = datavar(Float64, n)\n",
    "    \n",
    "    for i in 1:n\n",
    "        y[i] ~ NormalMeanPrecision(μ, τ)\n",
    "    end\n",
    "    \n",
    "    return μ, τ, y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, (μ, τ, y) = test_model7(constraints7, length(dataset));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "setmarginal!(μ, vague(NormalMeanPrecision))\n",
    "setmarginal!(τ, PointMass(1.0))\n",
    "\n",
    "μ_values = keep(Marginal)\n",
    "τ_values = keep(Marginal)\n",
    "\n",
    "μ_subscription = subscribe!(getmarginal(μ), μ_values)\n",
    "τ_subscription = subscribe!(getmarginal(τ), τ_values)\n",
    "\n",
    "for i in 1:10\n",
    "    update!(y, dataset)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Marginal(PointMass{Float64}(-3.0130688177034712))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(μ_values) |> last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Marginal(GammaShapeRate{Float64}(a=501.0, b=96.1625787250356))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(τ_values) |> last "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `ReactiveMP.jl` tries to compute an analytical product of two colliding messages and throws an error if no analytical solution is known. However, it is possible to fall back to a generic product that does not require an analytical solution to be known. In this case the inference backend will simply propagate the product of two message in a form of a tuple. It is not possible to use such a tuple-product during an inference and in this case it is mandatory to use some form constraint to approximate this product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```julia\n",
    "μ = randomvar() where { \n",
    "    prod_constraint = ProdGeneric(),\n",
    "    form_constraint = SampleListFormConstraint() \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is useful to preserve a specific parametrisation of the resulting product later on in an inference procedure. `ReactiveMP.jl` exports a special `prod_constraint` called `ProdPreserveType` especially for that purpose:\n",
    "\n",
    "```julia\n",
    "μ = randomvar() where { prod_constraint = ProdPreserveType(NormalWeightedMeanPrecision) }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: `@constraints` macro specifies required `prod_constraint` automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free Energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During variational inference `ReactiveMP.jl` optimises a special functional called the Bethe Free Energy functional. It is possible to obtain its values for all VMP iterations with the `score` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, (μ, τ, y) = test_model6(constraints6, length(dataset));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProxyObservable(Real, MapProxy(Tuple{ReactiveMP.InfCountingReal, ReactiveMP.InfCountingReal}))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfe_observable = score(BetheFreeEnergy(), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfe_subscription = subscribe!(bfe_observable, (fe) -> println(\"Current BFE value: \", fe));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current BFE value: 614.3116250537296\n",
      "Current BFE value: 601.8211489410164\n",
      "Current BFE value: 601.8211486920968\n",
      "Current BFE value: 601.8211486920954\n",
      "Current BFE value: 601.8211486920986\n",
      "Current BFE value: 601.8211486920968\n",
      "Current BFE value: 601.8211486920968\n",
      "Current BFE value: 601.8211486920964\n",
      "Current BFE value: 601.8211486920964\n",
      "Current BFE value: 601.8211486920968\n"
     ]
    }
   ],
   "source": [
    "# Reset the model with vague marginals\n",
    "setmarginal!(μ, vague(NormalMeanPrecision))\n",
    "setmarginal!(τ, vague(GammaShapeRate))\n",
    "\n",
    "for i in 1:10\n",
    "    update!(y, dataset)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It always necessary to unsubscribe and release computer resources\n",
    "unsubscribe!([ μ_subscription, τ_subscription, bfe_subscription ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta data specification\n",
    "\n",
    "During model specification some functional dependencies may accept an optional `meta` object in the `where { ... }` clause. The purpose of the `meta` object is to adjust, modify or supply some extra information to the inference backend during the computations of the messages. The `meta` object for example may contain an approximation method that needs to be used during various approximations or it may specify the tradeoff between accuracy and performance:\n",
    "\n",
    "```julia\n",
    "# In this example the `meta` object for the autoregressive `AR` node specifies the variate type of \n",
    "# the autoregressive process and its order. In addition it specifies that the message computation rules should\n",
    "# respect accuracy over speed with the `ARsafe()` strategy. In contrast, `ARunsafe()` strategy tries to speedup computations\n",
    "# by cost of possible numerical instabilities during an inference procedure\n",
    "s[i] ~ AR(s[i - 1], θ, γ) where { q = q(s[i - 1], s[i])q(θ)q(γ), meta = ARMeta(Multivariate, order, ARsafe()) }\n",
    "...\n",
    "s[i] ~ AR(s[i - 1], θ, γ) where { q = q(s[i - 1], s[i])q(θ)q(γ), meta = ARMeta(Univariate, order, ARunsafe()) }\n",
    "```\n",
    "\n",
    "Another example with `GaussianControlledVariance`, or simply `GCV` [see Hierarchical Gaussian Filter], node:\n",
    "\n",
    "```julia\n",
    "# In this example we specify structured factorisation and flag meta with `GaussHermiteCubature` \n",
    "# method with `21` sigma points for approximation of non-lineariety between hierarchy layers\n",
    "xt ~ GCV(xt_min, zt, real_k, real_w) where { q = q(xt, xt_min)q(zt)q(κ)q(ω), meta = GCVMetadata(GaussHermiteCubature(21)) }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Meta object is useful to pass any extra information to a node that is not a random variable or constant model variable. It may include extra approximation methods, differentiation methods, optional non-linear functions, extra inference parameters etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphPPL.jl `@meta` macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can use `@meta` macro from the `GraphPPL.jl` package to achieve the same goal. Read more about `@meta` macro in the corresponding documentation section. Here is a simple example of the same meta specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Meta specification:\n",
       "\tAR(s, θ, γ) = ARMeta{Multivariate, ARsafe}(5, ARsafe())"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@meta begin \n",
    "     AR(s, θ, γ) -> ARMeta(Multivariate, 5, ARsafe())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom nodes and message computation rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom nodes\n",
    "\n",
    "To create a custom functional form and to make it available during model specification `ReactiveMP.jl` exports the `@node` macro:\n",
    "\n",
    "```julia\n",
    "# `@node` macro accepts a name of the functional form, its type, either `Stochastic` or `Deterministic` and an array of interfaces:\n",
    "@node NormalMeanVariance Stochastic [ out, μ, v ]\n",
    "\n",
    "# Interfaces may have aliases for their names that might be convenient for factorisation constraints specification\n",
    "@node NormalMeanVariance Stochastic [ out, (μ, aliases = [ mean ]), (v, aliases = [ var ]) ]\n",
    "\n",
    "# `NormalMeanVariance` structure declaration must exist, otherwise `@node` macro will throw an error\n",
    "struct NormalMeanVariance end \n",
    "\n",
    "@node NormalMeanVariance Stochastic [ out, μ, v ]\n",
    "\n",
    "# It is also possible to use function objects as a node functional form\n",
    "function dot end\n",
    "\n",
    "# Syntax for functions is a bit differet, as it is necesssary to use `typeof(...)` function for them \n",
    "# out = dot(x, a)\n",
    "@node typeof(dot) Deterministic [ out, x, a ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Deterministic nodes do not support factorisation constraints with the `where { q = ... }` clause.\n",
    "\n",
    "After that it is possible to use the newly created node during model specification:\n",
    "\n",
    "```julia\n",
    "@model function test_model()\n",
    "    ...\n",
    "    y ~ dot(x, a)\n",
    "    ...\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom messages computation rules\n",
    "\n",
    "`ReactiveMP.jl` exports the `@rule` macro to create custom message computation rules. For example let us create a simple `+` node to be available for usage in the model specification usage. We refer to *A Factor Graph Approach to Signal Modelling , System Identification and Filtering* [ Sascha Korl, 2005, page 32 ] for a rigorous explanation of the `+` node in factor graphs. According to Korl, assuming that inputs are Gaussian Sum-Product message computation rule for `+` node is the following:\n",
    "\n",
    "$$\n",
    "\\mu_z = \\mu_x + \\mu_y \\\\\n",
    "V_z = V_x + V_y\n",
    "$$\n",
    "\n",
    "To specify this in `ReactiveMP.jl` we use the `@node` and `@rule` macros:\n",
    " \n",
    "```julia\n",
    "@node typeof(+) Deterministic  [ z, x, y ]\n",
    "\n",
    "@rule typeof(+)(:z, Marginalisation) (m_x::UnivariateNormalDistributionsFamily, m_y::UnivariateNormalDistributionsFamily) = begin\n",
    "    x_mean, x_var = mean_var(m_x)\n",
    "    y_mean, y_var = mean_var(m_y)\n",
    "    return NormalMeanVariance(x_mean + y_mean, x_var + y_var)\n",
    "end\n",
    "```\n",
    "\n",
    "In this example, for the `@rule` macro, we specify a type of our functional form: `typeof(+)`. Next, we specify an edge we are going to compute an outbound message for. `Marginalisation` indicates that the corresponding message respects the marginalisation constraint for posterior over corresponding edge:\n",
    "\n",
    "$$\n",
    "q(z) = \\int q(z, x, y) \\mathrm{d}x\\mathrm{d}y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look on difference between sum-product rules and variational rules with mean-field assumption we notice that they require different local information to compute an outgoing message:\n",
    "\n",
    "<div style=\"width:100%\">\n",
    "<div style=\"width:50%; left:25%; position: relative; padding-top: 50px; padding-bottom: 50px\">\n",
    "<img style=\"display: block;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "  width: 50%;\" src=\"./pics/sp.png\" align=\"left\"/>\n",
    "\n",
    "<img style=\"display: block;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "  width: 50%;\" src=\"./pics/vmp.png\" align=\"left\" />\n",
    "</div>\n",
    "<div style=\"width:50%\">&nbsp;&nbsp;&nbsp;&nbsp;</div>\n",
    "</div\n",
    "\n",
    "<div style=\"width:100%\">\n",
    "$$\n",
    "\\mu(z) = \\int f(x, y, z)\\mu(x)\\mu(y)\\mathrm{d}x\\mathrm{d}y\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nu(z) = \\exp{ \\int \\log f(x, y, z)q(x)q(y)\\mathrm{d}x\\mathrm{d}y }\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@rule` macro supports both cases with special prefixes during rule specification:\n",
    "- `m_` prefix corresponds to the incoming message on a specific edge\n",
    "- `q_` prefix corresponds to the posterior marginal of a specific edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a Sum-Product rule with `m_` messages used:\n",
    "\n",
    "```julia\n",
    "@rule NormalMeanPrecision(:μ, Marginalisation) (m_out::UnivariateNormalDistributionsFamily, m_τ::PointMass) = begin \n",
    "    m_out_mean, m_out_cov = mean_cov(m_out)\n",
    "    return NormalMeanPrecision(m_out_mean, inv(m_out_cov + inv(mean(m_τ))))\n",
    "end\n",
    "```\n",
    "\n",
    "Example of a Variational rule with Mean-Field assumption with `q_` posteriors used:\n",
    "\n",
    "```julia\n",
    "@rule NormalMeanPrecision(:μ, Marginalisation) (q_out::Any, q_τ::Any) = begin \n",
    "    return NormalMeanPrecision(mean(q_out), mean(q_τ))\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ReactiveMP.jl` also supports structured rules. It is possible to obtain joint marginal over a set of edges:\n",
    "\n",
    "```julia\n",
    "@rule NormalMeanPrecision(:τ, Marginalisation) (q_out_μ::Any, ) = begin\n",
    "    m, V = mean_cov(q_out_μ)\n",
    "    θ = 2 / (V[1,1] - V[1,2] - V[2,1] + V[2,2] + abs2(m[1] - m[2]))\n",
    "    α = convert(typeof(θ), 1.5)\n",
    "    return Gamma(α, θ)\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: In the `@rule` specification the messages or marginals arguments **must** be in order with interfaces specification from `@node` macro:\n",
    "\n",
    "```julia\n",
    "# Inference backend expects arguments in `@rule` macro to be in the same order\n",
    "@node NormalMeanPrecision Stochastic [ out, μ, τ ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any rule always has access to the meta information with hidden the `meta::Any` variable:\n",
    "\n",
    "```julia\n",
    "@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any) = begin \n",
    "    ...\n",
    "    println(meta)\n",
    "    ...\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to dispatch on a specific type of a meta object:\n",
    "\n",
    "```julia\n",
    "@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::LaplaceApproximation) = begin \n",
    "    ...\n",
    "end\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```julia\n",
    "@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::GaussHermiteCubature) = begin \n",
    "    ...\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing messages computational pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In certain situations it might be convenient to customize the default message computational pipeline. `GrahpPPL.jl` supports the `pipeline` keyword in the `where { ... }` clause to add some extra steps after a message has been computed. A use case might be an extra approximation method to preserve conjugacy in the model, debugging or simple printing.\n",
    "\n",
    "<img style=\"display: block;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "  width: 30%;\" src=\"./pics/pipeline.png\" width=\"20%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```julia\n",
    "# Logs all outbound messages\n",
    "y[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = LoggerPipelineStage() }\n",
    "# Initialise messages to be vague\n",
    "y[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = InitVaguePipelineStage() }\n",
    "# In principle, it is possible to approximate outbound messages with Laplace Approximation\n",
    "y[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = LaplaceApproximation() }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us return to the coin toss model, but this time we want to print flowing messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model [ default_factorisation = FullFactorisation() ] function coin_toss_model_log(n)\n",
    "\n",
    "    y = datavar(Float64, n)\n",
    "\n",
    "    θ ~ Beta(2.0, 7.0) where { pipeline = LoggerPipelineStage(\"θ\") }\n",
    "\n",
    "    for i in 1:n\n",
    "        y[i] ~ Bernoulli(θ)  where { pipeline = LoggerPipelineStage(\"y[$i]\") }\n",
    "    end\n",
    "    \n",
    "    return y, θ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (y, θ) = coin_toss_model_log(5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[θ][Beta][out]: Message(Beta{Float64}(α=2.0, β=7.0))\n"
     ]
    }
   ],
   "source": [
    "θ_subscription = subscribe!(getmarginal(θ), (value) -> println(\"New posterior marginal for θ: \", value));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "coinflips = float.(rand(Bernoulli(0.5), 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[y[1]][Bernoulli][p]: Message(Beta{Float64}(α=2.0, β=1.0))\n",
      "[y[2]][Bernoulli][p]: Message(Beta{Float64}(α=1.0, β=2.0))\n",
      "[y[3]][Bernoulli][p]: Message(Beta{Float64}(α=1.0, β=2.0))\n",
      "[y[4]][Bernoulli][p]: Message(Beta{Float64}(α=2.0, β=1.0))\n",
      "[y[5]][Bernoulli][p]: Message(Beta{Float64}(α=2.0, β=1.0))\n",
      "New posterior marginal for θ: Marginal(Beta{Float64}(α=5.0, β=9.0))\n"
     ]
    }
   ],
   "source": [
    "update!(y, coinflips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsubscribe!(θ_subscription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference is lazy and does not send messages if no one is listening for them\n",
    "update!(y, coinflips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
