diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml
index 8d5d2580..93d16876 100644
--- a/.github/workflows/ci.yml
+++ b/.github/workflows/ci.yml
@@ -1,9 +1,17 @@
 name: CI
 on:
   pull_request:
+    types: [ready_for_review,reopened,synchronize]
+  pull_request_review:
+    types: [submitted,edited]
   push:
+    branches:
+      - 'master'
+    tags: '*'
+  check_run:
+    types: [rerequested]
   schedule:
-    - cron: '44 9 16 * *' # run the cron job one time per month
+    - cron: '0 8 * * 1' # run the cron job one time per week on Monday 8:00 AM
 jobs:
   format: 
     name: Julia Formatter
@@ -17,6 +25,7 @@ jobs:
   test:
     name: Julia ${{ matrix.version }} - ${{ matrix.os }} - ${{ matrix.arch }} - ${{ github.event_name }}
     runs-on: ${{ matrix.os }}
+    continue-on-error: ${{ contains(matrix.version, 'nightly') }}
     needs: format
     strategy:
       fail-fast: false
@@ -110,4 +119,4 @@ jobs:
         env:
           PYTHON: ""
           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
-          DOCUMENTER_KEY: ${{ secrets.DOCUMENTER_KEY }}
\ No newline at end of file
+          DOCUMENTER_KEY: ${{ secrets.DOCUMENTER_KEY }}
diff --git a/Makefile b/Makefile
index 95a2851e..231f80bb 100644
--- a/Makefile
+++ b/Makefile
@@ -30,8 +30,8 @@ docs: doc_init ## Generate documentation
 
 .PHONY: test
 
-test: ## Run tests (use testset="folder1:test1 folder2:test2" argument to run reduced testset)
-	julia -e 'import Pkg; Pkg.activate("."); Pkg.test(test_args = split("$(testset)") .|> string)'	
+test: ## Run tests (use test_args="folder1:test1 folder2:test2" argument to run reduced testset)
+	julia -e 'import Pkg; Pkg.activate("."); Pkg.test(test_args = split("$(test_args)") .|> string)'	
 	
 help:  ## Display this help
 	@awk 'BEGIN {FS = ":.*##"; printf "\nUsage:\n  make \033[36m\033[0m\n"} /^[a-zA-Z_-]+:.*?##/ { printf "  \033[36m%-24s\033[0m %s\n", $$1, $$2 } /^##@/ { printf "\n\033[1m%s\033[0m\n", substr($$0, 5) } ' $(MAKEFILE_LIST)
\ No newline at end of file
diff --git a/Project.toml b/Project.toml
index d63ca068..c4754725 100644
--- a/Project.toml
+++ b/Project.toml
@@ -1,10 +1,9 @@
 name = "ReactiveMP"
 uuid = "a194aa59-28ba-4574-a09c-4a745416d6e3"
 authors = ["Dmitry Bagaev <d.v.bagaev@tue.nl>", "Albert Podusenko <a.podusenko@tue.nl>", "Bart van Erp <b.v.erp@tue.nl>", "Ismail Senoz <i.senoz@tue.nl>"]
-version = "3.6.1"
+version = "3.7.2"
 
 [deps]
-BlockArrays = "8e7c35d0-a365-5155-bbbb-fb81a777f24e"
 DataStructures = "864edb3b-99cc-5e75-8d2d-829cb0a9cfe8"
 Distributions = "31c24e10-a181-5473-b8eb-7969acd0382f"
 DomainIntegrals = "cc6bae93-f070-4015-88fd-838f9505a86c"
@@ -43,7 +42,7 @@ MacroTools = "0.5"
 Optim = "1.0.0"
 PositiveFactorizations = "0.2"
 Requires = "1"
-Rocket = "1.6.0"
+Rocket = "1.7.0"
 SpecialFunctions = "1.4, 2"
 StaticArrays = "1.2"
 StatsBase = "0.33"
diff --git a/docs/src/extra/contributing.md b/docs/src/extra/contributing.md
index c9e0e317..9a84ca4c 100644
--- a/docs/src/extra/contributing.md
+++ b/docs/src/extra/contributing.md
@@ -81,8 +81,8 @@ a new release of the broken dependecy is available.
 
 - `make help`: Shows help snippet
 - `make test`: Run tests, supports extra arguments
-  - `make test testset="distributions:normal_mean_variance"` would run tests only from `distributions/test_normal_mean_variance.jl`
-  - `make test testset="distributions:normal_mean_variance models:lgssm"` would run tests both from `distributions/test_normal_mean_variance.jl` and `models/test_lgssm.jl`
+  - `make test test_args="distributions:normal_mean_variance"` would run tests only from `distributions/test_normal_mean_variance.jl`
+  - `make test test_args="distributions:normal_mean_variance models:lgssm"` would run tests both from `distributions/test_normal_mean_variance.jl` and `models/test_lgssm.jl`
 - `make docs`: Compile documentation
 - `make benchmark`: Run simple benchmark
 - `make lint`: Check codestyle
diff --git a/src/ReactiveMP.jl b/src/ReactiveMP.jl
index 7b5c860c..fc0186b4 100644
--- a/src/ReactiveMP.jl
+++ b/src/ReactiveMP.jl
@@ -146,7 +146,6 @@ include("nodes/matrix_dirichlet.jl")
 include("nodes/dirichlet.jl")
 include("nodes/bernoulli.jl")
 include("nodes/gcv.jl")
-include("nodes/kernel_gcv.jl")
 include("nodes/wishart.jl")
 include("nodes/wishart_inverse.jl")
 include("nodes/normal_mixture.jl")
@@ -154,7 +153,6 @@ include("nodes/gamma_mixture.jl")
 include("nodes/dot_product.jl")
 include("nodes/transition.jl")
 include("nodes/autoregressive.jl")
-include("nodes/mv_autoregressive.jl")
 include("nodes/bifm.jl")
 include("nodes/bifm_helper.jl")
 include("nodes/probit.jl")
diff --git a/src/constraints/form.jl b/src/constraints/form.jl
index a746cdd6..c6c2edc4 100644
--- a/src/constraints/form.jl
+++ b/src/constraints/form.jl
@@ -172,7 +172,7 @@ function is_point_mass_form_constraint(composite::CompositeFormConstraint)
     is_point_mass = map(is_point_mass_form_constraint, composite.constraints)
     pmindex       = findnext(is_point_mass, 1)
     if pmindex !== nothing && pmindex !== length(is_point_mass)
-        error("Composite form constraint supports point mass constraint only at the end of the form constrains specification.")
+        error("Composite form constraint supports point mass constraint only at the end of the form constraints specification.")
     end
     return last(is_point_mass)
 end
diff --git a/src/distributions/bernoulli.jl b/src/distributions/bernoulli.jl
index 9757f0aa..bb30b485 100644
--- a/src/distributions/bernoulli.jl
+++ b/src/distributions/bernoulli.jl
@@ -47,6 +47,10 @@ function prod(::ProdAnalytical, left::Bernoulli, right::Categorical)
     return Categorical(ReactiveMP.normalize!(p_new, 1))
 end
 
+prod_analytical_rule(::Type{<:Categorical}, ::Type{<:Bernoulli}) = ProdAnalyticalRuleAvailable()
+
+prod(::ProdAnalytical, left::Categorical, right::Bernoulli) = prod(ProdAnalytical(), right, left)
+
 function compute_logscale(new_dist::Bernoulli, left_dist::Bernoulli, right_dist::Bernoulli)
     left_p = succprob(left_dist)
     right_p = succprob(right_dist)
diff --git a/src/distributions/beta.jl b/src/distributions/beta.jl
index 9541c90a..129c4f7f 100644
--- a/src/distributions/beta.jl
+++ b/src/distributions/beta.jl
@@ -1,7 +1,9 @@
 export Beta
+export BetaNaturalParameters
 
 import Distributions: Beta, params
-import SpecialFunctions: digamma, logbeta
+import SpecialFunctions: digamma, logbeta, loggamma
+import StatsFuns: betalogpdf
 
 vague(::Type{<:Beta}) = Beta(1.0, 1.0)
 
@@ -27,3 +29,47 @@ function mean(::typeof(mirrorlog), dist::Beta)
     a, b = params(dist)
     return digamma(b) - digamma(a + b)
 end
+
+struct BetaNaturalParameters{T <: Real} <: NaturalParameters
+    αm1::T
+    βm1::T
+end
+
+BetaNaturalParameters(αm1::Real, βm1::Real)       = BetaNaturalParameters(promote(αm1, βm1)...)
+BetaNaturalParameters(αm1::Integer, βm1::Integer) = BetaNaturalParameters(float(αm1), float(βm1))
+
+Base.convert(::Type{BetaNaturalParameters}, a::Real, b::Real) = convert(BetaNaturalParameters{promote_type(typeof(a), typeof(b))}, a, b)
+
+Base.convert(::Type{BetaNaturalParameters{T}}, a::Real, b::Real) where {T} = BetaNaturalParameters(convert(T, a), convert(T, b))
+
+Base.convert(::Type{BetaNaturalParameters}, vec::AbstractVector) = convert(BetaNaturalParameters{eltype(vec)}, vec)
+
+Base.convert(::Type{BetaNaturalParameters{T}}, vec::AbstractVector) where {T} = BetaNaturalParameters(convert(AbstractVector{T}, vec))
+
+function isproper(params::BetaNaturalParameters)
+    return ((params.αm1 + 1) > 0) && ((params.βm1 + 1) > 0)
+end
+
+naturalparams(dist::Beta) = BetaNaturalParameters(dist.α - 1, dist.β - 1)
+
+function Base.convert(::Type{Distribution}, η::BetaNaturalParameters)
+    return Beta(η.αm1 + 1, η.βm1 + 1, check_args = false)
+end
+
+function Base.vec(p::BetaNaturalParameters)
+    return [p.αm1, p.βm1]
+end
+
+ReactiveMP.as_naturalparams(::Type{T}, args...) where {T <: BetaNaturalParameters} = convert(BetaNaturalParameters, args...)
+
+function BetaNaturalParameters(v::AbstractVector{T}) where {T <: Real}
+    @assert length(v) === 2 "`BetaNaturalParameters` must accept a vector of length `2`."
+    return BetaNaturalParameters(v[1], v[2])
+end
+
+lognormalizer(params::BetaNaturalParameters) = logbeta(params.αm1 + 1, params.βm1 + 1)
+logpdf(params::BetaNaturalParameters, x) = betalogpdf(params.αm1 + 1, params.βm1 + 1, x)
+
+function Base.:-(left::BetaNaturalParameters, right::BetaNaturalParameters)
+    return BetaNaturalParameters(left.αm1 - right.αm1, left.βm1 - right.βm1)
+end
diff --git a/src/distributions/mv_normal_mean_covariance.jl b/src/distributions/mv_normal_mean_covariance.jl
index 7938bf16..079016ca 100644
--- a/src/distributions/mv_normal_mean_covariance.jl
+++ b/src/distributions/mv_normal_mean_covariance.jl
@@ -26,6 +26,13 @@ function MvNormalMeanCovariance(μ::AbstractVector{T}) where {T}
     return MvNormalMeanCovariance(μ, convert(AbstractArray{T}, ones(length(μ))))
 end
 
+function MvNormalMeanCovariance(μ::AbstractVector{T1}, Σ::UniformScaling{T2}) where {T1, T2}
+    T = promote_type(T1, T2)
+    μ_new = convert(AbstractArray{T}, μ)
+    Σ_new = convert(UniformScaling{T}, Σ)(length(μ))
+    return MvNormalMeanCovariance(μ_new, Σ_new)
+end
+
 Distributions.distrname(::MvNormalMeanCovariance) = "MvNormalMeanCovariance"
 
 function weightedmean(dist::MvNormalMeanCovariance)
@@ -88,7 +95,9 @@ function Base.prod(::ProdAnalytical, left::MvNormalMeanCovariance, right::MvNorm
     return MvNormalWeightedMeanPrecision(xi_left + xi_right, W_left + W_right)
 end
 
-function Base.prod(::ProdAnalytical, left::MvNormalMeanCovariance{T1}, right::MvNormalMeanCovariance{T2}) where {T1 <: LinearAlgebra.BlasFloat, T2 <: LinearAlgebra.BlasFloat}
+function Base.prod(
+    ::ProdAnalytical, left::MvNormalMeanCovariance{T1, <:AbstractVector, <:Matrix}, right::MvNormalMeanCovariance{T2, <:AbstractVector, <:Matrix}
+) where {T1 <: LinearAlgebra.BlasFloat, T2 <: LinearAlgebra.BlasFloat}
 
     # start with parameters of left
     xi, W = weightedmean_precision(left)
diff --git a/src/distributions/mv_normal_mean_precision.jl b/src/distributions/mv_normal_mean_precision.jl
index 889b0014..815eb424 100644
--- a/src/distributions/mv_normal_mean_precision.jl
+++ b/src/distributions/mv_normal_mean_precision.jl
@@ -26,6 +26,13 @@ function MvNormalMeanPrecision(μ::AbstractVector{T}) where {T}
     return MvNormalMeanPrecision(μ, convert(AbstractArray{T}, ones(length(μ))))
 end
 
+function MvNormalMeanPrecision(μ::AbstractVector{T1}, Λ::UniformScaling{T2}) where {T1, T2}
+    T = promote_type(T1, T2)
+    μ_new = convert(AbstractArray{T}, μ)
+    Λ_new = convert(UniformScaling{T}, Λ)(length(μ))
+    return MvNormalMeanPrecision(μ_new, Λ_new)
+end
+
 Distributions.distrname(::MvNormalMeanPrecision) = "MvNormalMeanPrecision"
 
 weightedmean(dist::MvNormalMeanPrecision) = precision(dist) * mean(dist)
@@ -92,7 +99,9 @@ function Base.prod(::ProdAnalytical, left::MvNormalMeanPrecision, right::MvNorma
     return MvNormalWeightedMeanPrecision(xi, W)
 end
 
-function Base.prod(::ProdAnalytical, left::MvNormalMeanPrecision{T1}, right::MvNormalMeanPrecision{T2}) where {T1 <: LinearAlgebra.BlasFloat, T2 <: LinearAlgebra.BlasFloat}
+function Base.prod(
+    ::ProdAnalytical, left::MvNormalMeanPrecision{T1, <:AbstractVector, <:Matrix}, right::MvNormalMeanPrecision{T2, <:AbstractVector, <:Matrix}
+) where {T1 <: LinearAlgebra.BlasFloat, T2 <: LinearAlgebra.BlasFloat}
     W = precision(left) + precision(right)
 
     # fast & efficient implementation of xi = precision(right)*mean(right) + precision(left)*mean(left)
diff --git a/src/distributions/mv_normal_weighted_mean_precision.jl b/src/distributions/mv_normal_weighted_mean_precision.jl
index 86aa046e..7137ccb4 100644
--- a/src/distributions/mv_normal_weighted_mean_precision.jl
+++ b/src/distributions/mv_normal_weighted_mean_precision.jl
@@ -26,6 +26,13 @@ function MvNormalWeightedMeanPrecision(xi::AbstractVector{T}) where {T}
     return MvNormalWeightedMeanPrecision(xi, convert(AbstractArray{T}, ones(length(xi))))
 end
 
+function MvNormalWeightedMeanPrecision(xi::AbstractVector{T1}, Λ::UniformScaling{T2}) where {T1, T2}
+    T = promote_type(T1, T2)
+    xi_new = convert(AbstractArray{T}, xi)
+    Λ_new = convert(UniformScaling{T}, Λ)(length(xi))
+    return MvNormalWeightedMeanPrecision(xi_new, Λ_new)
+end
+
 Distributions.distrname(::MvNormalWeightedMeanPrecision) = "MvNormalWeightedMeanPrecision"
 
 weightedmean(dist::MvNormalWeightedMeanPrecision) = dist.xi
diff --git a/src/distributions/normal_mean_precision.jl b/src/distributions/normal_mean_precision.jl
index d6b9ef8a..b5083756 100644
--- a/src/distributions/normal_mean_precision.jl
+++ b/src/distributions/normal_mean_precision.jl
@@ -11,6 +11,12 @@ NormalMeanPrecision(μ::Real, w::Real)       = NormalMeanPrecision(promote(μ, w
 NormalMeanPrecision(μ::Integer, w::Integer) = NormalMeanPrecision(float(μ), float(w))
 NormalMeanPrecision(μ::Real)                = NormalMeanPrecision(μ, one(μ))
 NormalMeanPrecision()                       = NormalMeanPrecision(0.0, 1.0)
+function NormalMeanPrecision(μ::T1, w::UniformScaling{T2}) where {T1 <: Real, T2}
+    T = promote_type(T1, T2)
+    μ_new = convert(T, μ)
+    w_new = convert(T, w.λ)
+    return NormalMeanPrecision(μ_new, w_new)
+end
 
 Distributions.@distr_support NormalMeanPrecision -Inf Inf
 
diff --git a/src/distributions/normal_mean_variance.jl b/src/distributions/normal_mean_variance.jl
index 759f319d..2b1770b9 100644
--- a/src/distributions/normal_mean_variance.jl
+++ b/src/distributions/normal_mean_variance.jl
@@ -11,6 +11,12 @@ NormalMeanVariance(μ::Real, v::Real)       = NormalMeanVariance(promote(μ, v).
 NormalMeanVariance(μ::Integer, v::Integer) = NormalMeanVariance(float(μ), float(v))
 NormalMeanVariance(μ::T) where {T <: Real} = NormalMeanVariance(μ, one(T))
 NormalMeanVariance()                       = NormalMeanVariance(0.0, 1.0)
+function NormalMeanVariance(μ::T1, v::UniformScaling{T2}) where {T1 <: Real, T2}
+    T = promote_type(T1, T2)
+    μ_new = convert(T, μ)
+    v_new = convert(T, v.λ)
+    return NormalMeanVariance(μ_new, v_new)
+end
 
 Distributions.@distr_support NormalMeanVariance -Inf Inf
 
diff --git a/src/distributions/normal_weighted_mean_precision.jl b/src/distributions/normal_weighted_mean_precision.jl
index f68a6abe..c17f9fb3 100644
--- a/src/distributions/normal_weighted_mean_precision.jl
+++ b/src/distributions/normal_weighted_mean_precision.jl
@@ -11,6 +11,12 @@ NormalWeightedMeanPrecision(xi::Real, w::Real)       = NormalWeightedMeanPrecisi
 NormalWeightedMeanPrecision(xi::Integer, w::Integer) = NormalWeightedMeanPrecision(float(xi), float(w))
 NormalWeightedMeanPrecision(xi::Real)                = NormalWeightedMeanPrecision(xi, one(xi))
 NormalWeightedMeanPrecision()                        = NormalWeightedMeanPrecision(0.0, 1.0)
+function NormalWeightedMeanPrecision(xi::T1, w::UniformScaling{T2}) where {T1 <: Real, T2}
+    T = promote_type(T1, T2)
+    xi_new = convert(T, xi)
+    w_new = convert(T, w.λ)
+    return NormalWeightedMeanPrecision(xi_new, w_new)
+end
 
 Distributions.@distr_support NormalWeightedMeanPrecision -Inf Inf
 
diff --git a/src/distributions/pointmass.jl b/src/distributions/pointmass.jl
index 38c15c24..c4612623 100644
--- a/src/distributions/pointmass.jl
+++ b/src/distributions/pointmass.jl
@@ -1,5 +1,7 @@
 export PointMass, getpointmass
 
+using LinearAlgebra: UniformScaling, I
+
 import Distributions: mean, var, cov, std, insupport, pdf, logpdf, entropy
 import Base: ndims, precision, getindex, size, convert, isapprox, eltype
 import SpecialFunctions: loggamma, logbeta
@@ -13,12 +15,14 @@ end
 variate_form(::PointMass{T}) where {T <: Real}                 = Univariate
 variate_form(::PointMass{V}) where {T, V <: AbstractVector{T}} = Multivariate
 variate_form(::PointMass{M}) where {T, M <: AbstractMatrix{T}} = Matrixvariate
+variate_form(::PointMass{U}) where {T, U <: UniformScaling{T}} = Matrixvariate
 
 ##
 
 sampletype(distribution::PointMass{T}) where {T} = T
 
 getpointmass(distribution::PointMass) = distribution.point
+getpointmass(point::Union{Real, AbstractArray}) = point
 
 ##
 
@@ -111,6 +115,31 @@ convert_eltype(::Type{PointMass}, ::Type{T}, distribution::PointMass{R}) where {
 
 Base.eltype(::PointMass{M}) where {T <: Real, M <: AbstractMatrix{T}} = T
 
+# UniformScaling-based matrixvariate point mass
+
+Distributions.insupport(distribution::PointMass{M}, x::UniformScaling) where {T <: Real, M <: UniformScaling{T}} = x == getpointmass(distribution)
+Distributions.pdf(distribution::PointMass{M}, x::UniformScaling) where {T <: Real, M <: UniformScaling{T}}       = Distributions.insupport(distribution, x) ? one(T) : zero(T)
+Distributions.logpdf(distribution::PointMass{M}, x::UniformScaling) where {T <: Real, M <: UniformScaling{T}}    = Distributions.insupport(distribution, x) ? zero(T) : convert(T, -Inf)
+
+Distributions.mean(distribution::PointMass{M}) where {T <: Real, M <: UniformScaling{T}} = getpointmass(distribution)
+Distributions.mode(distribution::PointMass{M}) where {T <: Real, M <: UniformScaling{T}} = mean(distribution)
+Distributions.var(distribution::PointMass{M}) where {T <: Real, M <: UniformScaling{T}}  = zero(T) * I
+Distributions.std(distribution::PointMass{M}) where {T <: Real, M <: UniformScaling{T}}  = zero(T) * I
+Distributions.cov(distribution::PointMass{M}) where {T <: Real, M <: UniformScaling{T}}  = error("Distributions.cov(::PointMass{ <: UniformScaling }) is not defined")
+
+probvec(distribution::PointMass{M}) where {T <: Real, M <: UniformScaling{T}} = error("probvec(::PointMass{ <: UniformScaling }) is not defined")
+
+mean(::typeof(inv), distribution::PointMass{M}) where {T <: Real, M <: UniformScaling{T}}     = inv(mean(distribution))
+mean(::typeof(cholinv), distribution::PointMass{M}) where {T <: Real, M <: UniformScaling{T}} = inv(mean(distribution))
+
+Base.precision(distribution::PointMass{M}) where {T <: Real, M <: UniformScaling{T}} = one(T) ./ cov(distribution)
+Base.ndims(distribution::PointMass{M}) where {T <: Real, M <: UniformScaling{T}}     = size(mean(distribution))
+
+convert_eltype(::Type{PointMass}, ::Type{T}, distribution::PointMass{R}) where {T <: Real, R <: UniformScaling}           = PointMass(convert(AbstractMatrix{T}, getpointmass(distribution)))
+convert_eltype(::Type{PointMass}, ::Type{T}, distribution::PointMass{R}) where {T <: AbstractMatrix, R <: UniformScaling} = PointMass(convert(T, getpointmass(distribution)))
+
+Base.eltype(::PointMass{M}) where {T <: Real, M <: UniformScaling{T}} = T
+
 Base.isapprox(left::PointMass, right::PointMass; kwargs...) = Base.isapprox(getpointmass(left), getpointmass(right); kwargs...)
 Base.isapprox(left::PointMass, right; kwargs...) = false
 Base.isapprox(left, right::PointMass; kwargs...) = false
diff --git a/src/marginal.jl b/src/marginal.jl
index 72cd8819..75a39143 100644
--- a/src/marginal.jl
+++ b/src/marginal.jl
@@ -109,6 +109,11 @@ struct SkipInitial <: MarginalSkipStrategy end
 struct SkipClampedAndInitial <: MarginalSkipStrategy end
 struct IncludeAll <: MarginalSkipStrategy end
 
+Base.broadcastable(::SkipClamped) = Ref(SkipClamped())
+Base.broadcastable(::SkipInitial) = Ref(SkipInitial())
+Base.broadcastable(::SkipClampedAndInitial) = Ref(SkipClampedAndInitial())
+Base.broadcastable(::IncludeAll) = Ref(IncludeAll())
+
 apply_skip_filter(observable, ::SkipClamped)           = observable |> filter(v -> !is_clamped(v))
 apply_skip_filter(observable, ::SkipInitial)           = observable |> filter(v -> !is_initial(v))
 apply_skip_filter(observable, ::SkipClampedAndInitial) = observable |> filter(v -> !is_initial(v) && !is_clamped(v))
diff --git a/src/node.jl b/src/node.jl
index 56b5e5e1..605d0c78 100644
--- a/src/node.jl
+++ b/src/node.jl
@@ -866,7 +866,7 @@ function activate!(factornode::AbstractFactorNode, options)
             vmessageout = combineLatest((msgs_observable, marginals_observable), PushNew())  # TODO check PushEach
             vmessageout = apply_pipeline_stage(get_pipeline_stages(interface), factornode, vtag, vmessageout)
 
-            mapping = let messagemap = MessageMapping(fform, vtag, vconstraint, msgs_names, marginal_names, meta, addons, factornode)
+            mapping = let messagemap = MessageMapping(fform, vtag, vconstraint, msgs_names, marginal_names, meta, addons, node_if_required(fform, factornode))
                 (dependencies) -> VariationalMessage(dependencies[1], dependencies[2], messagemap)
             end
 
@@ -939,11 +939,11 @@ function getmarginal!(factornode::FactorNode, localmarginal::FactorNodeLocalMarg
         vtag  = Val{name(localmarginal)}
         meta  = metadata(factornode)
 
-        mapping = MarginalMapping(fform, vtag, msgs_names, marginal_names, meta, factornode)
+        mapping = MarginalMapping(fform, vtag, msgs_names, marginal_names, meta, node_if_required(fform, factornode))
         # TODO: discontinue operator is needed for loopy belief propagation? Check
         marginalout = combineLatest((msgs_observable, marginals_observable), PushNew()) |> discontinue() |> map(Marginal, mapping)
 
-        connect!(cmarginal, marginalout) # MarginalObservable has RecentSubject by default, there is no need to share_recent() here
+        connect!(cmarginal, marginalout)
 
         return apply_skip_filter(cmarginal, skip_strategy)
     end
@@ -955,7 +955,7 @@ end
     make_node(node)
     make_node(node, options)
 
-Creates a factor node of a given type and options. See the list of avaialble factor nodes below.
+Creates a factor node of a given type and options. See the list of available factor nodes below.
 
 See also: [`@node`](@ref)
 
diff --git a/src/nodes/autoregressive.jl b/src/nodes/autoregressive.jl
index 4e80c275..07a620fe 100644
--- a/src/nodes/autoregressive.jl
+++ b/src/nodes/autoregressive.jl
@@ -1,4 +1,4 @@
-export AR, Autoregressive, ARsafe, ARunsafe, ARMeta, ar_unit, ar_slice
+export AR, Autoregressive, ARsafe, ARunsafe, ARMeta
 
 import LazyArrays
 import Distributions: VariateForm
diff --git a/src/nodes/delta/delta.jl b/src/nodes/delta/delta.jl
index d302ee7c..cfe34369 100644
--- a/src/nodes/delta/delta.jl
+++ b/src/nodes/delta/delta.jl
@@ -79,8 +79,11 @@ end
 # For missing rules error msg
 rule_method_error_extract_fform(f::Type{<:DeltaFn}) = "DeltaFn{f}"
 
+# `DeltaFn` requires an access to the node function, hence, node reference is required
+call_rule_is_node_required(::Type{<:DeltaFn}) = CallRuleNodeRequired()
+
 # For `@call_rule` and `@call_marginalrule`
-function call_rule_make_node(::UndefinedNodeFunctionalForm, fformtype::Type{<:DeltaFn}, nodetype::F, meta::DeltaMeta) where {F}
+function call_rule_make_node(::CallRuleNodeRequired, fformtype::Type{<:DeltaFn}, nodetype::F, meta::DeltaMeta) where {F}
     # This node is not initialized properly, but we do not expect rules to access internal uninitialized fields.
     # Doing so will most likely throw an error
     return DeltaFnNode(nodetype, NodeInterface(:out, Marginalisation()), (), nothing, collect_meta(DeltaFn{F}, meta))
diff --git a/src/nodes/kernel_gcv.jl b/src/nodes/kernel_gcv.jl
deleted file mode 100644
index 8434eacc..00000000
--- a/src/nodes/kernel_gcv.jl
+++ /dev/null
@@ -1,34 +0,0 @@
-export KernelGCV, KernelGCVMetadata
-
-import LinearAlgebra: logdet, tr
-
-struct KernelGCVMetadata{F, A}
-    kernelFn      :: F
-    approximation :: A
-end
-
-get_kernelfn(meta::KernelGCVMetadata)      = meta.kernelFn
-get_approximation(meta::KernelGCVMetadata) = meta.approximation
-
-struct KernelGCV end
-
-@node KernelGCV Stochastic [y, x, z]
-
-# TODO: Remove in favor of Generic Functional Message
-struct FnWithApproximation{F, A}
-    fn            :: F
-    approximation :: A
-end
-
-prod_analytical_rule(::Type{<:MultivariateNormalDistributionsFamily}, ::Type{<:FnWithApproximation}) = ProdAnalyticalRuleAvailable()
-
-function prod(::ProdAnalytical, left::MultivariateNormalDistributionsFamily, right::FnWithApproximation)
-    μ, Σ = approximate_meancov(right.approximation, (s) -> exp(right.fn(s)), left)
-    return MvNormalMeanCovariance(μ, Σ)
-end
-
-prod_analytical_rule(::Type{<:FnWithApproximation}, ::Type{<:MultivariateNormalDistributionsFamily}) = ProdAnalyticalRuleAvailable()
-
-function prod(::ProdAnalytical, left::FnWithApproximation, right::MultivariateNormalDistributionsFamily)
-    return prod(ProdAnalytical(), right, left)
-end
diff --git a/src/nodes/mv_autoregressive.jl b/src/nodes/mv_autoregressive.jl
deleted file mode 100644
index 1cc739b5..00000000
--- a/src/nodes/mv_autoregressive.jl
+++ /dev/null
@@ -1,156 +0,0 @@
-export MAR, MvAutoregressive, MARMeta, mar_transition, mar_shift
-
-import LazyArrays, BlockArrays
-import StatsFuns: log2π
-
-struct MAR end
-
-const MvAutoregressive = MAR
-
-struct MARMeta
-    order :: Int # order (lag) of MAR
-    ds    :: Int # dimensionality of MAR process, i.e., the number of correlated AR processes
-
-    function MARMeta(order, ds = 2)
-        if ds < 2
-            @error "ds parameter should be > 1. Use AR node if ds = 1"
-        end
-        return new(order, ds)
-    end
-end
-
-getorder(meta::MARMeta)          = meta.order
-getdimensionality(meta::MARMeta) = meta.ds
-
-@node MAR Stochastic [y, x, a, Λ]
-
-default_meta(::Type{MAR}) = error("MvAutoregressive node requires meta flag explicitly specified")
-
-@average_energy MAR (q_y_x::MultivariateNormalDistributionsFamily, q_a::MultivariateNormalDistributionsFamily, q_Λ::Wishart, meta::MARMeta) = begin
-    ma, Va   = mean_cov(q_a)
-    myx, Vyx = mean_cov(q_y_x)
-    mΛ       = mean(q_Λ)
-
-    order, ds = getorder(meta), getdimensionality(meta)
-    F = Multivariate
-    dim = order * ds
-    n = div(ndims(q_y_x), 2)
-
-    ma, Va = mean_cov(q_a)
-    mA = mar_companion_matrix(order, ds, ma)[1:ds, 1:dim]
-
-    mx, Vx   = ar_slice(F, myx, (dim + 1):(2dim)), ar_slice(F, Vyx, (dim + 1):(2dim), (dim + 1):(2dim))
-    my1, Vy1 = myx[1:ds], Vyx[1:ds, 1:ds]
-    Vy1x     = ar_slice(F, Vyx, 1:ds, (dim + 1):(2dim))
-
-    # @show Vyx
-    # @show Vy1x
-
-    # this should be inside MARMeta
-    es = [uvector(ds, i) for i in 1:ds]
-    Fs = [mask_mar(order, ds, i) for i in 1:ds]
-
-    g₁ = my1' * mΛ * my1 + tr(Vy1 * mΛ)
-    g₂ = mx' * mA' * mΛ * my1 + tr(Vy1x * mA' * mΛ)
-    g₃ = g₂
-    G = sum(sum(es[i]' * mΛ * es[j] * Fs[i] * (ma * ma' + Va) * Fs[j]' for i in 1:ds) for j in 1:ds)
-    g₄ = mx' * G * mx + tr(Vx * G)
-    AE = n / 2 * log2π - 0.5 * mean(logdet, q_Λ) + 0.5 * (g₁ - g₂ - g₃ + g₄)
-
-    if order > 1
-        AE += entropy(q_y_x)
-        idc = LazyArrays.Vcat(1:ds, (dim + 1):(2dim))
-        myx_n = view(myx, idc)
-        Vyx_n = view(Vyx, idc, idc)
-        q_y_x = MvNormalMeanCovariance(myx_n, Vyx_n)
-        AE -= entropy(q_y_x)
-    end
-
-    return AE
-end
-
-@average_energy MAR (
-    q_y::MultivariateNormalDistributionsFamily, q_x::MultivariateNormalDistributionsFamily, q_a::MultivariateNormalDistributionsFamily, q_Λ::Wishart, meta::MARMeta
-) = begin
-    ma, Va = mean_cov(q_a)
-    my, Vy = mean_cov(q_y)
-    mx, Vx = mean_cov(q_y)
-    mΛ     = mean(q_Λ)
-
-    order, ds = getorder(meta), getdimensionality(meta)
-    F = Multivariate
-    dim = order * ds
-    n = dim
-
-    ma, Va = mean_cov(q_a)
-    mA = mar_companion_matrix(order, ds, ma)[1:ds, 1:dim]
-
-    my1, Vy1 = my[1:ds], Vy[1:ds, 1:ds]
-
-    # this should be inside MARMeta
-    es = [uvector(ds, i) for i in 1:ds]
-    Fs = [mask_mar(order, ds, i) for i in 1:ds]
-
-    g₁ = my1' * mΛ * my1 + tr(Vy1 * mΛ)
-    g₂ = -mx' * mA' * mΛ * my1
-    g₃ = -g₂
-    G = sum(sum(es[i]' * mΛ * es[j] * Fs[i] * (ma * ma' + Va) * Fs[j]' for i in 1:ds) for j in 1:ds)
-    g₄ = mx' * G * mx + tr(Vx * G)
-    AE = n / 2 * log2π - 0.5 * mean(logdet, q_Λ) + 0.5 * (g₁ + g₂ + g₃ + g₄)
-
-    if order > 1
-        AE += entropy(q_y)
-        q_y = MvNormalMeanCovariance(my1, Vy1)
-        AE -= entropy(q_y)
-    end
-
-    return AE
-end
-
-# Helpers for AR rules
-function mask_mar(order, dimension, index)
-    F = zeros(dimension * order, dimension * dimension * order)
-    rows = repeat([dimension], order)
-    cols = repeat([dimension], dimension * order)
-    FB = BlockArrays.BlockArray(F, rows, cols)
-    for k in 1:order
-        for j in 1:(dimension * order)
-            if j == index + (k - 1) * dimension
-                view(FB, BlockArrays.Block(k, j)) .= diageye(dimension)
-            end
-        end
-    end
-    return Matrix(FB)
-end
-
-function mar_transition(order, Λ)
-    dim = size(Λ, 1)
-    W = 1.0 * diageye(dim * order)
-    W[1:dim, 1:dim] = Λ
-    return W
-end
-
-function mar_shift(order, ds)
-    dim = order * ds
-    S = diageye(dim)
-    for i in dim:-1:(ds + 1)
-        S[i, :] = S[i - ds, :]
-    end
-    S[1:ds, :] = zeros(ds, dim)
-    return S
-end
-
-function uvector(dim, pos = 1)
-    u = zeros(dim)
-    u[pos] = 1
-    return dim == 1 ? u[pos] : u
-end
-
-function mar_companion_matrix(order, ds, a)
-    dim = order * ds
-    S = mar_shift(order, ds)
-    es = [uvector(dim, i) for i in 1:ds]
-    Fs = [mask_mar(order, ds, i) for i in 1:ds]
-    L = S .+ sum(es[i] * a' * Fs[i]' for i in 1:ds)
-    return L
-end
diff --git a/src/rule.jl b/src/rule.jl
index 4cef52ef..4447e166 100644
--- a/src/rule.jl
+++ b/src/rule.jl
@@ -202,21 +202,48 @@ function call_rule_macro_parse_fn_args(inputs; specname, prefix, proxy)
     return names_arg, values_arg
 end
 
+# This trait indicates that a node reference is required for a proper rule execution 
+# Most of the message passing update rules do not require a node reference
+# An example of a rule that requires a node is the `delta`, that needs the node function
+struct CallRuleNodeRequired end
+
+# This trait indicates that a node reference is not required for a proper rule execution 
+# This is used by default
+struct CallRuleNodeNotRequired end
+
+"""
+    call_rule_is_node_required(fformtype)
+
+Returns either `CallRuleNodeRequired()` or `CallRuleNodeNotRequired()` depending on if a specific 
+`fformtype` requires an access to the corresponding node in order to compute a message update rule.
+Returns `CallRuleNodeNotRequired()` for all known functional forms by default and `CallRuleNodeRequired()` for all unknown functional forms.
+"""
+call_rule_is_node_required(fformtype) = call_rule_is_node_required(as_node_functional_form(fformtype), fformtype)
+
+call_rule_is_node_required(::ValidNodeFunctionalForm, fformtype) = CallRuleNodeNotRequired()
+call_rule_is_node_required(::UndefinedNodeFunctionalForm, fformtype) = CallRuleNodeRequired()
+
+# Returns the `node` if it is required for a rule, otherwise returns `nothing`
+node_if_required(fformtype, node) = node_if_required(call_rule_is_node_required(fformtype), node)
+
+node_if_required(::CallRuleNodeRequired, node) = node
+node_if_required(::CallRuleNodeNotRequired, node) = nothing
+
 """
     call_rule_create_node(::Type{ NodeType }, fformtype)
 
-Creates a node object that will be used inside `@call_rule` macro. The node object always creates with the default options for factorisation. 
+Creates a node object that will be used inside `@call_rule` macro. 
 """
 function call_rule_make_node(fformtype, nodetype, meta)
-    return call_rule_make_node(ReactiveMP.as_node_functional_form(nodetype), fformtype, nodetype, meta)
+    return call_rule_make_node(call_rule_is_node_required(nodetype), fformtype, nodetype, meta)
 end
 
-function call_rule_make_node(::UndefinedNodeFunctionalForm, fformtype, nodetype, meta)
-    return error("Cannot create a node of type `$nodetype` for the call rule routine.")
+function call_rule_make_node(::CallRuleNodeRequired, fformtype, nodetype, meta)
+    return error("Missing implementation for the `call_rule_make_node`. Cannot create a node of type `$nodetype` for the call rule routine.")
 end
 
-function call_rule_make_node(::ValidNodeFunctionalForm, fformtype, nodetype, meta)
-    return make_node(nodetype, FactorNodeCreationOptions(nothing, meta, nothing))
+function call_rule_make_node(::CallRuleNodeNotRequired, fformtype, nodetype, meta)
+    return nothing
 end
 
 call_rule_macro_construct_on_arg(on_type, on_index::Nothing) = MacroHelpers.bottom_type(on_type)
diff --git a/src/rules/bernoulli/marginals.jl b/src/rules/bernoulli/marginals.jl
index 78146875..40a68b2b 100644
--- a/src/rules/bernoulli/marginals.jl
+++ b/src/rules/bernoulli/marginals.jl
@@ -5,3 +5,7 @@ export marginalrule
     p = prod(ProdAnalytical(), Beta(one(r) + r, 2one(r) - r), m_p)
     return (out = m_out, p = p)
 end
+
+@marginalrule Bernoulli(:out_p) (m_out::Bernoulli, m_p::PointMass) = begin
+    return (out = prod(ProdAnalytical(), Bernoulli(mean(m_p)), m_out), p = m_p)
+end
diff --git a/src/rules/categorical/marginals.jl b/src/rules/categorical/marginals.jl
index 4befdbb1..88c8890f 100644
--- a/src/rules/categorical/marginals.jl
+++ b/src/rules/categorical/marginals.jl
@@ -2,3 +2,8 @@
 @marginalrule Categorical(:out_p) (m_out::Categorical, m_p::PointMass) = begin
     return (out = prod(ProdAnalytical(), Categorical(mean(m_p)), m_out), p = m_p)
 end
+
+@marginalrule Categorical(:out_p) (m_out::PointMass, m_p::Dirichlet) = begin
+    p = prod(ProdAnalytical(), Dirichlet(probvec(m_out) .+ one(eltype(probvec(m_out)))), m_p)
+    return (out = m_out, p = p)
+end
diff --git a/src/rules/kernel_gcv/marginals.jl b/src/rules/kernel_gcv/marginals.jl
deleted file mode 100644
index 9745498f..00000000
--- a/src/rules/kernel_gcv/marginals.jl
+++ /dev/null
@@ -1,33 +0,0 @@
-export marginalrule
-
-@marginalrule KernelGCV(:y_x) (m_y::MvNormalMeanCovariance, m_x::MvNormalMeanCovariance, q_z::MvNormalMeanCovariance, meta::KernelGCVMetadata) = begin
-    kernelfunction = get_kernelfn(meta)
-    Λ = approximate_kernel_expectation(get_approximation(meta), (z) -> cholinv(kernelfunction(z)), q_z)
-
-    Λy = invcov(m_y)
-    Λx = invcov(m_x)
-
-    wy = Λy * mean(m_y)
-    wx = Λx * mean(m_x)
-
-    C = cholinv([Λ+Λy -Λ; -Λ Λ+Λx])
-    m = C * [wy; wx]
-
-    return MvNormalMeanCovariance(m, C)
-end
-
-@marginalrule KernelGCV(:y_x) (m_y::MvNormalMeanPrecision, m_x::MvNormalMeanPrecision, q_z::MvNormalMeanPrecision, meta::KernelGCVMetadata) = begin
-    kernelfunction = get_kernelfn(meta)
-    C = approximate_kernel_expectation(get_approximation(meta), (z) -> cholinv(kernelfunction(z)), q_z)
-
-    Cy = invcov(m_y)
-    Cx = invcov(m_x)
-
-    wy = Cy * mean(m_y)
-    wx = Cx * mean(m_x)
-
-    Λ = [C+Cy -C; -C C+Cx]
-    μ = cholinv(Λ) * [wy; wx]
-
-    return MvNormalMeanPrecision(μ, Λ)
-end
diff --git a/src/rules/kernel_gcv/x.jl b/src/rules/kernel_gcv/x.jl
deleted file mode 100644
index 29776006..00000000
--- a/src/rules/kernel_gcv/x.jl
+++ /dev/null
@@ -1,13 +0,0 @@
-export rule
-
-@rule KernelGCV(:x, Marginalisation) (m_y::MvNormalMeanCovariance, q_z::MvNormalMeanCovariance, meta::KernelGCVMetadata) = begin
-    kernelfunction = get_kernelfn(meta)
-    Λ_out = approximate_kernel_expectation(get_approximation(meta), (s) -> cholinv(kernelfunction(s)), q_z)
-    return MvNormalMeanCovariance(mean(m_y), cov(m_y) + cholinv(Λ_out))
-end
-
-@rule KernelGCV(:x, Marginalisation) (m_y::MvNormalMeanPrecision, q_z::MvNormalMeanPrecision, meta::KernelGCVMetadata) = begin
-    kernelfunction = get_kernelfn(meta)
-    Λ_out = approximate_kernel_expectation(get_approximation(meta), (s) -> cholinv(kernelfunction(s)), q_z)
-    return MvNormalMeanPrecision(mean(m_y), cholinv(cov(m_y) + cholinv(Λ_out)))
-end
diff --git a/src/rules/kernel_gcv/y.jl b/src/rules/kernel_gcv/y.jl
deleted file mode 100644
index a6a4b82f..00000000
--- a/src/rules/kernel_gcv/y.jl
+++ /dev/null
@@ -1,13 +0,0 @@
-export rule
-
-@rule KernelGCV(:y, Marginalisation) (m_x::MvNormalMeanCovariance, q_z::MvNormalMeanCovariance, meta::KernelGCVMetadata) = begin
-    kernelfunction = get_kernelfn(meta)
-    Λ_out = approximate_kernel_expectation(get_approximation(meta), (s) -> cholinv(kernelfunction(s)), q_z)
-    return MvNormalMeanCovariance(mean(m_x), cov(m_x) + cholinv(Λ_out))
-end
-
-@rule KernelGCV(:y, Marginalisation) (m_x::MvNormalMeanPrecision, q_z::MvNormalMeanPrecision, meta::KernelGCVMetadata) = begin
-    kernelfunction = get_kernelfn(meta)
-    Λ_out = approximate_kernel_expectation(get_approximation(meta), (s) -> inv(kernelfunction(s)), q_z)
-    return MvNormalMeanPrecision(mean(m_x), cholinv(cov(m_x) + cholinv(Λ_out)))
-end
diff --git a/src/rules/kernel_gcv/z.jl b/src/rules/kernel_gcv/z.jl
deleted file mode 100644
index 5ebb4978..00000000
--- a/src/rules/kernel_gcv/z.jl
+++ /dev/null
@@ -1,53 +0,0 @@
-export rule
-
-@rule KernelGCV(:z, Marginalisation) (q_y_x::MvNormalMeanCovariance, meta::KernelGCVMetadata) = begin
-    dims = Int64(ndims(q_y_x) / 2)
-
-    m_yx   = mean(q_y_x)
-    cov_yx = cov(q_y_x)
-
-    cov11 = @view cov_yx[1:dims, 1:dims]
-    cov12 = @view cov_yx[1:dims, (dims + 1):end]
-    cov21 = @view cov_yx[(dims + 1):end, 1:dims]
-    cov22 = @view cov_yx[(dims + 1):end, (dims + 1):end]
-
-    m1 = @view m_yx[1:dims]
-    m2 = @view m_yx[(dims + 1):end]
-
-    psi = cov11 + cov22 - cov12 - cov21 + (m1 - m2) * (m1 - m2)'
-
-    kernelfunction = get_kernelfn(meta)
-
-    logpdf = (z) -> begin
-        gz = kernelfunction(z)
-        -0.5 * (logdet(gz) + tr(cholinv(gz) * psi))
-    end
-
-    return FnWithApproximation(logpdf, get_approximation(meta))
-end
-
-@rule KernelGCV(:z, Marginalisation) (q_y_x::MvNormalMeanPrecision, meta::KernelGCVMetadata) = begin
-    dims = Int64(ndims(q_y_x) / 2)
-
-    m_yx   = mean(q_y_x)
-    cov_yx = cov(q_y_x)
-
-    cov11 = @view cov_yx[1:dims, 1:dims]
-    cov12 = @view cov_yx[1:dims, (dims + 1):end]
-    cov21 = @view cov_yx[(dims + 1):end, 1:dims]
-    cov22 = @view cov_yx[(dims + 1):end, (dims + 1):end]
-
-    m1 = @view m_yx[1:dims]
-    m2 = @view m_yx[(dims + 1):end]
-
-    psi = cov11 + cov22 - cov12 - cov21 + (m1 - m2) * (m1 - m2)'
-
-    kernelfunction = get_kernelfn(meta)
-
-    logpdf = (z) -> begin
-        gz = kernelfunction(z)
-        -0.5 * (logdet(gz) + tr(cholinv(gz) * psi))
-    end
-
-    return FnWithApproximation(logpdf, get_approximation(meta))
-end
diff --git a/src/rules/mv_autoregressive/a.jl b/src/rules/mv_autoregressive/a.jl
deleted file mode 100644
index 0777efb7..00000000
--- a/src/rules/mv_autoregressive/a.jl
+++ /dev/null
@@ -1,50 +0,0 @@
-
-@rule MAR(:a, Marginalisation) (q_y_x::MultivariateNormalDistributionsFamily, q_Λ::Any, meta::MARMeta) = begin
-    order, ds = getorder(meta), getdimensionality(meta)
-    F = Multivariate
-
-    dim = order * ds
-
-    m, V = mean_cov(q_y_x)
-
-    my, Vy = ar_slice(F, m, 1:dim), ar_slice(F, V, 1:dim, 1:dim)
-    mx, Vx = ar_slice(F, m, (dim + 1):(2dim)), ar_slice(F, V, (dim + 1):(2dim), (dim + 1):(2dim))
-    Vyx    = ar_slice(F, V, 1:dim, (dim + 1):(2dim))
-
-    mΛ = mean(q_Λ)
-    mW = mar_transition(order, mΛ)
-
-    # this should be inside MARMeta
-    es = [uvector(dim, i) for i in 1:ds]
-    Fs = [mask_mar(order, ds, i) for i in 1:ds]
-    S = mar_shift(order, ds)
-
-    # NOTE: prove that sum(Fs[i]'*((mx*mx'+Vx')*S')*mW*es[i] for i in 1:ds) == 0.0
-    D = sum(sum(es[i]' * mW * es[j] * Fs[i]' * (mx * mx' + Vx) * Fs[j] for i in 1:ds) for j in 1:ds)
-    z = sum(Fs[i]' * (mx * my' + Vyx') * mW * es[i] for i in 1:ds)
-
-    return MvNormalWeightedMeanPrecision(z, D)
-end
-
-@rule MAR(:a, Marginalisation) (q_y::MultivariateNormalDistributionsFamily, q_x::MultivariateNormalDistributionsFamily, q_Λ::Any, meta::MARMeta) = begin
-    order, ds = getorder(meta), getdimensionality(meta)
-    F = Multivariate
-
-    dim = order * ds
-
-    my, Vy = mean_cov(q_y)
-    mx, Vx = mean_cov(q_x)
-    mΛ     = mean(q_Λ)
-
-    mW = mar_transition(order, mΛ)
-    S  = mar_shift(order, ds)
-
-    # this should be inside MARMeta
-    es = [uvector(dim, i) for i in 1:ds]
-    Fs = [mask_mar(order, ds, i) for i in 1:ds]
-
-    D = sum(sum(es[j]' * mW * es[i] * Fs[i]' * (mx * mx' + Vx) * Fs[j] for i in 1:ds) for j in 1:ds)
-    z = sum(Fs[i]' * ((mx * mx' + Vx') * S' + mx * my') * mW * es[i] for i in 1:ds)
-
-    return MvNormalWeightedMeanPrecision(z, D)
-end
diff --git a/src/rules/mv_autoregressive/lambda.jl b/src/rules/mv_autoregressive/lambda.jl
deleted file mode 100644
index 29f88cba..00000000
--- a/src/rules/mv_autoregressive/lambda.jl
+++ /dev/null
@@ -1,53 +0,0 @@
-@rule MAR(:Λ, Marginalisation) (q_y_x::MultivariateNormalDistributionsFamily, q_a::MultivariateNormalDistributionsFamily, meta::MARMeta) = begin
-    order, ds = getorder(meta), getdimensionality(meta)
-    F = Multivariate
-    dim = order * ds
-
-    ma, Va = mean_cov(q_a)
-
-    mA = mar_companion_matrix(order, ds, ma)
-
-    m, V   = mean_cov(q_y_x)
-    my, Vy = ar_slice(F, m, 1:dim), ar_slice(F, V, 1:dim, 1:dim)
-    mx, Vx = ar_slice(F, m, (dim + 1):(2dim)), ar_slice(F, V, (dim + 1):(2dim), (dim + 1):(2dim))
-    Vyx    = ar_slice(F, V, 1:dim, (dim + 1):(2dim))
-
-    es = [uvector(dim, i) for i in 1:ds]
-    Fs = [mask_mar(order, ds, i) for i in 1:ds]
-    S = mar_shift(order, ds)
-    G₁ = (my * my' + Vy)[1:ds, 1:ds]
-    G₂ = ((my * mx' + Vyx) * mA')[1:ds, 1:ds]
-    G₃ = transpose(G₂)
-    Ex_xx = mx * mx' + Vx
-    G₅ = sum(sum(es[i] * ma' * Fs[i]'Ex_xx * Fs[j] * ma * es[j]' for i in 1:ds) for j in 1:ds)[1:ds, 1:ds]
-    G₆ = sum(sum(es[i] * tr(Fs[i]' * Ex_xx * Fs[j] * Va) * es[j]' for i in 1:ds) for j in 1:ds)[1:ds, 1:ds]
-    Δ = G₁ - G₂ - G₃ + G₅ + G₆
-
-    return WishartMessage(ds + 2, Δ)
-end
-
-@rule MAR(:Λ, Marginalisation) (q_y::MultivariateNormalDistributionsFamily, q_x::MultivariateNormalDistributionsFamily, q_a::MultivariateNormalDistributionsFamily, meta::MARMeta) =
-    begin
-        order, ds = getorder(meta), getdimensionality(meta)
-        F = Multivariate
-        dim = order * ds
-
-        my, Vy = mean_cov(q_y)
-        mx, Vx = mean_cov(q_x)
-        ma, Va = mean_cov(q_a)
-
-        mA = mar_companion_matrix(order, ds, ma)
-
-        es = [uvector(dim, i) for i in 1:ds]
-        Fs = [mask_mar(order, ds, i) for i in 1:ds]
-        S = mar_shift(order, ds)
-        G₁ = (my * my' + Vy)[1:ds, 1:ds]
-        G₂ = (my * mx' * mA')[1:ds, 1:ds]
-        G₃ = transpose(G₂)
-        Ex_xx = mx * mx' + Vx
-        G₅ = sum(sum(es[i] * ma' * Fs[j]'Ex_xx * Fs[i] * ma * es[j]' for i in 1:ds) for j in 1:ds)[1:ds, 1:ds]
-        G₆ = sum(sum(es[i] * tr(Va * Fs[i]' * Ex_xx * Fs[j]) * es[j]' for i in 1:ds) for j in 1:ds)[1:ds, 1:ds]
-        Δ = G₁ - G₂ - G₃ + G₅ + G₆
-
-        return WishartMessage(ds + 2, Δ)
-    end
diff --git a/src/rules/mv_autoregressive/marginals.jl b/src/rules/mv_autoregressive/marginals.jl
deleted file mode 100644
index 92a71a65..00000000
--- a/src/rules/mv_autoregressive/marginals.jl
+++ /dev/null
@@ -1,46 +0,0 @@
-
-@marginalrule MAR(:y_x) (
-    m_y::MultivariateNormalDistributionsFamily, m_x::MultivariateNormalDistributionsFamily, q_a::MultivariateNormalDistributionsFamily, q_Λ::Any, meta::MARMeta
-) = begin
-    return ar_y_x_marginal(m_y, m_x, q_a, q_Λ, meta)
-end
-
-function ar_y_x_marginal(
-    m_y::MultivariateNormalDistributionsFamily, m_x::MultivariateNormalDistributionsFamily, q_a::MultivariateNormalDistributionsFamily, q_Λ::Any, meta::MARMeta
-)
-    order, ds = getorder(meta), getdimensionality(meta)
-    F = Multivariate
-    dim = order * ds
-
-    ma, Va = mean_cov(q_a)
-    mΛ = mean(q_Λ)
-
-    mA = mar_companion_matrix(order, ds, ma)
-    mW = mar_transition(getorder(meta), mΛ)
-
-    b_my, b_Vy = mean_cov(m_y)
-    f_mx, f_Vx = mean_cov(m_x)
-
-    inv_b_Vy = cholinv(b_Vy)
-    inv_f_Vx = cholinv(f_Vx)
-
-    # this should be inside MARMeta
-    es = [uvector(dim, i) for i in 1:ds]
-    Fs = [mask_mar(order, ds, i) for i in 1:ds]
-
-    Ξ = inv_f_Vx + sum(sum(es[j]' * mW * es[i] * Fs[j] * Va * Fs[i]' for i in 1:ds) for j in 1:ds)
-
-    W_11 = inv_b_Vy + mW
-
-    # negate_inplace!(mW * mA)
-    W_12 = -(mW * mA)
-
-    W_21 = -(mA' * mW)
-
-    W_22 = Ξ + mA' * mW * mA
-
-    W = [W_11 W_12; W_21 W_22]
-    ξ = [inv_b_Vy * b_my; inv_f_Vx * f_mx]
-
-    return MvNormalWeightedMeanPrecision(ξ, W)
-end
diff --git a/src/rules/mv_autoregressive/x.jl b/src/rules/mv_autoregressive/x.jl
deleted file mode 100644
index e191589a..00000000
--- a/src/rules/mv_autoregressive/x.jl
+++ /dev/null
@@ -1,50 +0,0 @@
-
-@rule MAR(:x, Marginalisation) (m_y::MultivariateNormalDistributionsFamily, q_a::MultivariateNormalDistributionsFamily, q_Λ::Any, meta::MARMeta) = begin
-    ma, Va = mean_cov(q_a)
-    my, Vy = mean_cov(m_y)
-
-    mΛ = mean(q_Λ)
-
-    order, ds = getorder(meta), getdimensionality(meta)
-    dim = order * ds
-
-    mA = mar_companion_matrix(order, ds, ma)
-    mW = mar_transition(getorder(meta), mΛ)
-    # this should be inside MARMeta
-    es = [uvector(dim, i) for i in 1:ds]
-    Fs = [mask_mar(order, ds, i) for i in 1:ds]
-
-    Λ = sum(sum(es[j]' * mW * es[i] * Fs[j] * Va * Fs[i]' for i in 1:ds) for j in 1:ds)
-
-    Σ₁ = Hermitian(pinv(mA) * (Vy) * pinv(mA') + pinv(mA' * mW * mA))
-
-    Ξ = (pinv(Σ₁) + Λ)
-    z = pinv(Σ₁) * pinv(mA) * my
-
-    return MvNormalWeightedMeanPrecision(z, Ξ)
-end
-
-@rule MAR(:x, Marginalisation) (q_y::MultivariateNormalDistributionsFamily, q_a::MultivariateNormalDistributionsFamily, q_Λ::Any, meta::MARMeta) = begin
-    ma, Va = mean_cov(q_a)
-    my, Vy = mean_cov(q_y)
-
-    mΛ = mean(q_Λ)
-
-    order, ds = getorder(meta), getdimensionality(meta)
-    dim = order * ds
-
-    mA = mar_companion_matrix(order, ds, ma)
-    mW = mar_transition(getorder(meta), mΛ)
-
-    # this should be inside MARMeta
-    es = [uvector(dim, i) for i in 1:ds]
-    Fs = [mask_mar(order, ds, i) for i in 1:ds]
-
-    Λ = sum(sum(es[j]' * mW * es[i] * Fs[j] * Va * Fs[i]' for i in 1:ds) for j in 1:ds)
-    Λ₀ = Hermitian(mA' * mW * mA)
-
-    Ξ = Λ₀ + Λ
-    z = Λ₀ * pinv(mA) * my
-
-    return MvNormalWeightedMeanPrecision(z, Ξ)
-end
diff --git a/src/rules/mv_autoregressive/y.jl b/src/rules/mv_autoregressive/y.jl
deleted file mode 100644
index b99ace9b..00000000
--- a/src/rules/mv_autoregressive/y.jl
+++ /dev/null
@@ -1,34 +0,0 @@
-@rule MAR(:y, Marginalisation) (m_x::MultivariateNormalDistributionsFamily, q_a::MultivariateNormalDistributionsFamily, q_Λ::Any, meta::MARMeta) = begin
-    ma, Va = mean_cov(q_a)
-    mx, Wx = mean_invcov(m_x)
-
-    mΛ = mean(q_Λ)
-
-    order, ds = getorder(meta), getdimensionality(meta)
-
-    mA = mar_companion_matrix(order, ds, ma)
-    mW = mar_transition(getorder(meta), mΛ)
-    dim = order * ds
-    # this should be inside MARMeta
-    es = [uvector(dim, i) for i in 1:ds]
-    Fs = [mask_mar(order, ds, i) for i in 1:ds]
-
-    Λ = sum(sum(es[j]' * mW * es[i] * Fs[j] * Va * Fs[i]' for i in 1:ds) for j in 1:ds)
-
-    Ξ = Λ + Wx
-    z = Wx * mx
-
-    Vy = mA * inv(Ξ) * mA' + inv(mW)
-    my = mA * inv(Ξ) * z
-
-    return MvNormalMeanCovariance(my, Vy)
-end
-
-@rule MAR(:y, Marginalisation) (q_x::MultivariateNormalDistributionsFamily, q_a::MultivariateNormalDistributionsFamily, q_Λ::Any, meta::MARMeta) = begin
-    order, ds = getorder(meta), getdimensionality(meta)
-
-    mA = mar_companion_matrix(order, ds, mean(q_a))
-    mW = mar_transition(getorder(meta), mean(q_Λ))
-
-    return MvNormalMeanPrecision(mA * mean(q_x), mW)
-end
diff --git a/src/rules/prototypes.jl b/src/rules/prototypes.jl
index 81395dd7..5c56ab65 100644
--- a/src/rules/prototypes.jl
+++ b/src/rules/prototypes.jl
@@ -52,11 +52,6 @@ include("gcv/w.jl")
 include("gcv/marginals.jl")
 include("gcv/gaussian_extension.jl")
 
-include("kernel_gcv/x.jl")
-include("kernel_gcv/y.jl")
-include("kernel_gcv/z.jl")
-include("kernel_gcv/marginals.jl")
-
 include("mv_normal_mean_covariance/out.jl")
 include("mv_normal_mean_covariance/mean.jl")
 include("mv_normal_mean_covariance/covariance.jl")
@@ -116,12 +111,6 @@ include("autoregressive/theta.jl")
 include("autoregressive/gamma.jl")
 include("autoregressive/marginals.jl")
 
-include("mv_autoregressive/y.jl")
-include("mv_autoregressive/x.jl")
-include("mv_autoregressive/a.jl")
-include("mv_autoregressive/lambda.jl")
-include("mv_autoregressive/marginals.jl")
-
 include("probit/marginals.jl")
 include("probit/in.jl")
 include("probit/out.jl")
diff --git a/src/variables/constant.jl b/src/variables/constant.jl
index 6cb40aed..b808acd6 100644
--- a/src/variables/constant.jl
+++ b/src/variables/constant.jl
@@ -50,8 +50,8 @@ function constvar end
 
 constvar(name::Symbol, constval, collection_type::AbstractVariableCollectionType = VariableIndividual())                 = ConstVariable(name, collection_type, constval, of(Message(constval, true, false, nothing)), 0)
 constvar(name::Symbol, constval::Real, collection_type::AbstractVariableCollectionType = VariableIndividual())           = constvar(name, PointMass(constval), collection_type)
-constvar(name::Symbol, constval::AbstractVector, collection_type::AbstractVariableCollectionType = VariableIndividual()) = constvar(name, PointMass(constval), collection_type)
-constvar(name::Symbol, constval::AbstractMatrix, collection_type::AbstractVariableCollectionType = VariableIndividual()) = constvar(name, PointMass(constval), collection_type)
+constvar(name::Symbol, constval::AbstractArray, collection_type::AbstractVariableCollectionType = VariableIndividual())  = constvar(name, PointMass(constval), collection_type)
+constvar(name::Symbol, constval::UniformScaling, collection_type::AbstractVariableCollectionType = VariableIndividual()) = constvar(name, PointMass(constval), collection_type)
 
 function constvar(name::Symbol, fn::Function, length::Int)
     return map(i -> constvar(name, fn(i), VariableVector(i)), 1:length)
diff --git a/src/variables/data.jl b/src/variables/data.jl
index beb05410..6600cda8 100644
--- a/src/variables/data.jl
+++ b/src/variables/data.jl
@@ -9,15 +9,19 @@ mutable struct DataVariable{D, S} <: AbstractVariable
     input_messages  :: Vector{MessageObservable{AbstractMessage}}
     messageout      :: S
     nconnected      :: Int
+    isproxy         :: Bool
+    isused          :: Bool
 end
 
 Base.show(io::IO, datavar::DataVariable) = print(io, "DataVariable(", indexed_name(datavar), ")")
 
 struct DataVariableCreationOptions{S}
-    subject::S
+    subject :: S
+    isproxy :: Bool
+    isused  :: Bool
 end
 
-Base.similar(options::DataVariableCreationOptions) = DataVariableCreationOptions(similar(options.subject))
+Base.similar(options::DataVariableCreationOptions) = DataVariableCreationOptions(similar(options.subject), options.isproxy, options.isused)
 
 DataVariableCreationOptions(::Type{D}) where {D}          = DataVariableCreationOptions(D, nothing)
 DataVariableCreationOptions(::Type{D}, subject) where {D} = DataVariableCreationOptions(D, subject, Val(false))
@@ -26,7 +30,7 @@ DataVariableCreationOptions(::Type{D}, subject::Nothing, allow_missing::Val{true
 DataVariableCreationOptions(::Type{D}, subject::Nothing, allow_missing::Val{false}) where {D} = DataVariableCreationOptions(D, RecentSubject(Union{Message{D}}), Val(false))
 
 DataVariableCreationOptions(::Type{D}, subject::S, ::Val{true}) where {D, S}  = error("Error in datavar options. Custom `subject` was specified and `allow_missing` was set to true, which is disallowed. Provide a custom subject that accept missing values by itself and do no use `allow_missing` option.")
-DataVariableCreationOptions(::Type{D}, subject::S, ::Val{false}) where {D, S} = DataVariableCreationOptions{S}(subject)
+DataVariableCreationOptions(::Type{D}, subject::S, ::Val{false}) where {D, S} = DataVariableCreationOptions{S}(subject, false, false)
 
 """ 
     datavar(::Type, [ dims... ])
@@ -72,7 +76,7 @@ datavar(name::Symbol, ::Type{D}, dims::Tuple) where {D}
 datavar(name::Symbol, ::Type{D}, dims::Vararg{Int}) where {D}                                                      = datavar(DataVariableCreationOptions(D), name, D, dims)
 
 datavar(options::DataVariableCreationOptions{S}, name::Symbol, ::Type{D}, collection_type::AbstractVariableCollectionType = VariableIndividual()) where {S, D} =
-    DataVariable{D, S}(name, collection_type, MarginalObservable(), Vector{MessageObservable{AbstractMessage}}(), options.subject, 0)
+    DataVariable{D, S}(name, collection_type, MarginalObservable(), Vector{MessageObservable{AbstractMessage}}(), options.subject, 0, options.isproxy, options.isused)
 
 function datavar(options::DataVariableCreationOptions, name::Symbol, ::Type{D}, length::Int) where {D}
     return map(i -> datavar(similar(options), name, D, VariableVector(i)), 1:length)
@@ -93,12 +97,13 @@ Base.eltype(::DataVariable{D}) where {D}         = D
 
 degree(datavar::DataVariable)          = nconnected(datavar)
 name(datavar::DataVariable)            = datavar.name
-proxy_variables(datavar::DataVariable) = nothing
+proxy_variables(datavar::DataVariable) = nothing    # not related to isproxy
 collection_type(datavar::DataVariable) = datavar.collection_type
 isconnected(datavar::DataVariable)     = datavar.nconnected !== 0
 nconnected(datavar::DataVariable)      = datavar.nconnected
 
-isproxy(::DataVariable) = false
+isproxy(datavar::DataVariable) = datavar.isproxy
+isused(datavar::DataVariable) = datavar.isused
 
 israndom(::DataVariable)                  = false
 israndom(::AbstractArray{<:DataVariable}) = false
@@ -117,7 +122,7 @@ function Base.getindex(datavar::DataVariable, i...)
     error("Variable $(indexed_name(datavar)) has been indexed with `[$(join(i, ','))]`. Direct indexing of `data` variables is not allowed.")
 end
 
-getlastindex(::DataVariable) = 1
+getlastindex(datavar::DataVariable) = degree(datavar) + 1
 
 messageout(datavar::DataVariable, ::Int) = datavar.messageout
 messagein(datavar::DataVariable, ::Int)  = error("It is not possible to get a reference for inbound message for datavar")
@@ -168,16 +173,18 @@ _getmarginal(datavar::DataVariable)              = datavar.messageout |> map(Mar
 _setmarginal!(datavar::DataVariable, observable) = error("It is not possible to set a marginal stream for `DataVariable`")
 _makemarginal(datavar::DataVariable)             = error("It is not possible to make marginal stream for `DataVariable`")
 
-# Extension for _getmarginal
-function Rocket.getrecent(proxy::ProxyObservable{<:Marginal, S, M}) where {S <: Rocket.RecentSubjectInstance, D, M <: Rocket.MapProxy{D, typeof(as_marginal)}}
-    return as_marginal(Rocket.getrecent(proxy.proxied_source))
-end
-
 setanonymous!(::DataVariable, ::Bool) = nothing
 
-function setmessagein!(datavar::DataVariable, ::Int, messagein)
-    datavar.nconnected += 1
-    push!(datavar.input_messages, messagein)
+function setmessagein!(datavar::DataVariable, index::Int, messagein)
+    if index === (degree(datavar) + 1)
+        push!(datavar.input_messages, messagein)
+        datavar.nconnected += 1
+        datavar.isused = true
+    else
+        error(
+            "Inconsistent state in setmessagein! function for data variable $(datavar). `index` should be equal to `degree(datavar) + 1 = $(degree(datavar) + 1)`, $(index) is given instead"
+        )
+    end
     return nothing
 end
 
diff --git a/src/variables/variable.jl b/src/variables/variable.jl
index 5d2be480..bf990a7b 100644
--- a/src/variables/variable.jl
+++ b/src/variables/variable.jl
@@ -147,7 +147,7 @@ track of `proxy_variables`. During the first call of `get_factorisation_referenc
 2. if yes we pass it futher to the `unchecked` version of the function 
    2.1 `unchecked` version return immediatelly if there is only one proxy var (see bullet 1)
    2.2 in case of multiple proxy vars we filter only `RandomVariable` and call `checked` version of the function 
-3. `checked` version of the function return immediatelly if there is only one proxy random variable left, if there are multuple proxy random vars we throw an error as this case is ambigous for factorisation constrains specification
+3. `checked` version of the function return immediatelly if there is only one proxy random variable left, if there are multiple proxy random vars we throw an error as this case is ambigous for factorisation constraints specification
 
 This function is a part of private API and should not be used explicitly.
 """
diff --git a/test/approximations/test_cvi.jl b/test/approximations/test_cvi.jl
index fbc9f3b7..cf6a1df0 100644
--- a/test/approximations/test_cvi.jl
+++ b/test/approximations/test_cvi.jl
@@ -80,8 +80,8 @@ end
         rng = StableRNG(42)
 
         tests = (
-            (method = CVI(StableRNG(42), 1, 1000, Descent(0.01), ForwardDiffGrad(), 1, Val(true), false), tol = 5e-1),
-            (method = CVI(StableRNG(42), 1, 1000, Descent(0.01), ZygoteGrad(), 1, Val(true), false), tol = 5e-1)
+            (method = CVI(StableRNG(42), 1, 1000, Descent(0.01), ForwardDiffGrad(), 10, Val(true), false), tol = 5e-1),
+            (method = CVI(StableRNG(42), 1, 1000, Descent(0.01), ZygoteGrad(), 10, Val(true), false), tol = 5e-1)
         )
 
         # Check several prods against their analytical solutions
@@ -122,9 +122,16 @@ end
 
             b1 = Bernoulli(logistic(randn(rng)))
             b2 = Bernoulli(logistic(randn(rng)))
-            b_analitical = prod(ProdAnalytical(), b1, b2)
+            b_analytical = prod(ProdAnalytical(), b1, b2)
             b_cvi = prod(test[:method], b1, b1)
-            @test isapprox(mean(b_analitical), mean(b_cvi), atol = test[:tol])
+            @test isapprox(mean(b_analytical), mean(b_cvi), atol = test[:tol])
+
+            beta_1 = Beta(abs(randn(rng)) + 1, abs(randn(rng)) + 1)
+            beta_2 = Beta(abs(randn(rng)) + 1, abs(randn(rng)) + 1)
+
+            beta_analytical = prod(ProdAnalytical(), beta_1, beta_2)
+            beta_cvi = prod(test[:method], beta_1, beta_2)
+            @test isapprox(mean(beta_analytical), mean(beta_cvi), atol = test[:tol])
         end
     end
 
diff --git a/test/distributions/test_bernoulli.jl b/test/distributions/test_bernoulli.jl
index 2abc4968..8a469cd6 100644
--- a/test/distributions/test_bernoulli.jl
+++ b/test/distributions/test_bernoulli.jl
@@ -26,6 +26,18 @@ using ReactiveMP: compute_logscale
         @test prod(ProdAnalytical(), Bernoulli(0.78), Bernoulli(0.05)) ≈ Bernoulli(0.1572580645161291)
     end
 
+    @testset "prod Bernoulli-Categorical" begin
+        @test prod(ProdAnalytical(), Bernoulli(0.5), Categorical([1.0])) == Categorical([1.0, 0.0])
+        @test prod(ProdAnalytical(), Bernoulli(0.6), Categorical([0.7, 0.3])) == Categorical([0.6086956521739131, 0.391304347826087])
+        @test prod(ProdAnalytical(), Bernoulli(0.8), Categorical([0.2, 0.4, 0.4])) == Categorical([0.11111111111111108, 0.8888888888888888, 0.0])
+    end
+
+    @testset "prod Categorical-Bernoulli" begin
+        @test prod(ProdAnalytical(), Categorical([1.0]), Bernoulli(0.5)) == Categorical([1.0, 0.0])
+        @test prod(ProdAnalytical(), Categorical([0.7, 0.3]), Bernoulli(0.6)) == Categorical([0.6086956521739131, 0.391304347826087])
+        @test prod(ProdAnalytical(), Categorical([0.2, 0.4, 0.4]), Bernoulli(0.8)) == Categorical([0.11111111111111108, 0.8888888888888888, 0.0])
+    end
+
     @testset "probvec" begin
         @test probvec(Bernoulli(0.5)) === (0.5, 0.5)
         @test probvec(Bernoulli(0.3)) === (0.7, 0.3)
diff --git a/test/distributions/test_beta.jl b/test/distributions/test_beta.jl
index 0510f61c..14b4f5ab 100644
--- a/test/distributions/test_beta.jl
+++ b/test/distributions/test_beta.jl
@@ -6,6 +6,7 @@ using Distributions
 using Random
 
 import ReactiveMP: mirrorlog
+import SpecialFunctions: loggamma
 
 @testset "Beta" begin
 
@@ -37,6 +38,38 @@ import ReactiveMP: mirrorlog
         @test mean(mirrorlog, Beta(0.1, 0.3)) ≈ -0.9411396776150167
         @test mean(mirrorlog, Beta(4.5, 0.3)) ≈ -4.963371962929249
     end
+
+    @testset "BetaNaturalParameters" begin
+        @testset "Constructor" begin
+            for i in 0:10, j in 0:10
+                @test convert(Distribution, BetaNaturalParameters(i, j)) == Beta(i + 1, j + 1)
+
+                @test convert(BetaNaturalParameters, i, j) == BetaNaturalParameters(i, j)
+                @test convert(BetaNaturalParameters, [i, j]) == BetaNaturalParameters(i, j)
+            end
+        end
+
+        @testset "lognormalizer" begin
+            @test lognormalizer(BetaNaturalParameters(0, 0)) ≈ 0
+            @test lognormalizer(BetaNaturalParameters(1, 1)) ≈ -loggamma(4)
+        end
+
+        @testset "logpdf" begin
+            for i in 0:10, j in 0:10
+                @test logpdf(BetaNaturalParameters(i, j), 0.01) ≈ logpdf(Beta(i + 1, j + 1), 0.01)
+                @test logpdf(BetaNaturalParameters(i, j), 0.5) ≈ logpdf(Beta(i + 1, j + 1), 0.5)
+            end
+        end
+
+        @testset "isproper" begin
+            for i in 0:10
+                @test isproper(BetaNaturalParameters(i, i)) === true
+            end
+            for i in 1:10
+                @test isproper(BetaNaturalParameters(-i, -i)) === false
+            end
+        end
+    end
 end
 
 end
diff --git a/test/distributions/test_mv_normal_mean_covariance.jl b/test/distributions/test_mv_normal_mean_covariance.jl
index ef9af84d..55b4d4dd 100644
--- a/test/distributions/test_mv_normal_mean_covariance.jl
+++ b/test/distributions/test_mv_normal_mean_covariance.jl
@@ -14,6 +14,13 @@ using Distributions
         @test MvNormalMeanCovariance([1, 2]) == MvNormalMeanCovariance([1.0, 2.0], [1.0, 1.0])
         @test MvNormalMeanCovariance([1.0f0, 2.0f0]) == MvNormalMeanCovariance([1.0f0, 2.0f0], [1.0f0, 1.0f0])
 
+        # uniformscaling
+        @test MvNormalMeanCovariance([1, 2], I) == MvNormalMeanCovariance([1, 2], Diagonal([1, 1]))
+        @test MvNormalMeanCovariance([1, 2], 6 * I) == MvNormalMeanCovariance([1, 2], Diagonal([6, 6]))
+        @test MvNormalMeanCovariance([1.0, 2.0], I) == MvNormalMeanCovariance([1.0, 2.0], Diagonal([1.0, 1.0]))
+        @test MvNormalMeanCovariance([1.0, 2.0], 6 * I) == MvNormalMeanCovariance([1.0, 2.0], Diagonal([6.0, 6.0]))
+        @test MvNormalMeanCovariance([1, 2], 6.0 * I) == MvNormalMeanCovariance([1.0, 2.0], Diagonal([6.0, 6.0]))
+
         @test eltype(MvNormalMeanCovariance([1.0, 1.0])) === Float64
         @test eltype(MvNormalMeanCovariance([1.0, 1.0], [1.0, 1.0])) === Float64
         @test eltype(MvNormalMeanCovariance([1, 1])) === Float64
@@ -91,6 +98,14 @@ using Distributions
         dist = MvNormalMeanCovariance(μ, Σ)
 
         @test prod(ProdAnalytical(), dist, dist) ≈ MvNormalWeightedMeanPrecision([2.0, 2.0, 2.0], diagm([2.0, 1.0, 2 / 3]))
+
+        # diagonal covariance matrix/uniformscaling
+        @test prod(ProdAnalytical(), MvNormalMeanCovariance([-1, -1], [2 0; 0 2]), MvNormalMeanCovariance([1, 1], Diagonal([2, 4]))) ≈
+            MvNormalWeightedMeanPrecision([0, -1 / 4], [1, 3 / 4])
+        @test prod(ProdAnalytical(), MvNormalMeanCovariance([-1, -1], [2, 2]), MvNormalMeanCovariance([1, 1], Diagonal([2, 4]))) ≈
+            MvNormalWeightedMeanPrecision([0, -1 / 4], [1, 3 / 4])
+        @test prod(ProdAnalytical(), MvNormalMeanCovariance([-1, -1], 2 * I), MvNormalMeanCovariance([1, 1], Diagonal([2, 4]))) ≈
+            MvNormalWeightedMeanPrecision([0, -1 / 4], [1, 3 / 4])
     end
 
     @testset "Primitive types conversion" begin
diff --git a/test/distributions/test_mv_normal_mean_precision.jl b/test/distributions/test_mv_normal_mean_precision.jl
index 97afa3b5..4f00fd2e 100644
--- a/test/distributions/test_mv_normal_mean_precision.jl
+++ b/test/distributions/test_mv_normal_mean_precision.jl
@@ -14,6 +14,13 @@ using Distributions
         @test MvNormalMeanPrecision([1, 2]) == MvNormalMeanPrecision([1.0, 2.0], [1.0, 1.0])
         @test MvNormalMeanPrecision([1.0f0, 2.0f0]) == MvNormalMeanPrecision([1.0f0, 2.0f0], [1.0f0, 1.0f0])
 
+        # uniformscaling
+        @test MvNormalMeanPrecision([1, 2], I) == MvNormalMeanPrecision([1, 2], Diagonal([1, 1]))
+        @test MvNormalMeanPrecision([1, 2], 6 * I) == MvNormalMeanPrecision([1, 2], Diagonal([6, 6]))
+        @test MvNormalMeanPrecision([1.0, 2.0], I) == MvNormalMeanPrecision([1.0, 2.0], Diagonal([1.0, 1.0]))
+        @test MvNormalMeanPrecision([1.0, 2.0], 6 * I) == MvNormalMeanPrecision([1.0, 2.0], Diagonal([6.0, 6.0]))
+        @test MvNormalMeanPrecision([1, 2], 6.0 * I) == MvNormalMeanPrecision([1.0, 2.0], Diagonal([6.0, 6.0]))
+
         @test eltype(MvNormalMeanPrecision([1.0, 1.0])) === Float64
         @test eltype(MvNormalMeanPrecision([1.0, 1.0], [1.0, 1.0])) === Float64
         @test eltype(MvNormalMeanPrecision([1, 1])) === Float64
@@ -91,6 +98,11 @@ using Distributions
         dist = MvNormalMeanPrecision(μ, Λ)
 
         @test prod(ProdAnalytical(), dist, dist) ≈ MvNormalWeightedMeanPrecision([2.0, 2.0, 2.0], diagm([2.0, 1.0, 2 / 3]))
+
+        # diagonal covariance matrix/uniformscaling
+        @test prod(ProdAnalytical(), MvNormalMeanPrecision([-1, -1], [2 0; 0 2]), MvNormalMeanPrecision([1, 1], Diagonal([2, 4]))) ≈ MvNormalWeightedMeanPrecision([0, 2], [4, 6])
+        @test prod(ProdAnalytical(), MvNormalMeanPrecision([-1, -1], [2, 2]), MvNormalMeanPrecision([1, 1], Diagonal([2, 4]))) ≈ MvNormalWeightedMeanPrecision([0, 2], [4, 6])
+        @test prod(ProdAnalytical(), MvNormalMeanPrecision([-1, -1], 2 * I), MvNormalMeanPrecision([1, 1], Diagonal([2, 4]))) ≈ MvNormalWeightedMeanPrecision([0, 2], [4, 6])
     end
 
     @testset "Primitive types conversion" begin
diff --git a/test/distributions/test_mv_normal_weighted_mean_precision.jl b/test/distributions/test_mv_normal_weighted_mean_precision.jl
index 681adf6d..ee28cd68 100644
--- a/test/distributions/test_mv_normal_weighted_mean_precision.jl
+++ b/test/distributions/test_mv_normal_weighted_mean_precision.jl
@@ -14,6 +14,13 @@ using Distributions
         @test MvNormalWeightedMeanPrecision([1, 2]) == MvNormalWeightedMeanPrecision([1.0, 2.0], [1.0, 1.0])
         @test MvNormalWeightedMeanPrecision([1.0f0, 2.0f0]) == MvNormalWeightedMeanPrecision([1.0f0, 2.0f0], [1.0f0, 1.0f0])
 
+        # uniformscaling
+        @test MvNormalWeightedMeanPrecision([1, 2], I) == MvNormalWeightedMeanPrecision([1, 2], Diagonal([1, 1]))
+        @test MvNormalWeightedMeanPrecision([1, 2], 6 * I) == MvNormalWeightedMeanPrecision([1, 2], Diagonal([6, 6]))
+        @test MvNormalWeightedMeanPrecision([1.0, 2.0], I) == MvNormalWeightedMeanPrecision([1.0, 2.0], Diagonal([1.0, 1.0]))
+        @test MvNormalWeightedMeanPrecision([1.0, 2.0], 6 * I) == MvNormalWeightedMeanPrecision([1.0, 2.0], Diagonal([6.0, 6.0]))
+        @test MvNormalWeightedMeanPrecision([1, 2], 6.0 * I) == MvNormalWeightedMeanPrecision([1.0, 2.0], Diagonal([6.0, 6.0]))
+
         @test eltype(MvNormalWeightedMeanPrecision([1.0, 1.0])) === Float64
         @test eltype(MvNormalWeightedMeanPrecision([1.0, 1.0], [1.0, 1.0])) === Float64
         @test eltype(MvNormalWeightedMeanPrecision([1, 1])) === Float64
@@ -91,6 +98,14 @@ using Distributions
         dist = MvNormalWeightedMeanPrecision(xi, Λ)
 
         @test prod(ProdAnalytical(), dist, dist) ≈ MvNormalWeightedMeanPrecision([0.40, 6.00, 8.00], [3.00 -0.20 0.20; -0.20 3.60 0.00; 0.20 0.00 7.00])
+
+        # diagonal covariance matrix/uniformscaling
+        @test prod(ProdAnalytical(), MvNormalWeightedMeanPrecision([-1, -1], [2 0; 0 2]), MvNormalWeightedMeanPrecision([1, 1], Diagonal([2, 4]))) ≈
+            MvNormalWeightedMeanPrecision([0, 0], [4, 6])
+        @test prod(ProdAnalytical(), MvNormalWeightedMeanPrecision([-1, -1], [2, 2]), MvNormalWeightedMeanPrecision([1, 1], Diagonal([2, 4]))) ≈
+            MvNormalWeightedMeanPrecision([0, 0], [4, 6])
+        @test prod(ProdAnalytical(), MvNormalWeightedMeanPrecision([-1, -1], 2 * I), MvNormalWeightedMeanPrecision([1, 1], Diagonal([2, 4]))) ≈
+            MvNormalWeightedMeanPrecision([0, 0], [4, 6])
     end
 
     @testset "Primitive types conversion" begin
diff --git a/test/distributions/test_normal_mean_precision.jl b/test/distributions/test_normal_mean_precision.jl
index 3db706ac..006ef088 100644
--- a/test/distributions/test_normal_mean_precision.jl
+++ b/test/distributions/test_normal_mean_precision.jl
@@ -3,6 +3,8 @@ module NormalMeanPrecisionTest
 using Test
 using ReactiveMP
 
+using LinearAlgebra: I
+
 @testset "NormalMeanPrecision" begin
     @testset "Constructor" begin
         @test NormalMeanPrecision <: NormalDistributionsFamily
@@ -20,6 +22,13 @@ using ReactiveMP
         @test NormalMeanPrecision(1.0f0, 2) == NormalMeanPrecision{Float32}(1.0f0, 2.0f0)
         @test NormalMeanPrecision(1.0f0, 2.0) == NormalMeanPrecision{Float64}(1.0, 2.0)
 
+        # uniformscaling
+        @test NormalMeanPrecision(2, I) == NormalMeanPrecision(2, 1)
+        @test NormalMeanPrecision(2, 6 * I) == NormalMeanPrecision(2, 6)
+        @test NormalMeanPrecision(2.0, I) == NormalMeanPrecision(2.0, 1.0)
+        @test NormalMeanPrecision(2.0, 6 * I) == NormalMeanPrecision(2.0, 6.0)
+        @test NormalMeanPrecision(2, 6.0 * I) == NormalMeanPrecision(2.0, 6.0)
+
         @test eltype(NormalMeanPrecision()) === Float64
         @test eltype(NormalMeanPrecision(0.0)) === Float64
         @test eltype(NormalMeanPrecision(0.0, 1.0)) === Float64
diff --git a/test/distributions/test_normal_mean_variance.jl b/test/distributions/test_normal_mean_variance.jl
index 27b27389..a17ff8a4 100644
--- a/test/distributions/test_normal_mean_variance.jl
+++ b/test/distributions/test_normal_mean_variance.jl
@@ -3,6 +3,8 @@ module NormalMeanVarianceTest
 using Test
 using ReactiveMP
 
+using LinearAlgebra: I
+
 @testset "NormalMeanVariance" begin
     @testset "Constructor" begin
         @test NormalMeanVariance <: NormalDistributionsFamily
@@ -20,6 +22,13 @@ using ReactiveMP
         @test NormalMeanVariance(1.0f0, 2) == NormalMeanVariance{Float32}(1.0f0, 2.0f0)
         @test NormalMeanVariance(1.0f0, 2.0) == NormalMeanVariance{Float64}(1.0, 2.0)
 
+        # uniformscaling
+        @test NormalMeanVariance(2, I) == NormalMeanVariance(2, 1)
+        @test NormalMeanVariance(2, 6 * I) == NormalMeanVariance(2, 6)
+        @test NormalMeanVariance(2.0, I) == NormalMeanVariance(2.0, 1.0)
+        @test NormalMeanVariance(2.0, 6 * I) == NormalMeanVariance(2.0, 6.0)
+        @test NormalMeanVariance(2, 6.0 * I) == NormalMeanVariance(2.0, 6.0)
+
         @test eltype(NormalMeanVariance()) === Float64
         @test eltype(NormalMeanVariance(0.0)) === Float64
         @test eltype(NormalMeanVariance(0.0, 1.0)) === Float64
diff --git a/test/distributions/test_normal_weighted_mean_precision.jl b/test/distributions/test_normal_weighted_mean_precision.jl
index ace5ebfd..5a90d97e 100644
--- a/test/distributions/test_normal_weighted_mean_precision.jl
+++ b/test/distributions/test_normal_weighted_mean_precision.jl
@@ -3,6 +3,8 @@ module NormalWeightedMeanPrecisionTest
 using Test
 using ReactiveMP
 
+using LinearAlgebra: I
+
 @testset "NormalWeightedMeanPrecision" begin
     @testset "Constructor" begin
         @test NormalWeightedMeanPrecision <: NormalDistributionsFamily
@@ -19,6 +21,13 @@ using ReactiveMP
         @test NormalWeightedMeanPrecision(1.0f0, 2.0f0) == NormalWeightedMeanPrecision{Float32}(1.0f0, 2.0f0)
         @test NormalWeightedMeanPrecision(1.0f0, 2.0) == NormalWeightedMeanPrecision{Float64}(1.0, 2.0)
 
+        # uniformscaling
+        @test NormalWeightedMeanPrecision(2, I) == NormalWeightedMeanPrecision(2, 1)
+        @test NormalWeightedMeanPrecision(2, 6 * I) == NormalWeightedMeanPrecision(2, 6)
+        @test NormalWeightedMeanPrecision(2.0, I) == NormalWeightedMeanPrecision(2.0, 1.0)
+        @test NormalWeightedMeanPrecision(2.0, 6 * I) == NormalWeightedMeanPrecision(2.0, 6.0)
+        @test NormalWeightedMeanPrecision(2, 6.0 * I) == NormalWeightedMeanPrecision(2.0, 6.0)
+
         @test eltype(NormalWeightedMeanPrecision()) === Float64
         @test eltype(NormalWeightedMeanPrecision(0.0)) === Float64
         @test eltype(NormalWeightedMeanPrecision(0.0, 1.0)) === Float64
diff --git a/test/distributions/test_pointmass.jl b/test/distributions/test_pointmass.jl
index 8f45a481..1a2e926b 100644
--- a/test/distributions/test_pointmass.jl
+++ b/test/distributions/test_pointmass.jl
@@ -5,6 +5,7 @@ using ReactiveMP
 using Distributions
 using Random
 using SpecialFunctions
+using LinearAlgebra: UniformScaling, I
 
 import ReactiveMP: CountingReal, tiny, huge
 import ReactiveMP.MacroHelpers: @test_inferred
@@ -163,6 +164,47 @@ import ReactiveMP: xtlog, mirrorlog
             @test @test_inferred(AbstractMatrix{T}, mean(loggamma, dist)) == loggamma.(matrix)
         end
     end
+
+    @testset "UniformScaling-based PointMass" begin
+        for T in (Float16, Float32, Float64, BigFloat)
+            matrix = convert(T, 5) * I
+            dist   = PointMass(matrix)
+
+            @test variate_form(dist) === Matrixvariate
+            @test dist[2, 1] == zero(T)
+            @test dist[3, 1] == zero(T)
+            @test dist[3, 3] === matrix[3, 3]
+
+            @test pdf(dist, matrix) == one(T)
+            @test pdf(dist, matrix + convert(T, tiny) * I) == zero(T)
+            @test pdf(dist, matrix - convert(T, tiny) * I) == zero(T)
+
+            @test logpdf(dist, matrix) == zero(T)
+            @test logpdf(dist, matrix + convert(T, tiny) * I) == convert(T, -Inf)
+            @test logpdf(dist, matrix - convert(T, tiny) * I) == convert(T, -Inf)
+
+            @test_throws MethodError insupport(dist, one(T))
+            @test_throws MethodError insupport(dist, ones(T, 2))
+            @test_throws MethodError pdf(dist, one(T))
+            @test_throws MethodError pdf(dist, ones(T, 2))
+            @test_throws MethodError logpdf(dist, one(T))
+            @test_throws MethodError logpdf(dist, ones(T, 2))
+
+            @test (@inferred entropy(dist)) == CountingReal(eltype(dist), -1)
+
+            @test mean(dist) == matrix
+            @test mode(dist) == matrix
+            @test var(dist) == zero(T) * I
+            @test std(dist) == zero(T) * I
+
+            @test_throws ErrorException cov(dist)
+            @test_throws ErrorException precision(dist)
+
+            @test_throws ErrorException probvec(dist)
+            @test mean(inv, dist) ≈ inv(matrix)
+            @test mean(cholinv, dist) ≈ inv(matrix)
+        end
+    end
 end
 
 end
diff --git a/test/rules/bernoulli/test_marginals.jl b/test/rules/bernoulli/test_marginals.jl
index da6be1e2..d1b835f8 100644
--- a/test/rules/bernoulli/test_marginals.jl
+++ b/test/rules/bernoulli/test_marginals.jl
@@ -13,5 +13,12 @@ import ReactiveMP: @test_marginalrules
             (input = (m_out = PointMass(0.0), m_p = Beta(1.0, 2.0)), output = (out = PointMass(0.0), p = Beta(1.0, 3.0)))
         ]
     end
+    @testset "out_p: (m_out::Bernoulli, m_p::PointMass)" begin
+        @test_marginalrules [with_float_conversions = true] Bernoulli(:out_p) [
+            (input = (m_out = Bernoulli(0.8), m_p = PointMass(1.0)), output = (out = Bernoulli(1.0), p = PointMass(1.0))),
+            (input = (m_out = Bernoulli(0.2), m_p = PointMass(1.0)), output = (out = Bernoulli(1.0), p = PointMass(1.0))),
+            (input = (m_out = Bernoulli(0.2), m_p = PointMass(0.0)), output = (out = Bernoulli(0.0), p = PointMass(0.0)))
+        ]
+    end
 end
 end
diff --git a/test/rules/bernoulli/test_p.jl b/test/rules/bernoulli/test_p.jl
index c6c34c34..f5b08bf7 100644
--- a/test/rules/bernoulli/test_p.jl
+++ b/test/rules/bernoulli/test_p.jl
@@ -19,7 +19,7 @@ import ReactiveMP: @test_rules
     end
 
     @testset "Variational Message Passing: (q_out::DiscreteNonParametric)" begin
-        # `with_falot_conversions = false` here is because apparently 
+        # `with_float_conversions = false` here is because apparently 
         # BigFloat(0.7) + BigFloat(0.3) != BigFloat(1.0)
         @test_rules [with_float_conversions = false] Bernoulli(:p, Marginalisation) [
             (input = (q_out = Categorical([0.0, 1.0]),), output = Beta(2.0, 1.0)), (input = (q_out = Categorical([0.7, 0.3]),), output = Beta(13 / 10, 17 / 10))
diff --git a/test/rules/categorical/test_marginals.jl b/test/rules/categorical/test_marginals.jl
new file mode 100644
index 00000000..43363b5c
--- /dev/null
+++ b/test/rules/categorical/test_marginals.jl
@@ -0,0 +1,25 @@
+module RulesCategoricalMarginalsTest
+
+using Test
+using ReactiveMP
+using Random
+using LinearAlgebra
+import ReactiveMP: @test_marginalrules
+
+@testset "marginalrules:Categorical" begin
+    @testset "out_p: (m_out::PointMass, m_p::Dirichlet)" begin
+        @test_marginalrules [with_float_conversions = true] Categorical(:out_p) [
+            (input = (m_out = PointMass([0.0, 1.0]), m_p = Dirichlet([2.0, 1.0])), output = (out = PointMass([0.0, 1.0]), p = Dirichlet([2.0, 2.0]))),
+            (input = (m_out = PointMass([0.0, 1.0]), m_p = Dirichlet([4.0, 2.0])), output = (out = PointMass([0.0, 1.0]), p = Dirichlet([4.0, 3.0]))),
+            (input = (m_out = PointMass([1.0, 0.0]), m_p = Dirichlet([1.0, 2.0])), output = (out = PointMass([1.0, 0.0]), p = Dirichlet([2.0, 2.0])))
+        ]
+    end
+    @testset "out_p: (m_out::Categorical, m_p::PointMass)" begin
+        @test_marginalrules [with_float_conversions = false] Categorical(:out_p) [
+            (input = (m_out = Categorical([0.2, 0.8]), m_p = PointMass([0.0, 1.0])), output = (out = Categorical(normalize([tiny, 0.8], 1)), p = PointMass([0.0, 1.0]))),
+            (input = (m_out = Categorical([0.8, 0.2]), m_p = PointMass([0.0, 1.0])), output = (out = Categorical(normalize([tiny, 0.2], 1)), p = PointMass([0.0, 1.0]))),
+            (input = (m_out = Categorical([0.8, 0.2]), m_p = PointMass([1.0, 0.0])), output = (out = Categorical(normalize([0.8, tiny], 1)), p = PointMass([1.0, 0.0])))
+        ]
+    end
+end
+end
diff --git a/test/rules/categorical/test_out.jl b/test/rules/categorical/test_out.jl
new file mode 100644
index 00000000..929ffbc2
--- /dev/null
+++ b/test/rules/categorical/test_out.jl
@@ -0,0 +1,27 @@
+module RulesCategoricalOutTest
+
+using Test
+using ReactiveMP
+using Random
+import ReactiveMP: @test_rules
+
+@testset "rules:Categorical:out" begin
+    @testset "Belief Propagation: (m_p::PointMass)" begin
+        @test_rules [with_float_conversions = false] Categorical(:out, Marginalisation) [
+            (input = (m_p = PointMass([0.0, 1.0]),), output = Categorical([0.0, 1.0])), (input = (m_p = PointMass([0.8, 0.2]),), output = Categorical([0.8, 0.2]))
+        ]
+    end
+
+    @testset "Variational Message Passing: (q_p::PointMass)" begin
+        @test_rules [with_float_conversions = false] Categorical(:out, Marginalisation) [
+            (input = (q_p = PointMass([0.0, 1.0]),), output = Categorical([0.0, 1.0])), (input = (q_p = PointMass([0.7, 0.3]),), output = Categorical([0.7, 0.3]))
+        ]
+    end
+
+    @testset "Variational Message Passing: (q_p::Dirichlet)" begin
+        @test_rules [with_float_conversions = false] Categorical(:out, Marginalisation) [
+            (input = (q_p = Dirichlet([1.0, 1.0]),), output = Categorical([0.5, 0.5])), (input = (q_p = Dirichlet([0.2, 0.2]),), output = Categorical([0.5, 0.5]))
+        ]
+    end
+end
+end
diff --git a/test/rules/categorical/test_p.jl b/test/rules/categorical/test_p.jl
new file mode 100644
index 00000000..55e396ca
--- /dev/null
+++ b/test/rules/categorical/test_p.jl
@@ -0,0 +1,21 @@
+module RulesCategoricalPTest
+
+using Test
+using ReactiveMP
+using Random
+import ReactiveMP: @test_rules
+
+@testset "rules:Categorical:p" begin
+    @testset "Variational Message Passing: (q_out::PointMass)" begin
+        @test_rules [with_float_conversions = true] Categorical(:p, Marginalisation) [
+            (input = (q_out = PointMass([0.0, 1.0]),), output = Dirichlet([1.0, 2.0])), (input = (q_out = PointMass([0.8, 0.2]),), output = Dirichlet([9 / 5, 12 / 10]))
+        ]
+    end
+
+    @testset "Variational Message Passing: (q_out::Categorical)" begin
+        @test_rules [with_float_conversions = false] Categorical(:p, Marginalisation) [
+            (input = (q_out = Categorical([0.0, 1.0]),), output = Dirichlet([1.0, 2.0])), (input = (q_out = Categorical([0.7, 0.3]),), output = Dirichlet([17 / 10, 13 / 10]))
+        ]
+    end
+end
+end
diff --git a/test/rules/dirichlet/test_marginals.jl b/test/rules/dirichlet/test_marginals.jl
new file mode 100644
index 00000000..51682227
--- /dev/null
+++ b/test/rules/dirichlet/test_marginals.jl
@@ -0,0 +1,17 @@
+module RulesDirichletMarginalsTest
+
+using Test
+using ReactiveMP
+using Random
+import ReactiveMP: @test_marginalrules
+
+@testset "marginalrules:Dirichlet" begin
+    @testset "out_a: (m_out::Dirichlet, m_a::PointMass)" begin
+        @test_marginalrules [with_float_conversions = true] Dirichlet(:out_a) [
+            (input = (m_out = Dirichlet([1.0, 2.0]), m_a = PointMass([0.2, 1.0])), output = (out = Dirichlet([0.2, 2.0]), a = PointMass([0.2, 1.0]))),
+            (input = (m_out = Dirichlet([2.0, 2.0]), m_a = PointMass([2.0, 0.5])), output = (out = Dirichlet([3.0, 1.5]), a = PointMass([2.0, 0.5]))),
+            (input = (m_out = Dirichlet([2.0, 3.0]), m_a = PointMass([3.0, 1.0])), output = (out = Dirichlet([4.0, 3.0]), a = PointMass([3.0, 1.0])))
+        ]
+    end
+end
+end
diff --git a/test/rules/dirichlet/test_out.jl b/test/rules/dirichlet/test_out.jl
new file mode 100644
index 00000000..874a5885
--- /dev/null
+++ b/test/rules/dirichlet/test_out.jl
@@ -0,0 +1,25 @@
+module RulesDirichletOutTest
+
+using Test
+using ReactiveMP
+using Random
+import ReactiveMP: @test_rules
+
+@testset "rules:Dirichlet:out" begin
+    @testset "Belief Propagation: (m_a::PointMass)" begin
+        @test_rules [with_float_conversions = true] Dirichlet(:out, Marginalisation) [
+            (input = (m_a = PointMass([0.2, 1.0]),), output = Dirichlet([0.2, 1.0])),
+            (input = (m_a = PointMass([2.0, 0.5]),), output = Dirichlet([2.0, 0.5])),
+            (input = (m_a = PointMass([3.0, 1.0]),), output = Dirichlet([3.0, 1.0]))
+        ]
+    end
+
+    @testset "Variational Message Passing: (q_a::PointMass)" begin
+        @test_rules [with_float_conversions = true] Dirichlet(:out, Marginalisation) [
+            (input = (q_a = PointMass([0.2, 1.0]),), output = Dirichlet([0.2, 1.0])),
+            (input = (q_a = PointMass([2.0, 0.5]),), output = Dirichlet([2.0, 0.5])),
+            (input = (q_a = PointMass([3.0, 1.0]),), output = Dirichlet([3.0, 1.0]))
+        ]
+    end
+end
+end
diff --git a/test/runtests.jl b/test/runtests.jl
index 2e50971e..0c2fa1e2 100644
--- a/test/runtests.jl
+++ b/test/runtests.jl
@@ -312,6 +312,10 @@ end
     addtests(testrunner, "rules/beta/test_out.jl")
     addtests(testrunner, "rules/beta/test_marginals.jl")
 
+    addtests(testrunner, "rules/categorical/test_out.jl")
+    addtests(testrunner, "rules/categorical/test_p.jl")
+    addtests(testrunner, "rules/categorical/test_marginals.jl")
+
     addtests(testrunner, "rules/delta/unscented/test_out.jl")
     addtests(testrunner, "rules/delta/unscented/test_in.jl")
     addtests(testrunner, "rules/delta/unscented/test_marginals.jl")
@@ -324,6 +328,9 @@ end
     addtests(testrunner, "rules/delta/cvi/test_marginals.jl")
     addtests(testrunner, "rules/delta/cvi/test_out.jl")
 
+    addtests(testrunner, "rules/dirichlet/test_marginals.jl")
+    addtests(testrunner, "rules/dirichlet/test_out.jl")
+
     addtests(testrunner, "rules/dot_product/test_out.jl")
     addtests(testrunner, "rules/dot_product/test_in1.jl")
     addtests(testrunner, "rules/dot_product/test_in2.jl")
diff --git a/test/variables/test_constant.jl b/test/variables/test_constant.jl
index 8453b723..0e349414 100644
--- a/test/variables/test_constant.jl
+++ b/test/variables/test_constant.jl
@@ -4,13 +4,15 @@ using Test
 using ReactiveMP
 using Rocket
 
+using LinearAlgebra: I
+
 import ReactiveMP: collection_type, VariableIndividual, VariableVector, VariableArray, linear_index
 import ReactiveMP: getconst, proxy_variables
 import ReactiveMP: israndom, isproxy
 
 @testset "ConstVariable" begin
     @testset "Simple creation" begin
-        for sym in (:x, :y, :z), value in (1.0, 1.0, "asd", [1.0, 1.0], [1.0 0.0; 0.0 1.0], (x) -> 1)
+        for sym in (:x, :y, :z), value in (1.0, 1.0, "asd", I, 0.3 * I, [1.0, 1.0], [1.0 0.0; 0.0 1.0], (x) -> 1)
             v = constvar(sym, value)
 
             @test !israndom(v)
diff --git a/test/variables/test_data.jl b/test/variables/test_data.jl
index 683cf1a1..42b517a5 100644
--- a/test/variables/test_data.jl
+++ b/test/variables/test_data.jl
@@ -4,10 +4,10 @@ using Test
 using ReactiveMP
 using Rocket
 
-import ReactiveMP: DataVariableCreationOptions
+import ReactiveMP: DataVariableCreationOptions, MessageObservable
 import ReactiveMP: collection_type, VariableIndividual, VariableVector, VariableArray, linear_index
 import ReactiveMP: getconst, proxy_variables
-import ReactiveMP: israndom, isproxy, allows_missings
+import ReactiveMP: israndom, isproxy, isused, isconnected, setmessagein!, allows_missings
 
 @testset "DataVariable" begin
     @testset "Simple creation" begin
@@ -44,10 +44,20 @@ import ReactiveMP: israndom, isproxy, allows_missings
             @test !israndom(variable)
             @test eltype(variable) === T
             @test name(variable) === sym
+            @test allows_missings(variable) === allow_missings
             @test collection_type(variable) isa VariableIndividual
             @test proxy_variables(variable) === nothing
             @test !isproxy(variable)
-            @test allows_missings(variable) === allow_missings
+            @test !isused(variable)
+            @test !isconnected(variable)
+
+            setmessagein!(variable, 1, MessageObservable())
+
+            @test isused(variable)
+            @test isconnected(variable)
+
+            # `100` could a valid index, but messages should be initialized in order, previous was `1`
+            @test_throws ErrorException setmessagein!(variable, 100, MessageObservable())
         end
 
         for sym in (:x, :y, :z), T in (Float64, Int64, Vector{Float64}), n in (10, 20), allow_missings in (true, false)
@@ -59,22 +69,24 @@ import ReactiveMP: israndom, isproxy, allows_missings
             @test variables isa Vector
             @test all(v -> !israndom(v), variables)
             @test all(v -> name(v) === sym, variables)
+            @test all(v -> allows_missings(v) === allow_missings, variables)
             @test all(v -> collection_type(v) isa VariableVector, variables)
             @test all(t -> linear_index(collection_type(t[2])) === t[1], enumerate(variables))
             @test all(v -> eltype(v) === T, variables)
             @test !isproxy(variables)
             @test all(v -> !isproxy(v), variables)
+            @test all(v -> !isused(v), variables)
+            @test all(v -> !isconnected(v), variables)
             @test test_updates(variables, T, (n,))
 
-            @test all(v -> allows_missings(v) === allow_missings, variables)
-            if allow_missings
-                test_updates(variables, Missing, (n,))
-            end
+            foreach(v -> setmessagein!(v, 1, MessageObservable()), variables)
+
+            @test all(v -> isused(v), variables)
+            @test all(v -> isconnected(v), variables)
         end
 
-        for sym in (:x, :y, :z), T in (Float64, Int64, Vector{Float64}), l in (10, 20), r in (10, 20), allow_missings in (true, false)
-            options = DataVariableCreationOptions(T, nothing, Val(allow_missings))
-            for variables in (datavar(options, sym, T, l, r), datavar(options, sym, T, (l, r)))
+        for sym in (:x, :y, :z), T in (Float64, Int64, Vector{Float64}), l in (10, 20), r in (10, 20)
+            for variables in (datavar(sym, T, l, r), datavar(sym, T, (l, r)))
                 @test !israndom(variables)
                 @test size(variables) === (l, r)
                 @test length(variables) === l * r
@@ -86,12 +98,13 @@ import ReactiveMP: israndom, isproxy, allows_missings
                 @test all(v -> eltype(v) === T, variables)
                 @test !isproxy(variables)
                 @test all(v -> !isproxy(v), variables)
+                @test all(v -> !isused(v), variables)
                 @test test_updates(variables, T, (l, r))
 
-                @test all(v -> allows_missings(v) === allow_missings, variables)
-                if allow_missings
-                    test_updates(variables, Missing, (l, r))
-                end
+                foreach(v -> setmessagein!(v, 1, MessageObservable()), variables)
+
+                @test all(v -> isused(v), variables)
+                @test all(v -> isconnected(v), variables)
             end
         end
     end
