var documenterSearchIndex = {"docs":
[{"location":"examples/custom_nonlinear_node/#examples-custom-nonlinear-node","page":"Custom Nonlinear Node","title":"Example: Custom nonlinear node","text":"","category":"section"},{"location":"examples/custom_nonlinear_node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"using ReactiveMP, Distributions, Random, BenchmarkTools, Rocket, GraphPPL, StableRNGs","category":"page"},{"location":"examples/custom_nonlinear_node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"Here is an example of creating custom node with nonlinear function approximation with samplelist.","category":"page"},{"location":"examples/custom_nonlinear_node/#Custom-node-structure","page":"Custom Nonlinear Node","title":"Custom node structure","text":"","category":"section"},{"location":"examples/custom_nonlinear_node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"struct NonlinearNode end # Dummy structure just to make Julia happy\n\nstruct NonlinearMeta{R, F}\n    rng      :: R\n    fn       :: F   # Nonlinear function, we assume 1 float input - 1 float ouput\n    nsamples :: Int # Number of samples used in approximation\nend","category":"page"},{"location":"examples/custom_nonlinear_node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"@node NonlinearNode Deterministic [ out, in ]","category":"page"},{"location":"examples/custom_nonlinear_node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"We need to define two Sum-product message computation rules for our new custom node","category":"page"},{"location":"examples/custom_nonlinear_node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"Rule for outbound message on out edge given inbound message on in edge\nRule for outbound message on in edge given inbound message on out edge\nBoth rules accept optional meta object","category":"page"},{"location":"examples/custom_nonlinear_node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"# Rule for outbound message on `out` edge given inbound message on `in` edge\n@rule NonlinearNode(:out, Marginalisation) (m_in::NormalMeanVariance, meta::NonlinearMeta) = begin \n    samples = rand(meta.rng, m_in, meta.nsamples)\n    return SampleList(map(meta.fn, samples))\nend\n\n# Rule for outbound message on `in` edge given inbound message on `out` edge\n@rule NonlinearNode(:in, Marginalisation) (m_out::Gamma, meta::NonlinearMeta) = begin     \n    return ContinuousUnivariateLogPdf((x) -> logpdf(m_out, meta.fn(x)))\nend","category":"page"},{"location":"examples/custom_nonlinear_node/#Model-specification","page":"Custom Nonlinear Node","title":"Model specification","text":"","category":"section"},{"location":"examples/custom_nonlinear_node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"After we have defined our custom node with custom rules we may proceed with a model specification:","category":"page"},{"location":"examples/custom_nonlinear_node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"beginequation\nbeginaligned\np(theta) = mathcalN(thetamu_theta sigma_theta)\np(m) = mathcalN(thetamu_m sigma_m)\np(w) = f(theta)\np(y_im w) = mathcalN(y_im w)\nendaligned\nendequation","category":"page"},{"location":"examples/custom_nonlinear_node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"Given this IID model, we aim to estimate the precision of a Gaussian distribution. We pass a random variable theta through a non-linear transformation f to make it positive and suitable for a precision parameter of a Gaussian distribution. We, later on, will estimate the posterior of theta. ","category":"page"},{"location":"examples/custom_nonlinear_node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"@model function nonlinear_estimation(n)\n    \n    θ ~ NormalMeanVariance(0.0, 100.0)\n    m ~ NormalMeanVariance(0.0, 1.0)\n    \n    w ~ NonlinearNode(θ)\n    \n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        y[i] ~ NormalMeanPrecision(m, w)\n    end\n    \n    return θ, m, w, y\nend","category":"page"},{"location":"examples/custom_nonlinear_node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"@constraints function nconstsraints(nsamples)\n    q(θ) :: SampleList(nsamples, LeftProposal())\n    q(w) :: SampleList(nsamples, RightProposal())\n    \n    q(θ, w, m) = q(θ)q(m)q(w)\nend","category":"page"},{"location":"examples/custom_nonlinear_node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"# TODO: check\n@meta function nmeta(fn, nsamples)\n    NonlinearNode(θ, w) -> NonlinearMeta(StableRNG(123), fn, nsamples)\nend","category":"page"},{"location":"examples/custom_nonlinear_node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"Here we generate some data with some arbitrary nonlinearity for precision parameter:","category":"page"},{"location":"examples/custom_nonlinear_node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"nonlinear_fn(x) = abs(exp(x) * sin(x))","category":"page"},{"location":"examples/custom_nonlinear_node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"seed = 123\nrng  = MersenneTwister(seed)\n\nniters   = 15 # Number of VMP iterations\nnsamples = 5_000 # Number of samples in approximation\n\nn = 500 # Number of IID samples\nμ = -10.0\nθ = -1.0\nw = nonlinear_fn(θ)\n\ndata = rand(rng, NormalMeanPrecision(μ, w), n)\nnothing #hide","category":"page"},{"location":"examples/custom_nonlinear_node/#Inference","page":"Custom Nonlinear Node","title":"Inference","text":"","category":"section"},{"location":"examples/custom_nonlinear_node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"result = inference(\n    model = Model(nonlinear_estimation, n),\n    meta =  nmeta(nonlinear_fn, nsamples),\n    constraints = nconstsraints(nsamples),\n    data = (y = data, ), \n    initmarginals = (m = vague(NormalMeanPrecision), w = vague(Gamma)),\n    returnvars = (θ = KeepLast(), ),\n    iterations = niters,  \n    showprogress = true\n)","category":"page"},{"location":"examples/custom_nonlinear_node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"using Plots, StatsPlots\n\nestimated = Normal(mean_std(result.posteriors[:θ])...)\n\nplot(estimated, title=\"Posterior for θ\", label = \"Estimated\", legend = :bottomright, fill = true, fillopacity = 0.2, xlim = (-3, 3), ylim = (0, 2))\nvline!([ θ ], label = \"Real value of θ\")","category":"page"},{"location":"examples/custom_nonlinear_node/#Benchmark","page":"Custom Nonlinear Node","title":"Benchmark","text":"","category":"section"},{"location":"examples/custom_nonlinear_node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"@benchmark inference(\n    model = $(Model(nonlinear_estimation, n)),\n    meta = $(nmeta(nonlinear_fn, nsamples)),\n    constraints = $(nconstsraints(nsamples)),\n    data = (y = $data, ), \n    initmarginals = (m = vague(NormalMeanPrecision), w = vague(Gamma)),\n    returnvars = (θ = KeepLast(), ),\n    iterations = $niters,  \n    showprogress = true\n)","category":"page"},{"location":"examples/linear_regression/#examples-linear-regression","page":"Linear Regression","title":"Example: Linear Regression","text":"","category":"section"},{"location":"examples/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"In this example we are going to perform a simple linear regression problem, but in the Bayesian setting. We specify the model:","category":"page"},{"location":"examples/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"y_i = a * x_i + b","category":"page"},{"location":"examples/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"where a and b are random variables with some vague priors.","category":"page"},{"location":"examples/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"First step is to import all needed packages and define the model:","category":"page"},{"location":"examples/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"using ReactiveMP, GraphPPL, Rocket, Random, Plots, StableRNGs, BenchmarkTools","category":"page"},{"location":"examples/linear_regression/#Model-specification","page":"Linear Regression","title":"Model specification","text":"","category":"section"},{"location":"examples/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"@model function linear_regression(n)\n    a ~ NormalMeanVariance(0.0, 1.0)\n    b ~ NormalMeanVariance(0.0, 100.0)\n    \n    x = datavar(Float64, n)\n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        y[i] ~ NormalMeanVariance(a * x[i] + b, 1.0)\n    end\n    \n    return a, b, x, y\nend","category":"page"},{"location":"examples/linear_regression/#Dataset","page":"Linear Regression","title":"Dataset","text":"","category":"section"},{"location":"examples/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"In order to test our inference procedure we create a test dataset where observations are corrupted with gaussian white noise (with known variance).","category":"page"},{"location":"examples/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"reala = 0.5\nrealb = 25\n\nN = 250\n\nrng = StableRNG(1234)\n\nxorig = collect(1:N)\n\nxdata = xorig .+ randn(rng, N)\nydata = realb .+ reala .* xorig .+ randn(rng, N);\n\nplot(xdata, label = \"X\", title = \"Linear regression dataset\")\nplot!(ydata, label = \"Y\")","category":"page"},{"location":"examples/linear_regression/#Inference","page":"Linear Regression","title":"Inference","text":"","category":"section"},{"location":"examples/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"results = inference(\n    model = Model(linear_regression, length(xdata)), \n    data  = (y = ydata, x = xdata), \n    initmessages = (b = NormalMeanVariance(0.0, 100.0), ), \n    returnvars   = (a = KeepLast(), b = KeepLast()), \n    iterations   = 20\n);","category":"page"},{"location":"examples/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"pra = plot(range(-3, 3, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 1.0), x), title=\"Prior for a parameter\", fillalpha=0.3, fillrange = 0, label=\"Prior P(a)\", c=1,)\npra = vline!(pra, [ reala ], label=\"Real a\", c = 3)\npsa = plot(range(0.45, 0.55, length = 1000), (x) -> pdf(results.posteriors[:a], x), title=\"Posterior for a parameter\", fillalpha=0.3, fillrange = 0, label=\"Posterior P(a)\", c=2,)\npsa = vline!(psa, [ reala ], label=\"Real a\", c = 3)\n\nplot(pra, psa, size = (1000, 200))","category":"page"},{"location":"examples/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"prb = plot(range(-40, 40, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 100.0), x), title=\"Prior for b parameter\", fillalpha=0.3, fillrange = 0, label=\"Prior P(b)\", c=1, legend = :topleft)\nprb = vline!(prb, [ realb ], label=\"Real b\", c = 3)\npsb = plot(range(23, 28, length = 1000), (x) -> pdf(results.posteriors[:b], x), title=\"Posterior for b parameter\", fillalpha=0.3, fillrange = 0, label=\"Posterior P(b)\", c=2, legend = :topleft)\npsb = vline!(psb, [ realb ], label=\"Real b\", c = 3)\n\nplot(prb, psb, size = (1000, 200))","category":"page"},{"location":"examples/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"a = results.posteriors[:a]\nb = results.posteriors[:b]\n\nprintln(\"Real a: \", reala, \" | Estimated a: \", mean(a), \" | Error: \", abs(mean(a) - reala))\nprintln(\"Real b: \", realb, \" | Estimated b: \", mean(b), \" | Error: \", abs(mean(b) - realb))\nnothing #hide","category":"page"},{"location":"examples/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"We can see that ReactiveMP.jl estimated real values of linear regression coefficients with high precision. Lets also test the benchmark of the resulting inference procedure.","category":"page"},{"location":"examples/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"@benchmark inference(\n    model = Model($linear_regression, length($xdata)), \n    data  = (y = $ydata, x = $xdata), \n    initmessages = (b = NormalMeanVariance(0.0, 100.0), ), \n    returnvars   = (a = KeepLast(), b = KeepLast()), \n    iterations = 20\n)","category":"page"},{"location":"lib/message/#lib-message","page":"Messages","title":"Messages implementation","text":"","category":"section"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"In message passing framework one of the most important concept is (wow!) messages. Messages flow on edges of a factor graph and usually hold some information in a form of probability distribution. In ReactiveMP.jl we distinguish two major types of messages: Belief Propagation and Variational.  ","category":"page"},{"location":"lib/message/#Abstract-message-type","page":"Messages","title":"Abstract message type","text":"","category":"section"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"Both belief propagation and variational messages are subtypes of a AbstractMessage supertype.","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"AbstractMessage","category":"page"},{"location":"lib/message/#ReactiveMP.AbstractMessage","page":"Messages","title":"ReactiveMP.AbstractMessage","text":"AbstractMessage\n\nAn abstract supertype for all concrete message types.\n\nSee also: Message\n\n\n\n\n\n","category":"type"},{"location":"lib/message/#lib-belief-propagation-message","page":"Messages","title":"Belief-Propagation (or Sum-Product) message","text":"","category":"section"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"Belief propagation messages are encoded with type Message. ","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"(Image: message) Belief propagation message","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"Message","category":"page"},{"location":"lib/message/#ReactiveMP.Message","page":"Messages","title":"ReactiveMP.Message","text":"Message{D} <: AbstractMessage\n\nMessage structure encodes a Belief Propagation message, which holds some data that usually a probability distribution, but can also be an arbitrary object. Message acts as a proxy structure to data object and proxies most of the statistical functions, e.g. mean, mode, cov etc.\n\nArguments\n\ndata::D: message always holds some data object associated with it\nis_clamped::Bool, specifies if this message is clamped\nis_initial::Bool, specifies if this message is initial\n\nExample\n\njulia> distribution = Gamma(10.0, 2.0)\nGamma{Float64}(α=10.0, θ=2.0)\n\njulia> message = Message(distribution, false, true)\nMessage(Gamma{Float64}(α=10.0, θ=2.0))\n\njulia> mean(message) \n20.0\n\njulia> getdata(message)\nGamma{Float64}(α=10.0, θ=2.0)\n\njulia> is_clamped(message)\nfalse\n\njulia> is_initial(message)\ntrue\n\n\nSee also: AbstractMessage, materialize!\n\n\n\n\n\n","category":"type"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"From implementation point a view Message structure does nothing but holds some data object and redirects most of the statistical related functions to that data object. However it used extensively in Julia's multiple dispatch. Implementation also uses extra is_initial and is_clamped fields to determine if product of two messages results in is_initial or is_clamped posterior marginal.","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"distribution = NormalMeanPrecision(0.0, 1.0)\nmessage      = Message(distribution, false, true)","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"mean(message), precision(message)","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"logpdf(message, 1.0)","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"is_clamped(message), is_initial(message)","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"User should not really interact with Message structure while working with ReactiveMP unless doing some advanced inference procedures that involves prediction.","category":"page"},{"location":"lib/message/#lib-variational-message","page":"Messages","title":"Variational message","text":"","category":"section"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"Variational messages are encoded with type VariationalMessage.","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"(Image: message) Variational message with structured factorisation q(x, y)q(z) assumption","category":"page"},{"location":"lib/form/#lib-forms","page":"Functional forms","title":"Built-in Functional Forms","text":"","category":"section"},{"location":"lib/form/","page":"Functional forms","title":"Functional forms","text":"This section describes built-in functional forms that can be used for posterior marginal and/or messages form constraints specification. Read more information about constraints specification syntax in the Constraints Specification section. An important part of the functional forms constraint implementation is the prod function. More information about prod function is present in the Prod Implementation section.","category":"page"},{"location":"lib/form/#lib-forms-custom-constraints","page":"Functional forms","title":"Custom functional forms","text":"","category":"section"},{"location":"lib/form/","page":"Functional forms","title":"Functional forms","text":"See the Custom functional form specification section for more information about defining novel custom functional forms that are compatible with ReactiveMP inference backend.","category":"page"},{"location":"lib/form/#lib-forms-unspecified-constraint","page":"Functional forms","title":"UnspecifiedFormConstraint","text":"","category":"section"},{"location":"lib/form/","page":"Functional forms","title":"Functional forms","text":"Unspecified functional form constraint is used by default and uses only analytical update rules for computing posterior marginals. Throws an error if a product of two colliding messages cannot be computed analytically.","category":"page"},{"location":"lib/form/","page":"Functional forms","title":"Functional forms","text":"@constraints begin \n    q(x) :: Nothing # This is the default setting\nend","category":"page"},{"location":"lib/form/","page":"Functional forms","title":"Functional forms","text":"UnspecifiedFormConstraint","category":"page"},{"location":"lib/form/#ReactiveMP.UnspecifiedFormConstraint","page":"Functional forms","title":"ReactiveMP.UnspecifiedFormConstraint","text":"UnspecifiedFormConstraint\n\nOne of the form constraint objects. Does not imply any form constraints and simply returns the same object as receives. However it does not allow DistProduct to be a valid functional form in the inference backend.\n\nTraits\n\nis_point_mass_form_constraint = false\ndefault_form_check_strategy   = FormConstraintCheckLast()\ndefault_prod_constraint       = ProdAnalytical()\nmake_form_constraint          = Nothing (for use in @constraints macro)\n\nSee also: constrain_form, DistProduct\n\n\n\n\n\n","category":"type"},{"location":"lib/form/#lib-forms-point-mass-constraint","page":"Functional forms","title":"PointMassFormConstraint","text":"","category":"section"},{"location":"lib/form/","page":"Functional forms","title":"Functional forms","text":"The most basic form of posterior marginal approximation is the PointMass function. In a few words PointMass represents delta function. In the context of functional form constraints PointMass approximation corresponds to the MAP estimate. For a given distribution d - PointMass functional form simply finds the argmax of the logpdf(d, x) by default. ","category":"page"},{"location":"lib/form/","page":"Functional forms","title":"Functional forms","text":"@constraints begin \n    q(x) :: PointMass # Materializes to the `PointMassFormConstraint` object\nend","category":"page"},{"location":"lib/form/","page":"Functional forms","title":"Functional forms","text":"PointMassFormConstraint","category":"page"},{"location":"lib/form/#ReactiveMP.PointMassFormConstraint","page":"Functional forms","title":"ReactiveMP.PointMassFormConstraint","text":"PointMassFormConstraint\n\nOne of the form constraint objects. Constraint a message to be in a form of dirac's delta point mass.  By default uses Optim.jl package to find argmin of -logpdf(x).  Accepts custom optimizer callback which might be used to customise optimisation procedure with different packages  or different arguments for Optim.jl package.\n\nKeyword arguments\n\noptimizer: specifies a callback function for logpdf optimisation. See also: ReactiveMP.default_point_mass_form_constraint_optimizer\nstarting_point: specifies a callback function for initial optimisation point: See also: ReactiveMP.default_point_mass_form_constraint_starting_point\nboundaries: specifies a callback function for determining optimisation boundaries: See also: ReactiveMP.default_point_mass_form_constraint_boundaries\n\nCustom optimizer callback interface\n\n# This is an example of the `custom_optimizer` interface\nfunction custom_optimizer(::Type{ Univariate }, ::Type{ Continuous }, constraint::PointMassFormConstraint, distribution)\n    # should return argmin of the -logpdf(distribution)\nend\n\nCustom starting point callback interface\n\n# This is an example of the `custom_starting_point` interface\nfunction custom_starting_point(::Type{ Univariate }, ::Type{ Continuous }, constraint::PointMassFormConstraint, distribution)\n    # built-in optimizer expects an array, even for a univariate distribution\n    return [ 0.0 ] \nend\n\nCustom boundaries callback interface\n\n# This is an example of the `custom_boundaries` interface\nfunction custom_boundaries(::Type{ Univariate }, ::Type{ Continuous }, constraint::PointMassFormConstraint, distribution)\n    # returns a tuple of `lower` and `upper` boundaries\n    return (-Inf, Inf)\nend\n\nTraits\n\nis_point_mass_form_constraint = true\ndefault_form_check_strategy   = FormConstraintCheckLast()\ndefault_prod_constraint       = ProdGeneric()\nmake_form_constraint          = PointMass (for use in @constraints macro)\n\nSee also: constrain_form, DistProduct\n\n\n\n\n\n","category":"type"},{"location":"lib/form/#lib-forms-sample-list-constraint","page":"Functional forms","title":"SampleListFormConstraint","text":"","category":"section"},{"location":"lib/form/","page":"Functional forms","title":"Functional forms","text":"SampleListFormConstraints approximates the resulting posterior marginal (product of two colliding messages) as a list of weighted samples. Hence, it requires one of the arguments to be a proper distribution (or at least be able to sample from it). This setting is controlled with LeftProposal(), RightProposal() or AutoProposal() objects. It is worth to note that SampleListFormConstraints approximates only DistProduct object and leaves any other distribution untouched. It also accepts an optional method object, but the only one available sampling method currently is the BootstrapImportanceSampling.","category":"page"},{"location":"lib/form/","page":"Functional forms","title":"Functional forms","text":"@constraints begin \n    q(x) :: SampleList(1000)\n    # or \n    q(y) :: SampleList(1000, LeftProposal())\nend","category":"page"},{"location":"lib/form/","page":"Functional forms","title":"Functional forms","text":"SampleListFormConstraint","category":"page"},{"location":"lib/form/#ReactiveMP.SampleListFormConstraint","page":"Functional forms","title":"ReactiveMP.SampleListFormConstraint","text":"SampleListFormConstraint(rng, strategy, method)\n\nOne of the form constraint objects. Approximates DistProduct with a SampleList object. \n\nTraits\n\nis_point_mass_form_constraint = false\ndefault_form_check_strategy   = FormConstraintCheckLast()\ndefault_prod_constraint       = ProdGeneric()\nmake_form_constraint          = SampleList (for use in @constraints macro)\n\nSee also: constrain_form, DistProduct\n\n\n\n\n\n","category":"type"},{"location":"lib/form/#lib-forms-fixed-marginal-constraint","page":"Functional forms","title":"FixedMarginalFormConstraint","text":"","category":"section"},{"location":"lib/form/","page":"Functional forms","title":"Functional forms","text":"Fixed marginal form constraint replaces the resulting posterior marginal obtained during the inference procedure with the prespecified one. Worth to note that the inference backend still tries to compute real posterior marginal and may fail during this process. Might be useful for debugging purposes. If nothing is passed then the computed posterior marginal is returned.","category":"page"},{"location":"lib/form/","page":"Functional forms","title":"Functional forms","text":"@constraints function block_updates(x_posterior = nothing) \n    # `nothing` returns the computed posterior marginal\n    q(x) :: Marginal(x_posterior)\nend","category":"page"},{"location":"lib/form/","page":"Functional forms","title":"Functional forms","text":"FixedMarginalFormConstraint","category":"page"},{"location":"lib/form/#ReactiveMP.FixedMarginalFormConstraint","page":"Functional forms","title":"ReactiveMP.FixedMarginalFormConstraint","text":"FixedMarginalFormConstraint\n\nOne of the form constraint objects. Provides a constraint on the marginal distribution such that it remains fixed during inference.  Can be viewed as blocking of updates of a specific edge associated with the marginal. If nothing is passed then the computed posterior marginal is returned.\n\nTraits\n\nis_point_mass_form_constraint = false\ndefault_form_check_strategy   = FormConstraintCheckLast()\ndefault_prod_constraint       = ProdAnalytical()\nmake_form_constraint          = Marginal (for use in @constraints macro)\n\nSee also: constrain_form, DistProduct\n\n\n\n\n\n","category":"type"},{"location":"lib/form/#lib-forms-composite-constraint","page":"Functional forms","title":"CompositeFormConstraint","text":"","category":"section"},{"location":"lib/form/","page":"Functional forms","title":"Functional forms","text":"It is possible to create a composite functional form constraint with either + operator or using @constraints macro, e.g:","category":"page"},{"location":"lib/form/","page":"Functional forms","title":"Functional forms","text":"form_constraint = SampleListFormConstraint(1000) + PointMassFormConstraint()","category":"page"},{"location":"lib/form/","page":"Functional forms","title":"Functional forms","text":"@constraints begin \n    q(x) :: SampleList(1000) :: PointMass()\nend","category":"page"},{"location":"lib/form/","page":"Functional forms","title":"Functional forms","text":"CompositeFormConstraint","category":"page"},{"location":"lib/form/#ReactiveMP.CompositeFormConstraint","page":"Functional forms","title":"ReactiveMP.CompositeFormConstraint","text":"CompositeFormConstraint\n\nCreates a composite form constraint that applies form constraints in order. The composed form constraints must be compatible and have the exact same form_check_strategy.  Any functional form constraint that defines is_point_mass_form_constraint() = true may be used only as the last element of the composition.\n\n\n\n\n\n","category":"type"},{"location":"examples/linear_gaussian_state_space_model/#examples-linear-gaussian-state-space-model","page":"Linear Gaussian Dynamical System","title":"Example: Linear Gaussian State Space Model","text":"","category":"section"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"In this example the goal is to estimate hidden states of a Linear Dynamical process where all hidden states are Gaussians. A simple multivariate Linear Gaussian State Space Model can be described with the following equations:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"beginaligned\n p(x_ix_i - 1)  = mathcalN(x_iA * x_i - 1 mathcalP)\n p(y_ix_i)  = mathcalN(y_iB * x_i mathcalQ)\nendaligned","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"where x_i are hidden states, y_i are noisy observations, A, B are state transition and observational matrices, mathcalP and mathcalQ are state transition noise and observation noise covariance matrices. For a more rigorous introduction to Linear Gaussian Dynamical systems we refer to Simo Sarkka, Bayesian Filtering and Smoothing book.","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"To model this process in ReactiveMP, first, we start with importing all needed packages:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"using Rocket, ReactiveMP, GraphPPL, Distributions\nusing BenchmarkTools, Random, LinearAlgebra, Plots","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"Next step, is to generate some synthetic data:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"function generate_data(rng, A, B, Q, P)\n    x_prev = [ 10.0, -10.0 ]\n\n    x = Vector{Vector{Float64}}(undef, n)\n    y = Vector{Vector{Float64}}(undef, n)\n\n    for i in 1:n\n        x[i] = rand(rng, MvNormal(A * x_prev, Q))\n        y[i] = rand(rng, MvNormal(B * x[i], P))\n        x_prev = x[i]\n    end\n    \n    return x, y\nend","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"# Seed for reproducibility\nseed = 1234\n\nrng = MersenneTwister(seed)\n\n# We will model 2-dimensional observations with rotation matrix `A`\n# To avoid clutter we also assume that matrices `A`, `B`, `P` and `Q`\n# are known and fixed for all time-steps\nθ = π / 35\nA = [ cos(θ) -sin(θ); sin(θ) cos(θ) ]\nB = diageye(2)\nQ = diageye(2)\nP = 25.0 .* diageye(2)\n\n# Number of observations\nn = 300\n\nnothing #hide","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"note: Note\nFor large number of observations you will need yo use limit_stack_depth = 100 option during model creation, e.g. model, (x, y) = create_model(model_options(limit_stack_depth = 100), ...)","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"x, y = generate_data(rng, A, B, Q, P)\nnothing #hide","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"Lets plot our synthetic dataset. Lines represent our hidden states we want to estimate using noisy observations, which are represented as dots.","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"slicedim(dim) = (a) -> map(e -> e[dim], a)\n\npx = plot()\n\npx = plot!(px, x |> slicedim(1), label = \"Hidden Signal (dim-1)\", color = :orange)\npx = scatter!(px, y |> slicedim(1), label = false, markersize = 2, color = :orange)\npx = plot!(px, x |> slicedim(2), label = \"Hidden Signal (dim-2)\", color = :green)\npx = scatter!(px, y |> slicedim(2), label = false, markersize = 2, color = :green)\n\nplot(px)","category":"page"},{"location":"examples/linear_gaussian_state_space_model/#Model-specification","page":"Linear Gaussian Dynamical System","title":"Model specification","text":"","category":"section"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"To create a model we use GraphPPL package and @model macro:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"@model function rotate_ssm(n, x0, A, B, Q, P)\n    \n    # We create constvar references for better efficiency\n    cA = constvar(A)\n    cB = constvar(B)\n    cQ = constvar(Q)\n    cP = constvar(P)\n    \n    # `x` is a sequence of hidden states\n    x = randomvar(n)\n    # `y` is a sequence of \"clamped\" observations\n    y = datavar(Vector{Float64}, n)\n    \n    x_prior ~ MvNormalMeanCovariance(mean(x0), cov(x0))\n    x_prev = x_prior\n    \n    for i in 1:n\n        x[i] ~ MvNormalMeanCovariance(cA * x_prev, cQ)\n        y[i] ~ MvNormalMeanCovariance(cB * x[i], cP)\n        x_prev = x[i]\n    end\n    \n    return x, y\nend","category":"page"},{"location":"examples/linear_gaussian_state_space_model/#Inference","page":"Linear Gaussian Dynamical System","title":"Inference","text":"","category":"section"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"Here we create a custom_inference function to infer hidden states of our system:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"function custom_inference(data, x0, A, B, Q, P)\n\n    # We create a model and get references for \n    # hidden states and observations\n    model, (x, y) = rotate_ssm(n, x0, A, B, Q, P);\n\n    xbuffer   = buffer(Marginal, n)\n    bfe       = nothing\n    \n    # We subscribe on posterior marginals of `x`\n    xsubscription = subscribe!(getmarginals(x), xbuffer)\n    # We are also intereset in BetheFreeEnergy functional,\n    # which in this case is equal to minus log evidence\n    fsubcription = subscribe!(score(BetheFreeEnergy(), model), (v) -> bfe = v)\n\n    # `update!` updates our clamped datavars\n    update!(y, data)\n\n    # It is important to always unsubscribe\n    unsubscribe!((xsubscription, fsubcription))\n    \n    return xbuffer, bfe\nend","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"Alternatively you can use ReactiveMP inference API:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"result = inference(\n    model = Model(rotate_ssm, length(y), x0, A, B, Q, P), \n    data  = (y = y,)\n);","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"To run inference we also specify prior for out first time-step:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"x0 = MvNormalMeanCovariance(zeros(2), 100.0 * diageye(2))\nnothing # hide","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"xmarginals, bfe = custom_inference(y, x0, A, B, Q, P)\nnothing #hide","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"px = plot()\n\npx = plot!(px, x |> slicedim(1), label = \"Hidden Signal (dim-1)\", color = :orange)\npx = plot!(px, x |> slicedim(2), label = \"Hidden Signal (dim-2)\", color = :green)\n\npx = plot!(px, mean.(xmarginals) |> slicedim(1), ribbon = var.(xmarginals) |> slicedim(1) .|> sqrt, fillalpha = 0.5, label = \"Estimated Signal (dim-1)\", color = :teal)\npx = plot!(px, mean.(xmarginals) |> slicedim(2), ribbon = var.(xmarginals) |> slicedim(2) .|> sqrt, fillalpha = 0.5, label = \"Estimated Signal (dim-1)\", color = :violet)\n\nplot(px)","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"As we can see from our plot, estimated signal resembles closely to the real hidden states with small variance. We maybe also interested in the value for minus log evidence:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"bfe","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"We may be also interested in performance of our resulting Belief Propagation algorithm:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"@benchmark custom_inference($y, $x0, $A, $B, $Q, $P)","category":"page"},{"location":"lib/node/#lib-node","page":"Overview","title":"Nodes implementation","text":"","category":"section"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"In message passing framework one of the most important concept is factor node.  Factor node represents a local function in a factorised representation of a generative model.","category":"page"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"note: Note\nTo quickly check the list of all available factor nodes that can be used in the model specification language call ?make_node or Base.doc(make_node).","category":"page"},{"location":"lib/node/#lib-node-traits","page":"Overview","title":"Node traits","text":"","category":"section"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"Each factor node has to define as_node_functional_form trait function and to specify ValidNodeFunctionalForm singleton as a return object. By default as_node_functional_form returns UndefinedNodeFunctionalForm. Objects that do not specify this property correctly cannot be used in model specification.","category":"page"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"note: Note\n@node macro does that automatically","category":"page"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"ValidNodeFunctionalForm\nUndefinedNodeFunctionalForm\nas_node_functional_form","category":"page"},{"location":"lib/node/#ReactiveMP.ValidNodeFunctionalForm","page":"Overview","title":"ReactiveMP.ValidNodeFunctionalForm","text":"ValidNodeFunctionalForm\n\nTrait specification for an object that can be used in model specification as a factor node.\n\nSee also: as_node_functional_form, UndefinedNodeFunctionalForm\n\n\n\n\n\n","category":"type"},{"location":"lib/node/#ReactiveMP.UndefinedNodeFunctionalForm","page":"Overview","title":"ReactiveMP.UndefinedNodeFunctionalForm","text":"UndefinedNodeFunctionalForm\n\nTrait specification for an object that can not be used in model specification as a factor node.\n\nSee also: as_node_functional_form, ValidNodeFunctionalForm\n\n\n\n\n\n","category":"type"},{"location":"lib/node/#ReactiveMP.as_node_functional_form","page":"Overview","title":"ReactiveMP.as_node_functional_form","text":"as_node_functional_form(object)\n\nDetermines object node functional form trait specification. Returns either ValidNodeFunctionalForm() or UndefinedNodeFunctionalForm().\n\nSee also: ValidNodeFunctionalForm, UndefinedNodeFunctionalForm\n\n\n\n\n\n","category":"function"},{"location":"lib/node/#lib-node-types","page":"Overview","title":"Node types","text":"","category":"section"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"We distinguish different types of factor nodes to have a better control over Bethe Free Energy computation. Each factor node has either Deterministic or Stochastic functional form type.","category":"page"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"Deterministic\nStochastic\nisdeterministic\nisstochastic\nsdtype","category":"page"},{"location":"lib/node/#ReactiveMP.Deterministic","page":"Overview","title":"ReactiveMP.Deterministic","text":"Deterministic\n\nDeterministic object used to parametrize factor node object with determinstic type of relationship between variables.\n\nSee also: Stochastic, isdeterministic, isstochastic, sdtype\n\n\n\n\n\n","category":"type"},{"location":"lib/node/#ReactiveMP.Stochastic","page":"Overview","title":"ReactiveMP.Stochastic","text":"Stochastic\n\nStochastic object used to parametrize factor node object with stochastic type of relationship between variables.\n\nSee also: Deterministic, isdeterministic, isstochastic, sdtype\n\n\n\n\n\n","category":"type"},{"location":"lib/node/#ReactiveMP.isdeterministic","page":"Overview","title":"ReactiveMP.isdeterministic","text":"isdeterministic(node)\n\nFunction used to check if factor node object is deterministic or not. Returns true or false.\n\nSee also: Deterministic, Stochastic, isstochastic, sdtype\n\n\n\n\n\n","category":"function"},{"location":"lib/node/#ReactiveMP.isstochastic","page":"Overview","title":"ReactiveMP.isstochastic","text":"isstochastic(node)\n\nFunction used to check if factor node object is stochastic or not. Returns true or false.\n\nSee also: Deterministic, Stochastic, isdeterministic, sdtype\n\n\n\n\n\n","category":"function"},{"location":"lib/node/#ReactiveMP.sdtype","page":"Overview","title":"ReactiveMP.sdtype","text":"sdtype(object)\n\nReturns either Deterministic or Stochastic for a given object (if defined).\n\nSee also: Deterministic, Stochastic, isdeterministic, isstochastic\n\n\n\n\n\n","category":"function"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"using ReactiveMP","category":"page"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"For example + node has the Deterministic type:","category":"page"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"plus_node = make_node(+)\n\nprintln(\"Is `+` node deterministic: \", isdeterministic(plus_node))\nprintln(\"Is `+` node stochastic: \", isstochastic(plus_node))\nnothing #hide","category":"page"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"On the other hand Bernoulli node has the Stochastic type:","category":"page"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"bernoulli_node = make_node(Bernoulli)\n\nprintln(\"Is `Bernoulli` node deterministic: \", isdeterministic(bernoulli_node))\nprintln(\"Is `Bernoulli` node stochastic: \", isstochastic(bernoulli_node))","category":"page"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"To get an actual instance of the type object we use sdtype function:","category":"page"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"println(\"sdtype() of `+` node is \", sdtype(plus_node))\nprintln(\"sdtype() of `Bernoulli` node is \", sdtype(bernoulli_node))\nnothing #hide","category":"page"},{"location":"lib/node/#lib-node-factorisation-constraints","page":"Overview","title":"Node factorisation constraints","text":"","category":"section"},{"location":"man/meta-specification/#user-guide-meta-specification","page":"Meta Specification","title":"Meta Specification","text":"","category":"section"},{"location":"man/meta-specification/","page":"Meta Specification","title":"Meta Specification","text":"Some nodes in ReactiveMP.jl accept optional meta structure that may be used to change or customise the inference procedure or the way node computes outbound messages. As an example GCV node accepts the approxximation method that will be used to approximate non-conjugate relationships between variables in this node. GraphPPL.jl exports @meta macro to specify node-specific meta and contextual information. For example:","category":"page"},{"location":"man/meta-specification/#General-syntax","page":"Meta Specification","title":"General syntax","text":"","category":"section"},{"location":"man/meta-specification/","page":"Meta Specification","title":"Meta Specification","text":"@meta macro accepts either regular julia function or a single begin ... end block. For example both are valid:","category":"page"},{"location":"man/meta-specification/","page":"Meta Specification","title":"Meta Specification","text":"\n@meta function create_meta(arg1, arg2)\n    ...\nend\n\nmymeta = @meta begin \n    ...\nend","category":"page"},{"location":"man/meta-specification/","page":"Meta Specification","title":"Meta Specification","text":"In the first case it returns a function that return meta upon calling, e.g. ","category":"page"},{"location":"man/meta-specification/","page":"Meta Specification","title":"Meta Specification","text":"@meta function create_meta(flag)\n    ...\nend\n\nmymeta = create_meta(true)","category":"page"},{"location":"man/meta-specification/","page":"Meta Specification","title":"Meta Specification","text":"and in the second case it returns constraints directly.","category":"page"},{"location":"man/meta-specification/","page":"Meta Specification","title":"Meta Specification","text":"mymeta = @meta begin \n    ...\nend","category":"page"},{"location":"man/meta-specification/#Options-specification","page":"Meta Specification","title":"Options specification","text":"","category":"section"},{"location":"man/meta-specification/","page":"Meta Specification","title":"Meta Specification","text":"@meta macro accepts optional list of options as a first argument and specified as an array of key = value pairs, e.g. ","category":"page"},{"location":"man/meta-specification/","page":"Meta Specification","title":"Meta Specification","text":"mymeta = @meta [ warn = false ] begin \n   ...\nend","category":"page"},{"location":"man/meta-specification/","page":"Meta Specification","title":"Meta Specification","text":"List of available options:","category":"page"},{"location":"man/meta-specification/","page":"Meta Specification","title":"Meta Specification","text":"warn::Bool - enables/disables various warnings with an incompatible model/meta specification","category":"page"},{"location":"man/meta-specification/#Meta-specification","page":"Meta Specification","title":"Meta specification","text":"","category":"section"},{"location":"man/meta-specification/","page":"Meta Specification","title":"Meta Specification","text":"First, lets start with an example:","category":"page"},{"location":"man/meta-specification/","page":"Meta Specification","title":"Meta Specification","text":"meta = @meta begin \n    GCV(x, k, w) -> GCVMetadata(GaussHermiteCubature(20))\nend","category":"page"},{"location":"man/meta-specification/","page":"Meta Specification","title":"Meta Specification","text":"This meta specification indicates, that for every GCV node in the model with x, k and w as connected variables should use the GCVMetadata(GaussHermiteCubature(20)) meta object.","category":"page"},{"location":"man/meta-specification/","page":"Meta Specification","title":"Meta Specification","text":"You can have a list of as many as possible meta specification entries for different nodes:","category":"page"},{"location":"man/meta-specification/","page":"Meta Specification","title":"Meta Specification","text":"meta = @meta begin \n    GCV(x1, k1, w1) -> GCVMetadata(GaussHermiteCubature(20))\n    GCV(x2, k2, w3) -> GCVMetadata(GaussHermiteCubature(30))\n    NormalMeanVariance(y, x) -> MyCustomMetaObject(arg1, arg2)\nend","category":"page"},{"location":"man/meta-specification/","page":"Meta Specification","title":"Meta Specification","text":"To create a model with extra constraints user may pass optional meta positional argument (comes either first, or after constraints if there are any) for the model function:","category":"page"},{"location":"man/meta-specification/","page":"Meta Specification","title":"Meta Specification","text":"@model function my_model(arguments...)\n   ...\nend\n\nconstraints = @constraints begin \n    ...\nend\n\nmeta = @meta begin \n    ...\nend\n\n# both are valid\nmodel, (x, y) = my_model(meta, arguments...)\nmodel, (x, y) = my_model(constraints, meta, arguments...)","category":"page"},{"location":"man/meta-specification/","page":"Meta Specification","title":"Meta Specification","text":"Alternatively, it is possible to use Model function directly or resort to the automatic inference function that accepts meta keyword argument. ","category":"page"},{"location":"man/advanced-tutorial/#user-guide-advanced-tutorial","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"","category":"section"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"This tutorials covers the fundametnal and advanced usage of the ReactiveMP.jl package. This tutorial also exists in the form of a Jupyter notebook in demo/ folder at GitHub repository.","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Reactive programming package for Julia\nusing Rocket \n# Core package for Constrained Bethe Free Energy minimsation with Factor graphs and message passing\nusing ReactiveMP \n# High-level user friendly probabilistic model and constraints specification language package for ReactiveMP\nusing GraphPPL\n# Optionally include the Distributions.jl package and the Random package from Base\nusing Distributions, Random","category":"page"},{"location":"man/advanced-tutorial/#General-model-specification-syntax","page":"Advanced Tutorial","title":"General model specification syntax","text":"","category":"section"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We use the @model macro from the GraphPPL.jl package to create a probabilistic model p(s y) and we also specify extra constraints on the variational family of distributions mathcalQ, used for approximating intractable posterior distributions. Below there is a simple example of the general syntax for model specification. In this tutorial we do not cover all possible ways to create models or advanced features of GraphPPL.jl.  Instead we refer the interested reader to the documentation for a more rigorous explanation and illustrative examples.","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# the `@model` macro accepts a regular Julia function\n@model function test_model1(s_mean, s_precision)\n    \n    # We use the `randomvar` function to create \n    # a random variable in our model\n    s = randomvar()\n    \n    # the `tilde` operator creates a functional dependency\n    # between variables in our model and can be read as \n    # `sampled from` or `is modeled by`\n    s ~ GaussianMeanPrecision(s_mean, s_precision)\n    \n    # We use the `datavar` function to create \n    # observed data variables in our models\n    # We also need to specify the type of our data \n    # In this example it is `Float64`\n    y = datavar(Float64)\n    \n    y ~ GaussianMeanPrecision(s, 1.0)\n    \n    # In general `@model` macro returns a variable of interests\n    # However it is also possible to obtain all variable in the model \n    # with the `ReactiveMP.getvardict(model)` function call\n    return s, y \nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The @model macro creates a function with the same name and with the same set of input arguments as the original function (test_model1(s_mean, s_precision) in this example). However, the return value is modified in such a way to contain a reference to the model object as the first value and to the user specified variables in the form of a tuple as the second value.","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"model, (s, y) = test_model1(0.0, 1.0)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Another way of creating the model is to use the Model function that returns an instance of ModelGenerator:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"modelgenerator = Model(test_model1, 0.0, 1.0)\n\nmodel, (s, y) = ReactiveMP.create_model(modelgenerator)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The benefits of using model generator as a way to create a model is that it allows to change inference constraints and meta specification for nodes. We will talk about factorisation and form constraints and meta specification later on in this demo.","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"GraphPPL.jl returns a factor graph-based representation of a model. We can examine this factor graph structure with the help of some utility functions such as: ","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getnodes(): returns an array of factor nodes in a correposning factor graph\ngetrandom(): returns an array of random variable in the model\ngetdata(): returns an array of data inputs in the model\ngetconstant(): return an array of constant values in the model","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getnodes(model)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getrandom(model) .|> name","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getdata(model) .|> name","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getconstant(model) .|> getconst","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to use control flow statements such as if or for blocks in the model specification function. In general, any valid snippet of Julia code can be used inside the @model block. As an example consider the following (valid!) model:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model2(n)\n    \n    if n <= 1\n        error(\"`n` argument must be greater than one.\")\n    end\n    \n    # `randomvar(n)` creates a dense sequence of \n    # random variables\n    s = randomvar(n)\n    \n    # `datavar(Float64, n)` creates a dense sequence of \n    # observed data variables of type `Float64`\n    y = datavar(Float64, n)\n    \n    s[1] ~ GaussianMeanPrecision(0.0, 0.1)\n    y[1] ~ GaussianMeanPrecision(s[1], 1.0)\n    \n    for i in 2:n\n        s[i] ~ GaussianMeanPrecision(s[i - 1], 1.0)\n        y[i] ~ GaussianMeanPrecision(s[i], 1.0)\n    end\n    \n    return s, y\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"There are some limitations though regarding using if-blocks to create random variables. It is advised to create random variables in advance before if block, e.g instead of ","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"if some_condition\n    x ~ Normal(0.0, 1.0)\nelse\n    x ~ Normal(0.0, 100.0)\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"some needs to write:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"x = randomvar()\n\nif some_condition\n    x ~ Normal(0.0, 1.0)\nelse\n    x ~ Normal(0.0, 100.0)\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"model, (s, y) = test_model2(10)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# An amount of factor nodes in generated Factor Graph\ngetnodes(model) |> length","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# An amount of random variables\ngetrandom(model) |> length","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# An amount of data inputs\ngetdata(model) |> length","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# An amount of constant values\ngetconstant(model) |> length","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to use complex expression inside the functional dependency expressions","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"y ~ NormalMeanPrecision(2.0 * (s + 1.0), 1.0)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The ~ operator automatically creates a random variable if none was created before with the same name and throws an error if this name already exists","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# s = randomvar() here is optional\n# `~` creates random variables automatically\ns ~ NormalMeanPrecision(0.0, 1.0)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"An example model which will throw an error:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function error_model1()\n    s = 1.0\n    s ~ NormalMeanPrecision(0.0, 1.0)\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"LoadError: Invalid name 's' for new random variable. 's' was already initialized with '=' operator before.","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"By default the GraphPPL.jl package creates new references for constants (literals like 0.0 or 1.0) in a model. In some situations this may not be efficient, especially when these constants represent large matrices. GraphPPL.jl will by default create new copies of some constant (e.g. matrix) in a model every time it uses it. However it is possible to use constvar() function to create and reuse similar constants in the model specification syntax as","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Creates constant reference in a model with a prespecified value\nc = constvar(0.0)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"An example:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model5(dim::Int, n::Int, A::Matrix, P::Matrix, Q::Matrix)\n    \n    s = randomvar(n)\n    \n    y = datavar(Vector{Float64}, n)\n    \n    # Here we create constant references\n    # for constant matrices in our model \n    # to make inference more memory efficient\n    cA = constvar(A)\n    cP = constvar(P)\n    cQ = constvar(Q)\n    \n    s[1] ~ MvGaussianMeanCovariance(zeros(dim), cP)\n    y[1] ~ MvGaussianMeanCovariance(s[1], cQ)\n    \n    for i in 2:n\n        s[i] ~ MvGaussianMeanCovariance(cA * s[i - 1], cP)\n        y[i] ~ MvGaussianMeanCovariance(s[i], cQ)\n    end\n    \n    return s, y\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The ~ expression can also return a reference to a newly created node in a corresponding factor graph for convenience in later usage:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model()\n\n    # In this example `ynode` refers to the corresponding \n    # `GaussianMeanVariance` node created in the factor graph\n    ynode, y ~ GaussianMeanVariance(0.0, 1.0)\n    \n    return ynode, y\nend","category":"page"},{"location":"man/advanced-tutorial/#Probabilistic-inference-in-ReactiveMP.jl","page":"Advanced Tutorial","title":"Probabilistic inference in ReactiveMP.jl","text":"","category":"section"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"ReactiveMP.jl uses the Rocket.jl package API for inference routines. Rocket.jl is a reactive programming extension for Julia that is higly inspired by RxJS and similar libraries from the Rx ecosystem. It consists of observables, actors, subscriptions and operators. For more infromation and rigorous examples see Rocket.jl github page.","category":"page"},{"location":"man/advanced-tutorial/#Observables","page":"Advanced Tutorial","title":"Observables","text":"","category":"section"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Observables are lazy push-based collections and they deliver their values over time.","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Timer that emits a new value every second and has an initial one second delay \nobservable = timer(1000, 1000)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"A subscription allows us to subscribe on future values of some observable, and actors specify what to do with these new values:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"actor = (value) -> println(value)\nsubscription1 = subscribe!(observable, actor)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# We always need to unsubscribe from some observables\nunsubscribe!(subscription1)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# We can modify our observables\nmodified = observable |> filter(d -> rem(d, 2) === 1) |> map(Int, d -> d ^ 2)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"subscription2 = subscribe!(modified, (value) -> println(value))","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"unsubscribe!(subscription2)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The ReactiveMP.jl package returns posterior marginal distributions in our specified model in the form of an observable. It is possible to subscribe on its future updates, but for convenience ReactiveMP.jl only caches the last obtained values of all marginals in a model. To get a reference for the posterior marginal of some random variable in a model ReactiveMP.jl exports two functions: ","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getmarginal(x): for a single random variable x\ngetmarginals(xs): for a dense sequence of random variables sx","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Lets see how it works in practice. Here we create a simple coin toss model. We assume that observations are governed by the Bernoulli distribution with unknown bias parameter θ. To have a fully Bayesian treatment of this problem we endow θ with the Beta prior.","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function coin_toss_model(n)\n\n    # `datavar` creates data 'inputs' in our model\n    # We will pass data later on to these inputs\n    # In this example we create a sequence of inputs that accepts Float64\n    y = datavar(Float64, n)\n    \n    # We endow θ parameter of our model with some prior\n    θ ~ Beta(2.0, 7.0)\n    \n    # We assume that the outcome of each coin flip \n    # is modeled by a Bernoulli distribution\n    for i in 1:n\n        y[i] ~ Bernoulli(θ)\n    end\n    \n    # We return references to our data inputs and θ parameter\n    # We will use these references later on during the inference step\n    return y, θ\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"_, (y, θ) = coin_toss_model(500)\nnothing #hide","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# As soon as we have a new value for the marginal posterior over the `θ` variable\n# we simply print the first two statistics of it\nθ_subscription = subscribe!(getmarginal(θ), (marginal) -> println(\"New update: mean(θ) = \", mean(marginal), \", std(θ) = \", std(marginal)));","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Next, lets define our dataset:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"p = 0.75 # Bias of a coin\n\ndataset = float.(rand(Bernoulli(p), 500))\nnothing #hide","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To pass data to our model we use update! function","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"update!(y, dataset)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# It is necessary to always unsubscribe from running observables\nunsubscribe!(θ_subscription)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# The ReactiveMP.jl inference backend is lazy and does not compute posterior marginals if no-one is listening for them\n# At this moment we have already unsubscribed from the new posterior updates so this `update!` does nothing\nupdate!(y, dataset)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Rocket.jl provides some useful built-in actors for obtaining posterior marginals especially with static datasets.","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# the `keep` actor simply keeps all incoming updates in an internal storage, ordered\nθvalues = keep(Marginal)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# `getmarginal` always emits last cached value as its first value\nsubscribe!(getmarginal(θ) |> take(1), θvalues);","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(θvalues)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"subscribe!(getmarginal(θ) |> take(1), θvalues);","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(θvalues)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# the `buffer` actor keeps very last incoming update in an internal storage and can also store \n# an array of updates for a sequence of random variables\nθbuffer = buffer(Marginal, 1)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"subscribe!(getmarginals([ θ ]) |> take(1), θbuffer);","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(θbuffer)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"subscribe!(getmarginals([ θ ]) |> take(1), θbuffer);","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(θbuffer)","category":"page"},{"location":"man/advanced-tutorial/#Reactive-Inference","page":"Advanced Tutorial","title":"Reactive Inference","text":"","category":"section"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"ReactiveMP.jl naturally supports reactive streams of data and it is possible to run reactive inference with some external datasource.","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function online_coin_toss_model()\n    \n    # We create datavars for the prior \n    # over `θ` variable\n    θ_a = datavar(Float64)\n    θ_b = datavar(Float64)\n    \n    θ ~ Beta(θ_a, θ_b)\n    \n    y = datavar(Float64)\n    y ~ Bernoulli(θ)\n\n    return θ_a, θ_b, θ, y\nend\n","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"_, (θ_a, θ_b, θ, y) = online_coin_toss_model()","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# In this example we subscribe on posterior marginal of θ variable and use it as a prior for our next observation\n# We also print into stdout for convenience\nθ_subscription = subscribe!(getmarginal(θ), (m) -> begin \n    m_a, m_b = params(m)\n    update!(θ_a, m_a)\n    update!(θ_b, m_b)\n    println(\"New posterior for θ: mean = \", mean(m), \", std = \", std(m))\nend)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Initial priors\nupdate!(θ_a, 10.0 * rand())\nupdate!(θ_b, 10.0 * rand())","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"data_source = timer(500, 500) |> map(Float64, (_) -> float(rand(Bernoulli(0.75)))) |> tap((v) -> println(\"New observation: \", v))\nnothing #hide","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"data_subscription = subscribe!(data_source, (data) -> update!(y, data))","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# It is important to unsubscribe from running observables to release computer resources\nunsubscribe!(data_subscription)\nunsubscribe!(θ_subscription)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"That was an example of exact Bayesian inference with Sum-Product (or Belief Propagation) algorithm. However, ReactiveMP.jl is not limited to only the sum-product algorithm but it also supports variational message passing with Constrained Bethe Free Energy Minimisation.","category":"page"},{"location":"man/advanced-tutorial/#Variational-inference","page":"Advanced Tutorial","title":"Variational inference","text":"","category":"section"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"On a very high-level, ReactiveMP.jl is aimed to solve the Constrained Bethe Free Energy minimisation problem. For this task we approximate our exact posterior marginal distribution by some family of distributions q in mathcalQ. Often this involves assuming some factorization over q. For this purpose the @model macro supports optional where { ... } clauses for every ~ expression in a model specification.","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model6_with_manual_constraints(n)\n    τ ~ GammaShapeRate(1.0, 1.0) \n    μ ~ NormalMeanVariance(0.0, 100.0)\n    \n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        # Here we assume a mean-field assumption on our \n        # variational family of distributions locally for the current node\n        y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) }\n    end\n    \n    return μ, τ, y\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this example we specified an extra constraints for q_a for Bethe factorisation:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"q(s) = prod_a in mathcalV q_a(s_a) prod_i in mathcalE q_i^-1(s_i)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"There are several options to specify the mean-field factorisation constraint. ","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) } # With names from model specification\ny[i] ~ NormalMeanPrecision(μ, τ) where { q = q(out)q(mean)q(precision) } # With names from node specification\ny[i] ~ NormalMeanPrecision(μ, τ) where { q = MeanField() } # With alias name","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to use local structured factorisation:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i], μ)q(τ) } # With names from model specification\ny[i] ~ NormalMeanPrecision(μ, τ) where { q = q(out, mean)q(precision) } # With names from node specification","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"As an option the @model macro accepts optional arguments for model specification, one of which is default_factorisation that accepts MeanField() as its argument for better convenience","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model [ default_factorisation = MeanField() ] function test_model(...)\n    ...\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"This will autatically impose a mean field factorization constraint over all marginal distributions in our model.","category":"page"},{"location":"man/advanced-tutorial/#GraphPPL.jl-constraints-macro","page":"Advanced Tutorial","title":"GraphPPL.jl constraints macro","text":"","category":"section"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"GraphPPL.jl package exports @constraints macro to simplify factorisation and form constraints specification. Read more about @constraints macro in the corresponding documentation section, here we show a simple example of the same factorisation constraints specification, but with @constraints macro:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"constraints6 = @constraints begin\n     q(μ, τ) = q(μ)q(τ) # Mean-Field over `μ` and `τ`\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Note: where blocks have higher priority over constraints specification","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model6(n)\n    τ ~ GammaShapeRate(1.0, 1.0) \n    μ ~ NormalMeanVariance(0.0, 100.0)\n    \n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        # Here we assume a mean-field assumption on our \n        # variational family of distributions locally for the current node\n        y[i] ~ NormalMeanPrecision(μ, τ)\n    end\n    \n    return μ, τ, y\nend","category":"page"},{"location":"man/advanced-tutorial/#Inference","page":"Advanced Tutorial","title":"Inference","text":"","category":"section"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To run inference in this model we again need to create a synthetic dataset:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"dataset = rand(Normal(-3.0, inv(sqrt(5.0))), 1000)\nnothing #hide","category":"page"},{"location":"man/advanced-tutorial/#inference-function","page":"Advanced Tutorial","title":"inference function","text":"","category":"section"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In order to simplify model and inference testing, ReactiveMP.jl exports pre-written inference function, that is aimed for simple use cases with static datasets:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"This function provides generic (but somewhat limited) way to run inference in ReactiveMP.jl. ","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"result = inference(\n    model         = Model(test_model6, length(dataset)),\n    data          = (y = dataset, ),\n    constraints   = constraints6, \n    initmarginals = (μ = vague(NormalMeanPrecision), τ = vague(GammaShapeRate)),\n    returnvars    = (μ = KeepLast(), τ = KeepLast()),\n    iterations    = 10,\n    free_energy   = true,\n    showprogress  = true\n)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"μ: mean = \", mean(result.posteriors[:μ]), \", std = \", std(result.posteriors[:μ]))\nnothing #hide","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"τ: mean = \", mean(result.posteriors[:τ]), \", std = \", std(result.posteriors[:τ]))\nnothing #hide","category":"page"},{"location":"man/advanced-tutorial/#Manual-inference","page":"Advanced Tutorial","title":"Manual inference","text":"","category":"section"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"For advanced use cases it is advised to write inference functions manually as it provides more flexibility, here is an example of manual inference specification:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"model, (μ, τ, y) = test_model6(constraints6, length(dataset))","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"For variational inference we also usually need to set initial marginals for our inference procedure. For that purpose ReactiveMP.jl export the setmarginal! function:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"setmarginal!(μ, vague(NormalMeanPrecision))\nsetmarginal!(τ, vague(GammaShapeRate))","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"μ_values = keep(Marginal)\nτ_values = keep(Marginal)\n\nμ_subscription = subscribe!(getmarginal(μ), μ_values)\nτ_subscription = subscribe!(getmarginal(τ), τ_values)\n\nfor i in 1:10\n    update!(y, dataset)\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(μ_values)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(τ_values)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"μ: mean = \", mean(last(μ_values)), \", std = \", std(last(μ_values)))\nnothing #hide","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"τ: mean = \", mean(last(τ_values)), \", std = \", std(last(τ_values)))\nnothing #hide","category":"page"},{"location":"man/advanced-tutorial/#Form-constraints","page":"Advanced Tutorial","title":"Form constraints","text":"","category":"section"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In order to support form constraints, the randomvar() function also supports a where { ... } clause with some optional arguments. One of these arguments is form_constraint that allows us to specify a form constraint to the random variables in our model. Another one is prod_constraint that allows to specify an additional constraints during computation of product of two colliding messages. For example we can perform the EM algorithm if we assign a point mass contraint on some variables in our model.","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model7_with_manual_constraints(n)\n    τ ~ GammaShapeRate(1.0, 1.0) \n    \n    # In case of form constraints `randomvar()` call is necessary\n    μ = randomvar() where { marginal_form_constraint = PointMassFormConstraint() }\n    μ ~ NormalMeanVariance(0.0, 100.0)\n    \n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) }\n    end\n    \n    return μ, τ, y\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"As in the previous example we can use @constraints macro to achieve the same goal with a nicer syntax:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"constraints7 = @constraints begin \n    q(μ) :: PointMass\n    \n    q(μ, τ) = q(μ)q(τ) # Mean-Field over `μ` and `τ`\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this example we specified an extra constraints for q_i for Bethe factorisation:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"q(s) = prod_a in mathcalV q_a(s_a) prod_i in mathcalE q_i^-1(s_i)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model7(n)\n    τ ~ GammaShapeRate(1.0, 1.0) \n    \n    # In case of form constraints `randomvar()` call is necessary\n    μ = randomvar()\n    μ ~ NormalMeanVariance(0.0, 100.0)\n    \n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        y[i] ~ NormalMeanPrecision(μ, τ)\n    end\n    \n    return μ, τ, y\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"model, (μ, τ, y) = test_model7(constraints7, length(dataset));","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"setmarginal!(μ, vague(NormalMeanPrecision))\nsetmarginal!(τ, PointMass(1.0))\n\nμ_values = keep(Marginal)\nτ_values = keep(Marginal)\n\nμ_subscription = subscribe!(getmarginal(μ), μ_values)\nτ_subscription = subscribe!(getmarginal(τ), τ_values)\n\nfor i in 1:10\n    update!(y, dataset)\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(μ_values) |> last","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(τ_values) |> last ","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"By default ReactiveMP.jl tries to compute an analytical product of two colliding messages and throws an error if no analytical solution is known. However, it is possible to fall back to a generic product that does not require an analytical solution to be known. In this case the inference backend will simply propagate the product of two message in a form of a tuple. It is not possible to use such a tuple-product during an inference and in this case it is mandatory to use some form constraint to approximate this product.","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"μ = randomvar() where { \n    prod_constraint = ProdGeneric(),\n    form_constraint = SampleListFormConstraint() \n}","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Sometimes it is useful to preserve a specific parametrisation of the resulting product later on in an inference procedure. ReactiveMP.jl exports a special prod_constraint called ProdPreserveType especially for that purpose:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"μ = randomvar() where { prod_constraint = ProdPreserveType(NormalWeightedMeanPrecision) }","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Note: @constraints macro specifies required prod_constraint automatically.","category":"page"},{"location":"man/advanced-tutorial/#Free-Energy","page":"Advanced Tutorial","title":"Free Energy","text":"","category":"section"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"During variational inference ReactiveMP.jl optimises a special functional called the Bethe Free Energy functional. It is possible to obtain its values for all VMP iterations with the score function.","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"model, (μ, τ, y) = test_model6(constraints6, length(dataset));","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"bfe_observable = score(BetheFreeEnergy(), model)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"bfe_subscription = subscribe!(bfe_observable, (fe) -> println(\"Current BFE value: \", fe));","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Reset the model with vague marginals\nsetmarginal!(μ, vague(NormalMeanPrecision))\nsetmarginal!(τ, vague(GammaShapeRate))\n\nfor i in 1:10\n    update!(y, dataset)\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# It always necessary to unsubscribe and release computer resources\nunsubscribe!([ μ_subscription, τ_subscription, bfe_subscription ])","category":"page"},{"location":"man/advanced-tutorial/#Meta-data-specification","page":"Advanced Tutorial","title":"Meta data specification","text":"","category":"section"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"During model specification some functional dependencies may accept an optional meta object in the where { ... } clause. The purpose of the meta object is to adjust, modify or supply some extra information to the inference backend during the computations of the messages. The meta object for example may contain an approximation method that needs to be used during various approximations or it may specify the tradeoff between accuracy and performance:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# In this example the `meta` object for the autoregressive `AR` node specifies the variate type of \n# the autoregressive process and its order. In addition it specifies that the message computation rules should\n# respect accuracy over speed with the `ARsafe()` strategy. In contrast, `ARunsafe()` strategy tries to speedup computations\n# by cost of possible numerical instabilities during an inference procedure\ns[i] ~ AR(s[i - 1], θ, γ) where { q = q(s[i - 1], s[i])q(θ)q(γ), meta = ARMeta(Multivariate, order, ARsafe()) }\n...\ns[i] ~ AR(s[i - 1], θ, γ) where { q = q(s[i - 1], s[i])q(θ)q(γ), meta = ARMeta(Univariate, order, ARunsafe()) }","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Another example with GaussianControlledVariance, or simply GCV [see Hierarchical Gaussian Filter], node:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# In this example we specify structured factorisation and flag meta with `GaussHermiteCubature` \n# method with `21` sigma points for approximation of non-lineariety between hierarchy layers\nxt ~ GCV(xt_min, zt, real_k, real_w) where { q = q(xt, xt_min)q(zt)q(κ)q(ω), meta = GCVMetadata(GaussHermiteCubature(21)) }","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The Meta object is useful to pass any extra information to a node that is not a random variable or constant model variable. It may include extra approximation methods, differentiation methods, optional non-linear functions, extra inference parameters etc.","category":"page"},{"location":"man/advanced-tutorial/#GraphPPL.jl-@meta-macro","page":"Advanced Tutorial","title":"GraphPPL.jl @meta macro","text":"","category":"section"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Users can use @meta macro from the GraphPPL.jl package to achieve the same goal. Read more about @meta macro in the corresponding documentation section. Here is a simple example of the same meta specification:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@meta begin \n     AR(s, θ, γ) -> ARMeta(Multivariate, 5, ARsafe())\nend","category":"page"},{"location":"man/advanced-tutorial/#Creating-custom-nodes-and-message-computation-rules","page":"Advanced Tutorial","title":"Creating custom nodes and message computation rules","text":"","category":"section"},{"location":"man/advanced-tutorial/#Custom-nodes","page":"Advanced Tutorial","title":"Custom nodes","text":"","category":"section"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To create a custom functional form and to make it available during model specification ReactiveMP.jl exports the @node macro:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# `@node` macro accepts a name of the functional form, its type, either `Stochastic` or `Deterministic` and an array of interfaces:\n@node NormalMeanVariance Stochastic [ out, μ, v ]\n\n# Interfaces may have aliases for their names that might be convenient for factorisation constraints specification\n@node NormalMeanVariance Stochastic [ out, (μ, aliases = [ mean ]), (v, aliases = [ var ]) ]\n\n# `NormalMeanVariance` structure declaration must exist, otherwise `@node` macro will throw an error\nstruct NormalMeanVariance end \n\n@node NormalMeanVariance Stochastic [ out, μ, v ]\n\n# It is also possible to use function objects as a node functional form\nfunction dot end\n\n# Syntax for functions is a bit differet, as it is necesssary to use `typeof(...)` function for them \n# out = dot(x, a)\n@node typeof(dot) Deterministic [ out, x, a ]","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Note: Deterministic nodes do not support factorisation constraints with the where { q = ... } clause.","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"After that it is possible to use the newly created node during model specification:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model()\n    ...\n    y ~ dot(x, a)\n    ...\nend","category":"page"},{"location":"man/advanced-tutorial/#Custom-messages-computation-rules","page":"Advanced Tutorial","title":"Custom messages computation rules","text":"","category":"section"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"ReactiveMP.jl exports the @rule macro to create custom message computation rules. For example let us create a simple + node to be available for usage in the model specification usage. We refer to A Factor Graph Approach to Signal Modelling , System Identification and Filtering [ Sascha Korl, 2005, page 32 ] for a rigorous explanation of the + node in factor graphs. According to Korl, assuming that inputs are Gaussian Sum-Product message computation rule for + node is the following:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"mu_z = mu_x + mu_y \nV_z = V_x + V_y","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To specify this in ReactiveMP.jl we use the @node and @rule macros:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@node typeof(+) Deterministic  [ z, x, y ]\n\n@rule typeof(+)(:z, Marginalisation) (m_x::UnivariateNormalDistributionsFamily, m_y::UnivariateNormalDistributionsFamily) = begin\n    x_mean, x_var = mean_var(m_x)\n    y_mean, y_var = mean_var(m_y)\n    return NormalMeanVariance(x_mean + y_mean, x_var + y_var)\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this example, for the @rule macro, we specify a type of our functional form: typeof(+). Next, we specify an edge we are going to compute an outbound message for. Marginalisation indicates that the corresponding message respects the marginalisation constraint for posterior over corresponding edge:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"q(z) = int q(z x y) mathrmdxmathrmdy","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"If we look on difference between sum-product rules and variational rules with mean-field assumption we notice that they require different local information to compute an outgoing message:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"mu(z) = int f(x y z)mu(x)mu(y)mathrmdxmathrmdy","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"nu(z) = exp int log f(x y z)q(x)q(y)mathrmdxmathrmdy ","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The @rule macro supports both cases with special prefixes during rule specification:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"m_ prefix corresponds to the incoming message on a specific edge\nq_ prefix corresponds to the posterior marginal of a specific edge","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Example of a Sum-Product rule with m_ messages used:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule NormalMeanPrecision(:μ, Marginalisation) (m_out::UnivariateNormalDistributionsFamily, m_τ::PointMass) = begin \n    m_out_mean, m_out_cov = mean_cov(m_out)\n    return NormalMeanPrecision(m_out_mean, inv(m_out_cov + inv(mean(m_τ))))\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Example of a Variational rule with Mean-Field assumption with q_ posteriors used:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule NormalMeanPrecision(:μ, Marginalisation) (q_out::Any, q_τ::Any) = begin \n    return NormalMeanPrecision(mean(q_out), mean(q_τ))\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"ReactiveMP.jl also supports structured rules. It is possible to obtain joint marginal over a set of edges:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule NormalMeanPrecision(:τ, Marginalisation) (q_out_μ::Any, ) = begin\n    m, V = mean_cov(q_out_μ)\n    θ = 2 / (V[1,1] - V[1,2] - V[2,1] + V[2,2] + abs2(m[1] - m[2]))\n    α = convert(typeof(θ), 1.5)\n    return Gamma(α, θ)\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"NOTE: In the @rule specification the messages or marginals arguments must be in order with interfaces specification from @node macro:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Inference backend expects arguments in `@rule` macro to be in the same order\n@node NormalMeanPrecision Stochastic [ out, μ, τ ]","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Any rule always has access to the meta information with hidden the meta::Any variable:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any) = begin \n    ...\n    println(meta)\n    ...\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to dispatch on a specific type of a meta object:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::LaplaceApproximation) = begin \n    ...\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"or","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::GaussHermiteCubature) = begin \n    ...\nend","category":"page"},{"location":"man/advanced-tutorial/#Customizing-messages-computational-pipeline","page":"Advanced Tutorial","title":"Customizing messages computational pipeline","text":"","category":"section"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In certain situations it might be convenient to customize the default message computational pipeline. GrahpPPL.jl supports the pipeline keyword in the where { ... } clause to add some extra steps after a message has been computed. A use case might be an extra approximation method to preserve conjugacy in the model, debugging or simple printing.","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Logs all outbound messages\ny[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = LoggerPipelineStage() }\n# Initialise messages to be vague\ny[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = InitVaguePipelineStage() }\n# In principle, it is possible to approximate outbound messages with Laplace Approximation\ny[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = LaplaceApproximation() }","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Let us return to the coin toss model, but this time we want to print flowing messages:","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model [ default_factorisation = FullFactorisation() ] function coin_toss_model_log(n)\n\n    y = datavar(Float64, n)\n\n    θ ~ Beta(2.0, 7.0) where { pipeline = LoggerPipelineStage(\"θ\") }\n\n    for i in 1:n\n        y[i] ~ Bernoulli(θ)  where { pipeline = LoggerPipelineStage(\"y[$i]\") }\n    end\n    \n    return y, θ\nend","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"_, (y, θ) = coin_toss_model_log(5);","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"θ_subscription = subscribe!(getmarginal(θ), (value) -> println(\"New posterior marginal for θ: \", value));","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"coinflips = float.(rand(Bernoulli(0.5), 5));","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"update!(y, coinflips)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"unsubscribe!(θ_subscription)","category":"page"},{"location":"man/advanced-tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Inference is lazy and does not send messages if no one is listening for them\nupdate!(y, coinflips)","category":"page"},{"location":"man/getting-started/#user-guide-getting-started","page":"Getting Started","title":"Getting started","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"ReactiveMP.jl is a Julia package for Bayesian Inference on Factor Graphs by Message Passing. It supports both exact and variational inference algorithms.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"ReactiveMP package is a successor of the ForneyLab package. It follows the same ideas and concepts for message-passing based inference, but uses new reactive and efficient message passing implementation under the hood. The API between two packages is different due to a better flexibility, performance and new reactive approach for solving inference problems.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"This page provides the necessary information you need to get started with ReactiveMP. We will show the general approach to solving inference problems with ReactiveMP by means of a running example: inferring the bias of a coin.","category":"page"},{"location":"man/getting-started/#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Install ReactiveMP through the Julia package manager:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"] add ReactiveMP","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"note: Note\nFor best user experience you also need to install GraphPPL, Rocket and Distributions packages.","category":"page"},{"location":"man/getting-started/#Example:-Inferring-the-bias-of-a-coin","page":"Getting Started","title":"Example: Inferring the bias of a coin","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"The ReactiveMP approach to solving inference problems consists of three phases:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Model specification: ReactiveMP uses GraphPPL package for model specification part. It offers a domain-specific language to specify your probabilistic model.\nInference specification: ReactiveMP inference API has been designed to be as flexible as possible and it is compatible both with asynchronous infinite data streams and with static datasets. For most of the use cases it consists of the same simple building blocks. In this example we will show one of the many possible ways to infer your quantities of interest.\nInference execution: Given model specification and inference procedure it is pretty straightforward to use reactive API from Rocket to pass data to the inference backend and to run actual inference.","category":"page"},{"location":"man/getting-started/#Coin-flip-simulation","page":"Getting Started","title":"Coin flip simulation","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Let's start by creating some dataset. One approach could be flipping a coin N times and recording each outcome. For simplicity in this example we will use static pre-generated dataset. Each sample can be thought of as the outcome of single flip which is either heads or tails (1 or 0). We will assume that our virtual coin is biased, and lands heads up on 75% of the trials (on average).","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"First lets setup our environment by importing all needed packages:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Next, lets define our dataset:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"rng = MersenneTwister(42)\nn = 10\np = 0.75\ndistribution = Bernoulli(p)\n\ndataset = float.(rand(rng, Bernoulli(p), n))","category":"page"},{"location":"man/getting-started/#getting-started-model-specification","page":"Getting Started","title":"Model specification","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"In a Bayesian setting, the next step is to specify our probabilistic model. This amounts to specifying the joint probability of the random variables of the system.","category":"page"},{"location":"man/getting-started/#Likelihood","page":"Getting Started","title":"Likelihood","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"We will assume that the outcome of each coin flip is governed by the Bernoulli distribution, i.e.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"y_i sim mathrmBernoulli(theta)","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"where y_i = 1 represents \"heads\", y_i = 0 represents \"tails\". The underlying probability of the coin landing heads up for a single coin flip is theta in 01.","category":"page"},{"location":"man/getting-started/#Prior","page":"Getting Started","title":"Prior","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"We will choose the conjugate prior of the Bernoulli likelihood function defined above, namely the beta distribution, i.e.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"theta sim Beta(a b)","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"where a and b are the hyperparameters that encode our prior beliefs about the possible values of theta. We will assign values to the hyperparameters in a later step.   ","category":"page"},{"location":"man/getting-started/#Joint-probability","page":"Getting Started","title":"Joint probability","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"The joint probability is given by the multiplication of the likelihood and the prior, i.e.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"P(y_1N θ) = P(θ) prod_i=1^N P(y_i  θ)","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Now let's see how to specify this model using GraphPPL's package syntax.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"\n# GraphPPL.jl export `@model` macro for model specification\n# It accepts a regular Julia function and builds an FFG under the hood\n@model function coin_model(n)\n\n    # `datavar` creates data 'inputs' in our model\n    # We will pass data later on to these inputs\n    # In this example we create a sequence of inputs that accepts Float64\n    y = datavar(Float64, n)\n    \n    # We endow θ parameter of our model with some prior\n    θ ~ Beta(2.0, 7.0)\n    \n    # We assume that outcome of each coin flip is governed by the Bernoulli distribution\n    for i in 1:n\n        y[i] ~ Bernoulli(θ)\n    end\n    \n    # We return references to our data inputs and θ parameter\n    # We will use these references later on during inference step\n    return y, θ\nend\n","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"As you can see, GraphPPL offers a model specification syntax that resembles closely to the mathematical equations defined above. We use datavar function to create \"clamped\" variables that take specific values at a later date. θ ~ Beta(2.0, 7.0) expression creates random variable θ and assigns it as an output of Beta node in the corresponding FFG. ","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"note: Note\nTo quickly check the list of all available factor nodes that can be used in the model specification language call ?make_node or Base.doc(make_node).","category":"page"},{"location":"man/getting-started/#getting-started-inference-specification","page":"Getting Started","title":"Inference specification","text":"","category":"section"},{"location":"man/getting-started/#Automatic-inference-specification","page":"Getting Started","title":"Automatic inference specification","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Once we have defined our model, the next step is to use ReactiveMP API to infer quantities of interests. To do this we can use a generic inference function from ReactiveMP.jl that supports static datasets.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"result = inference(\n    model = Model(coin_model, length(dataset)),\n    data  = (y = dataset, )\n)","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"θestimated = last(result.posteriors[:θ])","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"println(\"mean: \", mean(θestimated))\nprintln(\"std:  \", std(θestimated))\nnothing #hide","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Read more information about the inference function in the Inference execution and Advanced Tutorial sections.","category":"page"},{"location":"man/getting-started/#Manual-inference-specification","page":"Getting Started","title":"Manual inference specification","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"There is a way to manually specify an inference procedure for advanced use-cases. ReactiveMP API is flexible in terms of inference specification and is compatible both with real-time inference processing and with static datasets. In most of the cases for static datasets, as in our example, it consists of same basic building blocks:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Return variables of interests from model specification\nSubscribe on variables of interests posterior marginal updates\nPass data to the model\nUnsubscribe ","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Here is an example of inference procedure:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"function custom_inference(data)\n    n = length(data)\n\n    # `coin_model` function from `@model` macro returns a reference to the model object and \n    # the same output as in `return` statement in the original function specification\n    model, (y, θ) = coin_model(n)\n    \n    # Reference for future posterior marginal \n    mθ = nothing\n\n    # `getmarginal` function returns an observable of future posterior marginal updates\n    # We use `Rocket.jl` API to subscribe on this observable\n    # As soon as posterior marginal update is available we just save it in `mθ`\n    subscription = subscribe!(getmarginal(θ), (m) -> mθ = m)\n    \n    # `update!` function passes data to our data inputs\n    update!(y, data)\n    \n    # It is always a good practice to unsubscribe and to \n    # free computer resources held by the subscription\n    unsubscribe!(subscription)\n    \n    # Here we return our resulting posterior marginal\n    return mθ\nend","category":"page"},{"location":"man/getting-started/#getting-started-inference-execution","page":"Getting Started","title":"Inference execution","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Here after everything is ready we just call our inference function to get a posterior marginal distribution over θ parameter in the model.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"θestimated = custom_inference(dataset)","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"println(\"mean: \", mean(θestimated))\nprintln(\"std:  \", std(θestimated))\nnothing #hide","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"using Plots, LaTeXStrings; theme(:default)\n\nrθ = range(0, 1, length = 1000)\n\np1 = plot(rθ, (x) -> pdf(Beta(2.0, 7.0), x), title=\"Prior\", fillalpha=0.3, fillrange = 0, label=L\"P\\:(\\theta)\", c=1,)\np2 = plot(rθ, (x) -> pdf(θestimated, x), title=\"Posterior\", fillalpha=0.3, fillrange = 0, label=L\"P\\:(\\theta|y)\", c=3)\n\nplot(p1, p2, layout = @layout([ a; b ]))","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"In our dataset we used 10 coin flips to estimate the bias of a coin. It resulted in a vague posterior distribution, however ReactiveMP scales very well for large models and factor graphs. We may use more coin flips in our dataset for better posterior distribution estimates:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"dataset_100   = float.(rand(rng, Bernoulli(p), 100))\ndataset_1000  = float.(rand(rng, Bernoulli(p), 1000))\ndataset_10000 = float.(rand(rng, Bernoulli(p), 10000))\nnothing # hide","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"θestimated_100   = custom_inference(dataset_100)\nθestimated_1000  = custom_inference(dataset_1000)\nθestimated_10000 = custom_inference(dataset_10000)\nnothing #hide","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"p3 = plot(title = \"Posterior\", legend = :topleft)\n\np3 = plot!(p3, rθ, (x) -> pdf(θestimated_100, x), fillalpha = 0.3, fillrange = 0, label = L\"P\\:(\\theta\\:|y_{1:100})\", c = 4)\np3 = plot!(p3, rθ, (x) -> pdf(θestimated_1000, x), fillalpha = 0.3, fillrange = 0, label = L\"P\\:(\\theta\\:|y_{1:1000})\", c = 5)\np3 = plot!(p3, rθ, (x) -> pdf(θestimated_10000, x), fillalpha = 0.3, fillrange = 0, label = L\"P\\:(\\theta\\:|y_{1:10000})\", c = 6)\n\nplot(p1, p3, layout = @layout([ a; b ]))","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"With larger dataset our posterior marginal estimate becomes more and more accurate and represents real value of the bias of a coin.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"println(\"mean: \", mean(θestimated_10000))\nprintln(\"std:  \", std(θestimated_10000))\nnothing #hide","category":"page"},{"location":"man/getting-started/#Where-to-go-next?","page":"Getting Started","title":"Where to go next?","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"There are a set of demos available in ReactiveMP repository that demonstrate the more advanced features of the package and also Examples section in the documentation. Alternatively, you can head to the Model specification which provides more detailed information of how to use ReactiveMP and GraphPPL to specify probabilistic models. Inference execution section provides a documentation about ReactiveMP API for running reactive Bayesian inference.","category":"page"},{"location":"examples/overview/#examples-overview","page":"Overview","title":"Examples overview","text":"","category":"section"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"This section contains a set of examples for Bayesian Inference with ReactiveMP package in various probabilistic models.","category":"page"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"note: Note\nMore examples can be found in demo/ folder at GitHub repository.","category":"page"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"Linear regression: An example of linear regression Bayesian inference.\nGaussian Linear Dynamical System: An example of inference procedure for Gaussian Linear Dynamical System with multivariate noisy observations using Belief Propagation (Sum Product) algorithm. Reference: Simo Sarkka, Bayesian Filtering and Smoothing.\nHidden Markov Model: An example of structured variational Bayesian inference in Hidden Markov Model with unknown transition and observational matrices.\nHierarchical Gaussian Filter: An example of online inference procedure for Hierarchical Gaussian Filter with univariate noisy observations using Variational Message Passing algorithm. Reference: Ismail Senoz, Online Message Passing-based Inference in the Hierarchical Gaussian Filter.\nAutoregressive Model: An example of variational Bayesian Inference on full graph for Autoregressive model. Reference: Albert Podusenko, Message Passing-Based Inference for Time-Varying Autoregressive Models.\nNormalising Flows: An example of variational Bayesian Inference with Normalizing Flows. Reference: Bard van Erp, Hybrid Inference with Invertible Neural Networks in Factor Graphs (submitted).\nUnivariate Gaussian Mixture: This example implements variational Bayesian inference in a univariate Gaussian mixture model with mean-field assumption.\nMultivariate Gaussian Mixture: This example implements variational Bayesian inference in a multivariate Gaussian mixture model with mean-field assumption.\nGamma Mixture: This example implements one of the experiments outlined in https://biaslab.github.io/publication/mp-based-inference-in-gmm/ .\nCustom nonlinear node: This example shows how to build an arbitrary factor node for non-linear function using sample list approximation.\nMissing data: This examples show how to extend basic functionality of ReactiveMP to support missing data points.","category":"page"},{"location":"examples/missing_data/#examples-missing-data","page":"Missing data","title":"Example: Missing data","text":"","category":"section"},{"location":"examples/missing_data/","page":"Missing data","title":"Missing data","text":"Let us assume that the following model generates the data","category":"page"},{"location":"examples/missing_data/","page":"Missing data","title":"Missing data","text":"beginalign*\n    x_t = x_t-1 + c \n    y_t sim mathcalNleft(x_t p right) \nendalign*","category":"page"},{"location":"examples/missing_data/","page":"Missing data","title":"Missing data","text":"with prior x_0 sim mathcalN(m_x_0 v_x_0). Suppose that our measurement device fails to acquire data from time to time.  In this case, instead of scalar observation haty_t in mathrmR we sometimes will catch missing observations.","category":"page"},{"location":"examples/missing_data/","page":"Missing data","title":"Missing data","text":"using Rocket, ReactiveMP, GraphPPL, BenchmarkTools, Distributions","category":"page"},{"location":"examples/missing_data/#Model-specification","page":"Missing data","title":"Model specification","text":"","category":"section"},{"location":"examples/missing_data/","page":"Missing data","title":"Missing data","text":"We don't know our data yet so we will attempt to fit it with a simple Gaussian random walk with unknown noise","category":"page"},{"location":"examples/missing_data/","page":"Missing data","title":"Missing data","text":"@model function smoothing(n, x0)\n    \n    P ~ Gamma(0.001, 0.001)\n    x_prior ~ NormalMeanVariance(mean(x0), cov(x0)) \n\n    x = randomvar(n)\n    y = datavar(Float64, n) where { allow_missing = true }\n    c = constvar(1.0)\n\n    x_prev = x_prior\n\n    for i in 1:n\n        x[i] ~ NormalMeanPrecision(x_prev, 1.0)\n        y[i] ~ NormalMeanPrecision(x[i], P)\n        \n        x_prev = x[i]\n    end\n\n    return x, y\nend","category":"page"},{"location":"examples/missing_data/","page":"Missing data","title":"Missing data","text":"To support missing values we extend a list of possible rules in ReactiveMP:","category":"page"},{"location":"examples/missing_data/","page":"Missing data","title":"Missing data","text":"@rule NormalMeanPrecision(:μ, Marginalisation) (q_out::Any, q_τ::Missing) = missing\n@rule NormalMeanPrecision(:μ, Marginalisation) (q_out::Missing, q_τ::Any) = missing\n\n@rule NormalMeanPrecision(:τ, Marginalisation) (q_out::Any, q_μ::Missing) = missing\n@rule NormalMeanPrecision(:τ, Marginalisation) (q_out::Missing, q_μ::Any) = missing\n\n@rule typeof(+)(:in1, Marginalisation) (m_out::Missing, m_in2::Any) = missing\n@rule typeof(+)(:in1, Marginalisation) (m_out::Any, m_in2::Missing) = missing","category":"page"},{"location":"examples/missing_data/#Dataset","page":"Missing data","title":"Dataset","text":"","category":"section"},{"location":"examples/missing_data/","page":"Missing data","title":"Missing data","text":"For our dataset we create a simple sin signal with missing region in the middle:","category":"page"},{"location":"examples/missing_data/","page":"Missing data","title":"Missing data","text":"P = 1.0\nn = 250\n\nreal_signal     = map(e -> sin(0.05 * e), collect(1:n))\nnoisy_data      = real_signal + rand(Normal(0.0, sqrt(P)), n);\nmissing_indices = 100:125 # clamp.(map(d -> rem(abs(d), n), rand(Int, Int(floor(n / 10)))), 1, n)\nmissing_data    = similar(noisy_data, Union{Float64, Missing}, )\n\ncopyto!(missing_data, noisy_data)\n\nfor index in missing_indices\n    missing_data[index] = missing\nend","category":"page"},{"location":"examples/missing_data/#Inference","page":"Missing data","title":"Inference","text":"","category":"section"},{"location":"examples/missing_data/","page":"Missing data","title":"Missing data","text":"constraints = @constraints begin\n    q(x, P) = q(x)q(P)\nend\n\nx0_prior = NormalMeanVariance(0.0, 1000.0)\n\nresult = inference(\n    model = Model(smoothing, n, x0_prior), \n    data  = (y = missing_data,), \n    constraints = constraints,\n    initmarginals = (P = Gamma(0.001, 0.001), ),\n    returnvars = (x = KeepLast(),),\n    iterations = 20\n)","category":"page"},{"location":"examples/missing_data/#Results","page":"Missing data","title":"Results","text":"","category":"section"},{"location":"examples/missing_data/","page":"Missing data","title":"Missing data","text":"using Plots\n\nplot(real_signal, label = \"Noisy signal\", legend = :bottomright)\nscatter!(missing_indices, real_signal[missing_indices], ms = 2, opacity = 0.75, label = \"Missing region\")\nplot!(mean.(result.posteriors[:x]), ribbon = var.(result.posteriors[:x]), label = \"Estimated hidden state\")","category":"page"},{"location":"man/model-specification/#user-guide-model-specification","page":"Model Specification","title":"Model Specification","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"The GraphPPL.jl package exports the @model macro for model specification. This @model macro accepts two arguments: model options and the model specification itself in a form of regular Julia function. For example: ","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"@model [ option1 = ..., option2 = ... ] function model_name(model_arguments...; model_keyword_arguments...)\n    # model specification here\n    return ...\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Model options, model_arguments and model_keyword_arguments are optional and may be omitted:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"@model function model_name()\n    # model specification here\n    return ...\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"The @model macro returns a regular Julia function (in this example model_name()) which can be executed as usual. It returns a reference to a model object itself and a tuple of a user specified return variables, e.g:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"@model function my_model(model_arguments...)\n    # model specification here\n    # ...\n    return x, y\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"model, (x, y) = my_model(model_arguments...)","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"It is not necessary to return anything from the model, in that case GraphPPL.jl will automatically inject return nothing to the end of the model function.","category":"page"},{"location":"man/model-specification/#A-full-example-before-diving-in","page":"Model Specification","title":"A full example before diving in","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Before presenting the details of the model specification syntax, an example of a probabilistic model is given. Here is an example of a simple state space model with latent random variables x and noisy observations y:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"@model [ options... ] function state_space_model(n_observations, noise_variance)\n\n    c = constvar(1.0)\n    x = randomvar(n_observations)\n    y = datavar(Float64, n_observations)\n\n    x[1] ~ NormalMeanVariance(0.0, 100.0)\n\n    for i in 2:n_observations\n       x[i] ~ x[i - 1] + c\n       y[i] ~ NormalMeanVariance(x[i], noise_var)\n    end\n\n    return x, y\nend","category":"page"},{"location":"man/model-specification/#Graph-variables-creation","page":"Model Specification","title":"Graph variables creation","text":"","category":"section"},{"location":"man/model-specification/#user-guide-model-specification-constant-variables","page":"Model Specification","title":"Constants","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Even though any runtime constant passed to a model as a model argument will be automatically converted to a fixed constant, sometimes it might be useful to create constants by hand (e.g. to avoid copying large matrices across the model and to avoid extensive memory allocations).","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"You can create a constant within a model specification macro with constvar() function. For example:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"@model function model_name(...)\n    ...\n    c = constvar(1.0)\n\n    for i in 2:n\n        x[i] ~ x[i - 1] + c # Reuse the same reference to a constant 1.0\n    end\n    ...\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"note: Note\nconstvar() function is supposed to be used only within the @model macro.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Additionally you can specify an extra ::ConstVariable type for some of the model arguments. In this case macro automatically converts them to a single constant using constvar() function. E.g.:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"@model function model_name(nsamples::Int, c::ConstVariable)\n    ...\n    # no need to call for a constvar() here\n    for i in 2:n\n        x[i] ~ x[i - 1] + c # Reuse the same reference to a constant `c`\n    end\n    ...\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"note: Note\n::ConstVariable annotation does not play role in Julia's multiple dispatch. GraphPPL.jl removes this annotation and replaces it with ::Any.","category":"page"},{"location":"man/model-specification/#user-guide-model-specification-data-variables","page":"Model Specification","title":"Data variables","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"It is important to have a mechanism to pass data values to the model. You can create data inputs with datavar() function. As a first argument it accepts a type specification and optional dimensionality (as additional arguments or as a tuple). User can treat datavar()s in the model as both clamped values for priors and observations.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Examples: ","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"@model function model_name(...)\n    ...\n    y = datavar(Float64) # Creates a single data input with `y` as identificator\n    y = datavar(Float64, n) # Returns a vector of  `y_i` data input objects with length `n`\n    y = datavar(Float64, n, m) # Returns a matrix of `y_i_j` data input objects with size `(n, m)`\n    y = datavar(Float64, (n, m)) # It is also possible to use a tuple for dimensionality\n    ...\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"note: Note\ndatavar() function is supposed to be used only within the @model macro.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"datavar() call within @model macro supports where { options... } block for extra options specification, e.g:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"@model function model_name(...)\n    ...\n    y = datavar(Float64, n) where { allow_missing = true }\n    ...\nend","category":"page"},{"location":"man/model-specification/#Data-variables-available-options","page":"Model Specification","title":"Data variables available options","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"allow_missing = true/false: Specifies if it is possible to pass missing object as an observation. Note however that by default ReactiveMP.jl does not expose any message computation rules that involve missings.","category":"page"},{"location":"man/model-specification/#user-guide-model-specification-random-variables","page":"Model Specification","title":"Random variables","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"There are several ways to create random variables. The first one is an explicit call to randomvar() function. By default it doesn't accept any argument, creates a single random variable in the model and returns it. It is also possible to pass dimensionality arguments to randomvar() function in the same way as for the datavar() function.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Examples: ","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"@model function model_name(...)\n    ...\n    x = randomvar() # Returns a single random variable which can be used later in the model\n    x = randomvar(n) # Returns an vector of random variables with length `n`\n    x = randomvar(n, m) # Returns a matrix of random variables with size `(n, m)`\n    x = randomvar((n, m)) # It is also possible to use a tuple for dimensionality\n    ...\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"note: Note\nrandomvar() function is supposed to be used only within the @model macro.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"randomvar() call within @model macro supports where { options... } block for extra options specification, e.g:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"@model function model_name(...)\n    ...\n    y = randomvar() where { prod_constraint = ProdGeneric() }\n    ...\nend","category":"page"},{"location":"man/model-specification/#Random-variables-available-options","page":"Model Specification","title":"Random variables available options","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"prod_constraint\nprod_strategy\nmarginal_form_constraint\nmarginal_form_check_strategy\nmessages_form_constraint\nmessages_form_check_strategy\npipeline","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"The second way to create a random variable is to create a node with the ~ operator. If the random variable has not yet been created before this call, it will be created automatically during the creation of the node. Read more about the ~ operator below.","category":"page"},{"location":"man/model-specification/#Node-creation","page":"Model Specification","title":"Node creation","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Factor nodes are used to define a relationship between random variables and/or constants and data inputs. A factor node defines a probability distribution over selected random variables. ","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"note: Note\nTo quickly check the list of all available factor nodes that can be used in the model specification language call ?make_node or Base.doc(make_node).","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"We model a random variable by a probability distribution using the ~ operator. For example, to create a random variable y which is modeled by a Normal distribution, where its mean and variance are controlled by the random variables m and v respectively, we define","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"@model function model_name(...)\n    ...\n    m = randomvar()\n    v = randomvar()\n    y ~ NormalMeanVariance(m, v) # Creates a `y` random variable automatically\n    ...\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Another example, but using a determnistic relation between random variables:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"@model function model_name(...)\n    ...\n    a = randomvar()\n    b = randomvar()\n    c ~ a + b\n    ...\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"note: Note\nThe GraphPPL.jl package uses the ~ operator for modelling both stochastic and deterministic relationships between random variables.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"The @model macro automatically resolves any inner function calls into anonymous extra nodes in case this inner function call is a non-linear transformations. It will also create needed anonymous random variables. But it is important to note that the inference backend will try to optimize inner non-linear deterministic function calls in the case where all arguments are constants or data inputs. For example:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"noise ~ NormalMeanVariance(mean, inv(precision)) # Will create a non-linear `inv` node in case if `precision` is a random variable. Won't create an additional non-linear node in case if `precision` is a constant or data input.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"It is possible to use any functional expression within the ~ operator arguments list. The only one exception is the ref expression (e.g x[i]). All reference expressions within the ~ operator arguments list are left untouched during model parsing. This means that the model parser will not create unnecessary nodes when only simple indexing is involved.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"note: Note\nIt is forbidden to use random variable within square brackets in the model specification.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"y ~ NormalMeanVariance(x[i - 1], variance) # While in principle `i - 1` is an inner function call (`-(i, 1)`) model parser will leave it untouched and won't create any anonymous nodes for `ref` expressions.\n\ny ~ NormalMeanVariance(A * x[i - 1], variance) # This example will create a `*` anonymous node (in case if x[i - 1] is a random variable) and leave `x[i - 1]` untouched.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"It is also possible to return a node reference from the ~ operator. Use the following syntax:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"node, y ~ NormalMeanVariance(mean, var)","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Having a node reference can be useful in case the user wants to return it from a model and to use it later on to specify initial joint marginal distributions.","category":"page"},{"location":"man/model-specification/#Node-creation-options","page":"Model Specification","title":"Node creation options","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"To pass optional arguments to the node creation constructor the user can use the where { options...  } options specification syntax.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Example:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"y ~ NormalMeanVariance(y_mean, y_var) where { q = q(y_mean)q(y_var)q(y) } # mean-field factorisation over q","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"A list of the available options specific to ReactiveMP.jl is presented below.","category":"page"},{"location":"man/model-specification/#Factorisation-constraint-option","page":"Model Specification","title":"Factorisation constraint option","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"See also Constraints Specification section.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Users can specify a factorisation constraint over the approximate posterior q for variational inference. The general syntax for factorisation constraints over q is the following:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"variable ~ Node(node_arguments...) where { q = RecognitionFactorisationConstraint }","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"where RecognitionFactorisationConstraint can be the following","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"MeanField()","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Automatically specifies a mean-field factorisation","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Example:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"y ~ NormalMeanVariance(y_mean, y_var) where { q = MeanField() }","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"FullFactorisation()","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Automatically specifies a full factorisation","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Example:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"y ~ NormalMeanVariance(y_mean, y_var) where { q = FullFactorisation() }","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"q(μ)q(v)q(out) or q(μ) * q(v) * q(out)","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"A user can specify any factorisation he wants as the multiplication of q(interface_names...) factors. As interface names the user can use the interface names of an actual node (read node's documentation), its aliases (if available) or actual random variable names present in the ~ operator expression.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Examples: ","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"# Using interface names of a `NormalMeanVariance` node for factorisation constraint. \n# Call `?NormalMeanVariance` to know more about interface names for some node\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(μ)q(v)q(out) }\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(μ, v)q(out) }\n\n# Using interface names aliases of a `NormalMeanVariance` node for factorisation constraint. \n# Call `?NormalMeanVariance` to know more about interface names aliases for some node\n# In general aliases correspond to the function names for distribution parameters\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(mean)q(var)q(out) }\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(mean, var)q(out) }\n\n# Using random variables names from `~` operator expression\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(y_mean)q(y_var)q(y) }\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(y_mean, y_var)q(y) }\n\n# All methods can be combined easily\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(μ)q(y_var)q(out) }\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(y_mean, v)q(y) }","category":"page"},{"location":"man/model-specification/#Metadata-option","page":"Model Specification","title":"Metadata option","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Is is possible to pass any extra metadata to a factor node with the meta option. Metadata can be later accessed in message computation rules. See also Meta specification section.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"z ~ f(x, y) where { meta = ... }","category":"page"},{"location":"man/model-specification/#Pipeline-option","page":"Model Specification","title":"Pipeline option","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"WIP","category":"page"},{"location":"lib/math/#lib-math","page":"Math utils","title":"Math utilities","text":"","category":"section"},{"location":"lib/math/","page":"Math utils","title":"Math utils","text":"ReactiveMP package exports tiny and huge objects to represent tiny and huge numbers. These objects aren't really numbers and behave differently depending on the context. They do support any operation that is defined for Real numbers. For more info see Julia's documentation about promotion.","category":"page"},{"location":"lib/math/","page":"Math utils","title":"Math utils","text":"tiny\nhuge\nTinyNumber\nHugeNumber","category":"page"},{"location":"lib/math/#ReactiveMP.tiny","page":"Math utils","title":"ReactiveMP.tiny","text":"tiny\n\nAn instance of a TinyNumber. Behaviour and actual value of the tiny number depends on the context.\n\nExample\n\njulia> tiny\ntiny\n\njulia> 1 + tiny\n1.000000000001\n\njulia> tiny + 1\n1.000000000001\n\njulia> 1f0 + tiny\n1.000001f0\n\njulia> big\"1.0\" + tiny\n1.000000000000000000000001\n\njulia> big\"1\" + tiny\n1.000000000000000000000001\n\nSee also: huge, TinyNumber, HugeNumber\n\n\n\n\n\n","category":"constant"},{"location":"lib/math/#ReactiveMP.huge","page":"Math utils","title":"ReactiveMP.huge","text":"huge\n\nAn instance of a HugeNumber. Behaviour and actual value of the huge number depends on the context.\n\nExample\n\njulia> huge\nhuge\n\njulia> 1 + huge\n1.000000000001e12\n\njulia> huge + 1\n1.000000000001e12\n\njulia> 1f0 + huge\n1.000001f6\n\njulia> big\"1.0\" + huge\n1.000000000000000000000001e+24\n\njulia> big\"1\" + huge\n1.000000000000000000000001e+24\n\nSee also: tiny, TinyNumber, HugeNumber\n\n\n\n\n\n","category":"constant"},{"location":"lib/math/#ReactiveMP.TinyNumber","page":"Math utils","title":"ReactiveMP.TinyNumber","text":"TinyNumber <: Real\n\nTinyNumber represents (wow!) tiny number that can be used in a various computations without unnecessary type promotions.\n\nSee also: HugeNumber\n\n\n\n\n\n","category":"type"},{"location":"lib/math/#ReactiveMP.HugeNumber","page":"Math utils","title":"ReactiveMP.HugeNumber","text":"HugeNumber <: Real\n\nHugeNumber represents (wow!) huge number that can be used in a various computations without unnecessary type promotions.\n\nSee also: TinyNumber\n\n\n\n\n\n","category":"type"},{"location":"examples/hierarchical_gaussian_filter/#examples-hgf","page":"Hierarchical Gaussian Filter","title":"Example: Hierarchical Gaussian Filter","text":"","category":"section"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"In this demo the goal is to perform approximate variational Bayesian Inference for Univariate Hierarchical Gaussian Filter (HGF).","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Simple HGF model can be defined as:","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"beginaligned\n    x^(j)_k  sim  mathcalN(x^(j)_k - 1 f_k(x^(j - 1)_k)) \n    y_k  sim  mathcalN(x^(j)_k tau_k)\nendaligned","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"where j is an index of layer in hierarchy, k is a time step and f_k is a variance activation function. ReactiveMP.jl export Gaussian Controlled Variance (GCV) node with f_k = exp(kappa x + omega) variance activation function. By default uses Gauss-Hermite cubature with a prespecified number of approximation points in the cubature. We can change the number of points in Gauss-Hermite cubature with the help of metadata structures in ReactiveMP.jl. ","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":" beginaligned\n    z_k  sim  mathcalN(z_k - 1 mathcaltau_z) \n    x_k  sim  mathcalN(x_k - 1 exp(kappa z_k + omega)) \n    y_k  sim  mathcalN(x_k mathcaltau_y)\nendaligned","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"In this experiment we will create a single time step of the graph and perform variational message passing filtering alrogithm to estimate hidden states of the system. For a more rigorous introduction to Hierarchical Gaussian Filter we refer to Ismail Senoz, Online Message Passing-based Inference in the Hierarchical Gaussian Filter paper.","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"For simplicity we will consider tau_z, tau_y, kappa and omega known and fixed.","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"To model this process in ReactiveMP, first, we start with importing all needed packages:","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"using Rocket, ReactiveMP, GraphPPL, Distributions\nusing BenchmarkTools, Random, Plots","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Next step, is to generate some synthetic data:","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"function generate_data(rng, k, w, zv, yv)\n    z_prev = 0.0\n    x_prev = 0.0\n\n    z = Vector{Float64}(undef, n)\n    v = Vector{Float64}(undef, n)\n    x = Vector{Float64}(undef, n)\n    y = Vector{Float64}(undef, n)\n\n    for i in 1:n\n        z[i] = rand(rng, Normal(z_prev, sqrt(zv)))\n        v[i] = exp(k * z[i] + w)\n        x[i] = rand(rng, Normal(x_prev, sqrt(v[i])))\n        y[i] = rand(rng, Normal(x[i], sqrt(yv)))\n\n        z_prev = z[i]\n        x_prev = x[i]\n    end \n    \n    return z, x, y\nend","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"# Seed for reproducibility\nseed = 123\n\nrng = MersenneTwister(seed)\n\n# Parameters of HGF process\nreal_k = 1.0\nreal_w = 0.0\nz_variance = abs2(0.5)\ny_variance = abs2(1.0)\n\n# Number of observations\nn = 300\n\nz, x, y = generate_data(rng, real_k, real_w, z_variance, y_variance)\nnothing #hide","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Lets plot our synthetic dataset. Lines represent our hidden states we want to estimate using noisy observations.","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"let \n    pz = plot(title = \"Hidden States Z\")\n    px = plot(title = \"Hidden States X\")\n    \n    plot!(pz, 1:n, z, label = \"z_i\", color = :orange)\n    plot!(px, 1:n, x, label = \"x_i\", color = :green)\n    \n    plot(pz, px, layout = @layout([ a; b ]))\nend","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"To create a model we use GraphPPL package and @model macro:","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"# We create a single-time step of corresponding state-space process to\n# perform online learning (filtering)\n@model function hgf(real_k, real_w, z_variance, y_variance)\n    \n    # Priors from previous time step for `z`\n    zt_min_mean = datavar(Float64)\n    zt_min_var  = datavar(Float64)\n    \n    # Priors from previous time step for `x`\n    xt_min_mean = datavar(Float64)\n    xt_min_var  = datavar(Float64)\n\n    zt_min ~ NormalMeanVariance(zt_min_mean, zt_min_var)\n    xt_min ~ NormalMeanVariance(xt_min_mean, xt_min_var)\n\n    # Higher layer is modelled as a random walk \n    zt ~ NormalMeanVariance(zt_min, z_variance)\n    \n    # Lower layer is modelled with `GCV` node\n    gcv_node, xt ~ GCV(xt_min, zt, real_k, real_w)\n    \n    # Noisy observations \n    y = datavar(Float64)\n    y ~ NormalMeanVariance(xt, y_variance)\n    \n    return zt, xt, y, gcv_node, xt_min_mean, xt_min_var, zt_min_mean, zt_min_var\nend","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"function reactive_online_inference(data, vmp_iters, real_k, real_w, z_variance, y_variance)\n    n = length(data)\n    \n    # We don't want to save all marginals from all VMP iterations\n    # but only last one after all VMP iterations per time step\n    # Rocket.jl exports PendingScheduler() object that postpones \n    # any update unless manual `resolve!()` has been called\n    ms_scheduler = PendingScheduler()\n    \n    mz = keep(Marginal)\n    mx = keep(Marginal)\n    fe = ScoreActor(Float64)\n\n    hgf_constraints = @constraints begin\n        q(zt, zt_min, z_variance) = q(zt, zt_min)q(z_variance)\n        q(xt, zt, xt_min) = q(xt, xt_min)q(zt)\n    end\n\n    model, (zt, xt, y, gcv_node, xt_min_mean, xt_min_var, zt_min_mean, zt_min_var) = hgf(hgf_constraints, real_k, real_w, z_variance, y_variance)\n\n    # Initial priors\n    current_zt_mean, current_zt_var = 0.0, 10.0\n    current_xt_mean, current_xt_var = 0.0, 10.0\n    \n    s_mz = subscribe!(getmarginal(zt) |> schedule_on(ms_scheduler), mz)\n    s_mx = subscribe!(getmarginal(xt) |> schedule_on(ms_scheduler), mx)\n    s_fe = subscribe!(score(Float64, BetheFreeEnergy(), model), fe)\n\n    # Initial marginals to start VMP procedire\n    setmarginal!(gcv_node, :y_x, MvNormalMeanCovariance([ 0.0, 0.0 ], [ 5.0, 5.0 ]))\n    setmarginal!(gcv_node, :z, NormalMeanVariance(0.0, 5.0))\n    \n    # For each observations we perofrm `vmp_iters` VMP iterations\n    for i in 1:n\n        \n        for _ in 1:vmp_iters\n            update!(y, data[i])\n            update!(zt_min_mean, current_zt_mean)\n            update!(zt_min_var, current_zt_var)\n            update!(xt_min_mean, current_xt_mean)\n            update!(xt_min_var, current_xt_var)\n        end\n        \n        # After all VMP iterations we release! `PendingScheduler`\n        # as well as release! `ScoreActor` to indicate new time step\n        release!(ms_scheduler)\n        release!(fe)\n        \n        current_zt_mean, current_zt_var = mean_var(last(mz))::Tuple{Float64, Float64}\n        current_xt_mean, current_xt_var = mean_var(last(mx))::Tuple{Float64, Float64}\n    end\n    \n    # It is important to unsubscribe at the end of the inference procedure\n    unsubscribe!((s_mz, s_mx, s_fe))\n    \n    return map(getvalues, (mz, mx, fe))\nend","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"To run inference we also specify number of VMP iterations we want to perform as well as an approximation method for GCV node:","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"vmp_iters = 10\nnothing #hide","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"mz, mx, fe = reactive_online_inference(y, vmp_iters, real_k, real_w, z_variance, y_variance)\nnothing #hide","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"let \n    pz = plot(title = \"Hidden States Z\")\n    px = plot(title = \"Hidden States X\")\n    \n    plot!(pz, 1:n, z, label = \"z_i\", color = :orange)\n    plot!(pz, 1:n, mean.(mz), ribbon = std.(mz), label = \"estimated z_i\", color = :teal)\n    \n    plot!(px, 1:n, x, label = \"x_i\", color = :green)\n    plot!(px, 1:n, mean.(mx), ribbon = std.(mx), label = \"estimated x_i\", color = :violet)\n    \n    plot(pz, px, layout = @layout([ a; b ]))\nend","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"As we can see from our plot, estimated signal resembles closely to the real hidden states with small variance. We maybe also interested in the values for Bethe Free Energy functional:","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"plot(fe, label = \"Bethe Free Energy\")","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"As we can see BetheFreeEnergy converges nicely to a stable point.","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"We may be also interested in performance of our resulting Variational Message Passing algorithm:","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"@benchmark reactive_online_inference($y, $vmp_iters, $real_k, $real_w, $z_variance, $y_variance)","category":"page"},{"location":"man/constraints-specification/#user-guide-constraints-specification","page":"Constraints Specification","title":"Constraints Specification","text":"","category":"section"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"GraphPPL.jl exports @constraints macro for the extra constraints specification that can be used during the inference step in ReactiveMP.jl package.","category":"page"},{"location":"man/constraints-specification/#General-syntax","page":"Constraints Specification","title":"General syntax","text":"","category":"section"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"@constraints macro accepts either regular julia function or a single begin ... end block. For example both are valid:","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"\n# `functional` style\n@constraints function create_my_constraints(arg1, arg2)\n    ...\nend\n\n# `block` style\nmyconstraints = @constraints begin \n    ...\nend\n","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"In the first case it returns a function that return constraints upon calling, e.g. ","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"@constraints function make_constraints(mean_field)\n    q(x) :: PointMass\n\n    if mean_field\n        q(x, y) = q(x)q(y)\n    end\nend\n\nmyconstraints = make_constraints(true)","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"and in the second case it evaluates automatically and returns constraints object directly.","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"myconstraints = @constraints begin \n    q(x) :: PointMass\n    q(x, y) = q(x)q(y)\nend","category":"page"},{"location":"man/constraints-specification/#Options-specification","page":"Constraints Specification","title":"Options specification","text":"","category":"section"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"@constraints macro accepts optional list of options as a first argument and specified as an array of key = value pairs, e.g. ","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"myconstraints = @constraints [ warn = false ] begin \n   ...\nend","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"List of available options:","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"warn::Bool - enables/disables various warnings with an incompatible model/constraints specification","category":"page"},{"location":"man/constraints-specification/#Marginal-and-messages-form-constraints","page":"Constraints Specification","title":"Marginal and messages form constraints","text":"","category":"section"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"To specify marginal or messages form constraints @constraints macro uses :: operator (in somewhat similar way as Julia uses it for multiple dispatch type specification)","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"The following constraint:","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"@constraints begin \n    q(x) :: PointMass\nend","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"indicates that the resulting marginal of the variable (or array of variables) named x must be approximated with a PointMass object. Message passing based algorithms compute posterior marginals as a normalized product of two colliding messages on corresponding edges of a factor graph. In a few words q(x)::PointMass reads as:","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"mathrmapproximate q(x) = fracoverrightarrowmu(x)overleftarrowmu(x)int overrightarrowmu(x)overleftarrowmu(x) mathrmdxmathrmasPointMass","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"Sometimes it might be usefull to set a functional form constraint on messages too. For example if it is essential to keep a specific Gaussian parametrisation or if some messages are intractable and need approximation. To set messages form constraint @constraints macro uses μ(...) instead of q(...):","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"@constraints begin \n    q(x) :: PointMass\n    μ(x) :: SampleList \n    # it is possible to assign different form constraints on the same variable \n    # both for the marginal and for the messages \nend","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"@constraints macro understands \"stacked\" form constraints. For example the following form constraint","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"@constraints begin \n    q(x) :: SampleList(1000) :: PointMass\nend","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"indicates that the q(x) first must be approximated with a SampleList and in addition the result of this approximation should be approximated as a PointMass. ","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"note: Note\nNot all combinations of \"stacked\" form constraints are compatible between each other.","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"You can find more information about built-in functional form constraint in the Built-in Functional Forms section. In addition, Custom Functional Form Specification explains the functional form interfaces and shows how to build a custom functional form constraint that is compatible with ReactiveMP.jl inference backend.","category":"page"},{"location":"man/constraints-specification/#Factorisation-constraints-on-posterior-distribution-q()","page":"Constraints Specification","title":"Factorisation constraints on posterior distribution q()","text":"","category":"section"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"@model macro specifies generative model p(s, y) where s is a set of random variables and y is a set of observations. In a nutshell the goal of probabilistic programming is to find p(s|y). ReactiveMP approximates p(s|y) with a proxy distribution q(x) using KL divergency and Bethe Free Energy optimisation procedure. By default there are no extra factorisation constraints on q(s) and the optimal solution is q(s) = p(s|y). However, inference may be not tractable for every model without extra factorisation constraints. To circumvent this, GraphPPL.jl and ReactiveMP.jl accepts optional factorisation constraints specification syntax:","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"For example:","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"@constraints begin \n    q(x, y) = q(x)q(y)\nend","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"specifies a so-called mean-field assumption on variables x and y in the model. Futhermore, if x is an array of variables in our model we may induce extra mean-field assumption on x in the following way.","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"@constraints begin \n    q(x) = q(x[begin])..q(x[end])\n    q(x, y) = q(x)q(y)\nend","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"These constraints specifies a mean-field assumption between variables x and y (either single variable or collection of variables) and additionally specifies mean-field assumption on variables x_i.","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"note: Note\n@constraints macro does not support matrix-based collections of variables. E.g. it is not possible to write q(x[begin, begin])..q(x[end, end])","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"It is possible to write more complex factorisation constraints, for example:","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"@constraints begin \n    q(x, y) = q(x[begin], y[begin])..q(x[end], y[end])\nend","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"Specifies a mean-field assumption between collection of variables named x and y only for variables with different indices. Another example is","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"@constraints function make_constraints(k)\n    q(x) = q(x[begin:k])q(x[k+1:end])\nend","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"In this example we specify a mean-field assumption between a set of variables x[begin:k] and x[k+1:end]. ","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"To create a model with extra constraints user may pass optional constraints positional argument for the model function:","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"@model function my_model(arguments...)\n   ...\nend\n\nconstraints = @constraints begin \n    ...\nend\n\nmodel, (x, y) = my_model(constraints, arguments...)","category":"page"},{"location":"man/constraints-specification/","page":"Constraints Specification","title":"Constraints Specification","text":"Alternatively, it is possible to use Model function directly or resort to the automatic inference function that accepts constraints keyword argument. ","category":"page"},{"location":"examples/flow_tutorial/#examples-flow","page":"Normalizing Flows Tutorial","title":"Example: Normalizing Flow tutorial","text":"","category":"section"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"Table of contents","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"Introduction\nModel specification\nModel compilation\nProbabilistic inference\nParameter estimation","category":"page"},{"location":"examples/flow_tutorial/#examples-flow-introduction","page":"Normalizing Flows Tutorial","title":"Introduction","text":"","category":"section"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"Normalizing flows are parameterized mappings of random variables, which map simple base distributions to more complex distributions. These mappings are constrained to be invertible and differentiable and can be composed of multiple simpler mappings for improved expressivity.","category":"page"},{"location":"examples/flow_tutorial/#Load-required-packages","page":"Normalizing Flows Tutorial","title":"Load required packages","text":"","category":"section"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"Before we can start, we need to import some packages:","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"using ReactiveMP\nusing Rocket\nusing GraphPPL\nusing Random \nusing StableRNGs\n\nusing LinearAlgebra     # only used for some matrix specifics\nusing PyPlot            # only used for visualisation\nusing Distributions     # only used for sampling from multivariate distributions\nusing Optim             # only used for parameter optimisation","category":"page"},{"location":"examples/flow_tutorial/#examples-flow-model-specification","page":"Normalizing Flows Tutorial","title":"Model specification","text":"","category":"section"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"Specifying a flow model is easy. The general recipe looks like follows: model = FlowModel(input_dim, (layer1(options), layer2(options), ...)). Here the first argument corresponds to the input dimension of the model and the second argument is a tuple of layers. An example flow model can be defined as","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"model = FlowModel(2,\n    (\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n)\nnothing #hide","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"Alternatively, the input_dim can also be passed as an InputLayer layer as ","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"model = FlowModel(\n    (\n        InputLayer(2),\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n)\nnothing #hide","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"In the above AdditiveCouplingLayer layers the input bfx = x_1 x_2 ldots x_N is partitioned into chunks of unit length. These partitions are additively coupled to an output bfy = y_1 y_2 ldots y_N as ","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"beginalign*\n    y_1 = x_1 \n    y_2 = x_2 + f_1(x_1) \n    vdots \n    y_N = x_N + f_N-1(x_N-1)\nendalign*","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"math","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"Importantly, this structure can easily be converted as ","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"beginalign*\n    x_1 = y_1 \n    x_2 = y_2 - f_1(x_1) \n    vdots \n    x_N = y_N - f_N-1(x_N-1)\nendalign*","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"f_n","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"is an arbitrarily complex function, here chosen to be a PlanarFlow, but this can be interchanged for any function or neural network. The permute keyword argument (which defaults to true) specifies whether the output of this layer should be randomly permuted or shuffled. This makes sure that the first element is also transformed in consecutive layers.","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"A permutation layer can also be added by itself as a PermutationLayer layer with a custom permutation matrix if desired.","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"model = FlowModel(\n    (\n        InputLayer(2),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false),\n        PermutationLayer(PermutationMatrix(2)),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n)\nnothing #hide","category":"page"},{"location":"examples/flow_tutorial/#examples-flow-model-compilation","page":"Normalizing Flows Tutorial","title":"Model compilation","text":"","category":"section"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"In the current models, the layers are setup to work with the passed input dimension. This means that the function f_n is repeated input_dim-1 times for each of the partitions. Furthermore the permutation layers are set up with proper permutation matrices. If we print the model we get","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"model","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"The text below describes the terms above. Please note the distinction in typing and elements, i.e. FlowModel{types}(elements):","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"FlowModel - specifies that we are dealing with a flow model.\n3 - Number of layers.\nTuple{AdditiveCouplingLayerEmpty{...},PermutationLayer{Int64},AdditiveCouplingLayerEmpty{...}} - tuple of layer types.\nTuple{ReactiveMP.PlanarFlowEmpty{1},ReactiveMP.PlanarFlowEmpty{1}} - tuple of functions f_n.\nPermutationLayer{Int64}(2, [0 1; 1 0]) - permutation layer with input dimension 2 and permutation matrix [0 1; 1 0].","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"From inspection we can see that the AdditiveCouplingLayerEmpty and PlanarFlowEmpty objects are different than before. They are initialized for the correct dimension, but they do not have any parameters registered to them. This is by design to allow for separating the model specification from potential optimization procedures. Before we perform inference in this model, the parameters should be initialized. We can randomly initialize the parameters as","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"compiled_model = compile(model)","category":"page"},{"location":"examples/flow_tutorial/#examples-flow-probabilistic-inference","page":"Normalizing Flows Tutorial","title":"Probabilistic inference","text":"","category":"section"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"We can perform inference in our compiled model through standard usage of ReactiveMP. Let's first generate some random 2D data which has been sampled from a standard normal distribution and is consecutively passed through a normalizing flow. Using the forward(model, data) function we can propagate data in the forward direction through the flow.","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"function generate_data(nr_samples::Int64, model::CompiledFlowModel; seed = 123)\n\n    rng = StableRNG(seed)\n    \n    # specify latent sampling distribution\n    dist = MvNormal([1.5, 0.5], I)\n\n    # sample from the latent distribution\n    x = rand(rng, dist, nr_samples)\n\n    # transform data\n    y = zeros(Float64, size(x))\n    for k = 1:nr_samples\n        y[:,k] .= ReactiveMP.forward(model, x[:,k])\n    end\n\n    # return data\n    return y, x\n\nend;","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"# generate data\ny, x = generate_data(1000, compiled_model)\n\n# plot generated data\n_, ax = plt.subplots(ncols=2, figsize=(15,5))\nax[1].scatter(x[1,:], x[2,:], alpha=0.3)\nax[2].scatter(y[1,:], y[2,:], alpha=0.3)\nax[1].set_title(\"Original data\")\nax[2].set_title(\"Transformed data\")\nax[1].grid(), ax[2].grid()\nplt.gcf()","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"The probabilistic model for doing inference can be described as ","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"@model function normalizing_flow(nr_samples::Int64)\n    \n    # initialize variables\n    z_μ   = randomvar()\n    z_Λ   = randomvar()\n    x     = randomvar(nr_samples)\n    y_lat = randomvar(nr_samples)\n    y     = datavar(Vector{Float64}, nr_samples)\n\n    # specify prior\n    z_μ ~ MvNormalMeanCovariance(zeros(2), huge*diagm(ones(2)))\n    z_Λ ~ Wishart(2.0, tiny*diagm(ones(2)))\n\n    # specify observations\n    for k = 1:nr_samples\n\n        # specify latent state\n        x[k] ~ MvNormalMeanPrecision(z_μ, z_Λ)\n\n        # specify transformed latent value\n        y_lat[k] ~ Flow(x[k])\n\n        # specify observations\n        y[k] ~ MvNormalMeanCovariance(y_lat[k], tiny*diagm(ones(2)))\n\n    end\n\n    # return variables\n    return z_μ, z_Λ, x, y_lat, y\n\nend;","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"Here the flow model is passed inside a meta data object of the flow node. Inference then resorts to","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"observations = [y[:,k] for k=1:size(y,2)]\n\nfmodel         = Model(normalizing_flow, length(observations))\ndata          = (y = observations, )\ninitmarginals = (z_μ = MvNormalMeanCovariance(zeros(2), huge*diagm(ones(2))), z_Λ = Wishart(2.0, tiny*diagm(ones(2))))\nreturnvars    = (z_μ = KeepLast(), z_Λ = KeepLast(), x = KeepLast(), y_lat = KeepLast())\n\nconstraints = @constraints begin\n    q(z_μ, x, z_Λ) = q(z_μ)q(z_Λ)q(x)\nend\n\n@meta function fmeta(model)\n    compiled_model = compile(model, randn(StableRNG(321), nr_params(model)))\n    Flow(y_lat, x) -> FlowMeta(compiled_model) # defaults to FlowMeta(compiled_model; approximation=Linearization()). \n                                               # other approximation methods can be e.g. FlowMeta(compiled_model; approximation=Unscented(input_dim))\nend\n\n# First execution is slow due to Julia's initial compilation \nresult = inference(\n    model = fmodel, \n    data  = data,\n    constraints   = constraints,\n    meta          = fmeta(model),\n    initmarginals = initmarginals,\n    returnvars    = returnvars,\n    free_energy   = true,\n    iterations    = 10, \n    showprogress  = false\n)","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"The following line of code then executes the inference algorithm.","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"fe_flow = result.free_energy\nzμ_flow = result.posteriors[:z_μ]\nzΛ_flow = result.posteriors[:z_Λ]\nx_flow  = result.posteriors[:x]\ny_flow  = result.posteriors[:y_lat]\nnothing #hide","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"As we can see, the variational free energy decreases inside of our model.","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"plt.figure()\nplt.plot(1:10, fe_flow/size(y,2))\nplt.grid()\nplt.xlim(1,10)\nplt.xlabel(\"iteration\")\nplt.ylabel(\"normalized variational free energy [nats/sample]\")\nplt.gcf()","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"If we plot a random noisy observation and its approximated transformed uncertainty we obtain:","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"# pick a random observation\nid = rand(1:size(y,2))\nrand_observation = MvNormal(y[:,id], 5e-1*diagm(ones(2)))\nwarped_observation = MvNormal(ReactiveMP.backward(compiled_model, y[:,id]), ReactiveMP.inv_jacobian(compiled_model, y[:,id])*5e-1*diagm(ones(2))*ReactiveMP.inv_jacobian(compiled_model, y[:,id])');\n\n# plot inferred means and transformed point\nfig, ax = plt.subplots(ncols = 2, figsize=(15,5))\nax[1].scatter(x[1,:], x[2,:], alpha=0.1, label=\"generated data\")\nax[1].contour(repeat(-5:0.1:5, 1, 101), repeat(-5:0.1:5, 1, 101)', map( (x) -> pdf(MvNormal([1.5, 0.5], I), [x...]), collect(Iterators.product(-5:0.1:5, -5:0.1:5))), label=\"true distribution\")\nax[1].scatter(mean(zμ_flow)[1], mean(zμ_flow)[2], color=\"red\", marker=\"x\", label=\"inferred mean\")\nax[1].contour(repeat(-10:0.01:10, 1, 2001), repeat(-10:0.01:10, 1, 2001)', map( (x) -> pdf(warped_observation, [x...]), collect(Iterators.product(-10:0.01:10, -10:0.01:10))), colors=\"red\", levels=1)\nax[1].scatter(mean(warped_observation)..., color=\"red\", s=10, label=\"transformed noisy observation\")\nax[2].scatter(y[1,:], y[2,:], alpha=0.1, label=\"generated data\")\nax[2].scatter(ReactiveMP.forward(compiled_model, mean(zμ_flow))..., color=\"red\", marker=\"x\", label=\"inferred mean\")\nax[2].contour(repeat(-10:0.1:10, 1, 201), repeat(-10:0.1:10, 1, 201)', map( (x) -> pdf(MvNormal([1.5, 0.5], I), ReactiveMP.backward(compiled_model, [x...])), collect(Iterators.product(-10:0.1:10, -10:0.1:10))))\nax[2].contour(repeat(-10:0.1:10, 1, 201), repeat(-10:0.1:10, 1, 201)', map( (x) -> pdf(rand_observation, [x...]), collect(Iterators.product(-10:0.1:10, -10:0.1:10))), colors=\"red\", levels=1, label=\"random noisy observation\")\nax[2].scatter(mean(rand_observation)..., color=\"red\", s=10, label=\"random noisy observation\")\nax[1].grid(), ax[2].grid()\nax[1].set_xlim(-4,4), ax[1].set_ylim(-4,4), ax[2].set_xlim(-10,10), ax[2].set_ylim(-10,10)\nax[1].legend(), ax[2].legend()\nfig.suptitle(\"Generated data\")\nax[1].set_title(\"Latent distribution\"), ax[2].set_title(\"Observed distribution\")\nplt.gcf()","category":"page"},{"location":"examples/flow_tutorial/#examples-flow-parameter-estimation","page":"Normalizing Flows Tutorial","title":"Parameter estimation","text":"","category":"section"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"The flow model is often used to learn unknown probabilistic mappings. Here we will demonstrate it as follows for a binary classification task with the following data:","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"function generate_data(nr_samples::Int64; seed = 123)\n    \n    rng = StableRNG(seed)\n\n    # sample weights\n    w = rand(rng, nr_samples, 2)\n\n    # sample appraisal\n    y = zeros(Float64, nr_samples)\n    for k = 1:nr_samples\n        y[k] = 1.0*(w[k,1] > 0.5)*(w[k,2] < 0.5)\n    end\n\n    # return data\n    return y, w\n\nend;","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"data_y, data_x = generate_data(50);\nplt.figure()\nplt.scatter(data_x[:,1], data_x[:,2], c=data_y)\nplt.grid()\nplt.xlabel(\"w1\")\nplt.ylabel(\"w2\")\nplt.gcf()","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"We will then specify a possible flow model as","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"# specify flow model\nmodel = FlowModel(2,\n    (\n        AdditiveCouplingLayer(PlanarFlow()), # defaults to AdditiveCouplingLayer(PlanarFlow(); permute=true)\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"The corresponding probabilistic model for the binary classification task can be created as","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"@model [ default_factorisation = FullFactorisation() ] function flow_classifier(nr_samples::Int64)\n    \n    # initialize variables\n    x_lat  = randomvar(nr_samples)\n    y_lat1 = randomvar(nr_samples)\n    y_lat2 = randomvar(nr_samples)\n    y      = datavar(Float64, nr_samples)\n    x      = datavar(Vector{Float64}, nr_samples)\n\n    # specify observations\n    for k = 1:nr_samples\n\n        # specify latent state\n        x_lat[k] ~ MvNormalMeanPrecision(x[k], 1e3*diagm(ones(2)))\n\n        # specify transformed latent value\n        y_lat1[k] ~ Flow(x_lat[k])\n        y_lat2[k] ~ dot(y_lat1[k], [1, 1])\n\n        # specify observations\n        y[k] ~ Probit(y_lat2[k]) # default: where { pipeline = RequireInbound(in = NormalMeanPrecision(0, 1.0)) }\n\n    end\n\n    # return variables\n    return x_lat, x, y_lat1, y_lat2, y\n\nend","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"fcmodel       = Model(flow_classifier, length(data_y))\ndata          = (y = data_y, x = [data_x[k,:] for k=1:size(data_x,1)], )\n\n@meta function fmeta(model, params)\n    compiled_model = compile(model, params)\n    Flow(y_lat1, x_lat) -> FlowMeta(compiled_model)\nend","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"Here we see that the compilation occurs inside of our probabilistic model. As a result we can pass parameters (and a model) to this function which we wish to opmize for some criterium, such as the variational free energy. Inference can be described as","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"For the optimization procedure, we will simplify our inference loop, such that it only accepts parameters as an argument (which is wishes to optimize) and outputs a performance metric.","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"function f(params)\n    Random.seed!(42) # Flow uses random permutation matrices, which is not good for the optimisation procedure\n    result = inference(\n        model         = fcmodel, \n        data          = data,\n        meta          = fmeta(model, params),\n        free_energy   = true,\n        iterations    = 10, \n        showprogress  = false\n    );\n    \n    result.free_energy[end]\nend;","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"Optimization can be performed using the Optim package. Alternatively, other (custom) optimizers can be implemented, such as:","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"res = optimize(f, randn(StableRNG(42), nr_params(model)), GradientDescent(), Optim.Options(f_tol = 1e-3, store_trace = true, show_trace = true, show_every = 100), autodiff=:forward)","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"uses finitediff and is slower/less accurate.","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"or","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"# create gradient function\ng = (x) -> ForwardDiff.gradient(f, x);\n\n# specify initial params\nparams = randn(nr_params(model))\n\n# create custom optimizer (here Adam)\noptimizer = Adam(params; λ=1e-1)\n\n# allocate space for gradient\n∇ = zeros(nr_params(model))\n\n# perform optimization\nfor it = 1:10000\n\n    # backward pass\n    ∇ .= ForwardDiff.gradient(f, optimizer.x)\n\n    # gradient update\n    ReactiveMP.update!(optimizer, ∇)\n\nend\n","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"res = optimize(f, randn(StableRNG(42), nr_params(model)), GradientDescent(), Optim.Options(store_trace = true, show_trace = true, show_every = 50), autodiff=:forward)\nnothing #hide","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"optimization results are then given as","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"params = Optim.minimizer(res)\ninferred_model = compile(model, params)\ntrans_data_x_1 = hcat(map((x) -> ReactiveMP.forward(inferred_model, x), [data_x[k,:] for k=1:size(data_x,1)])...)'\ntrans_data_x_2 = map((x) -> dot([1, 1], x), [trans_data_x_1[k,:] for k=1:size(data_x,1)])\ntrans_data_x_2_split = [trans_data_x_2[data_y .== 1.0], trans_data_x_2[data_y .== 0.0]]\nfig, ax = plt.subplots(ncols = 3, figsize=(15,5))\nax[1].scatter(data_x[:,1], data_x[:,2], c = data_y)\nax[2].scatter(trans_data_x_1[:,1], trans_data_x_1[:,2], c = data_y)\nax[3].hist(trans_data_x_2_split; stacked=true, bins=50, color = [\"gold\", \"purple\"])\nax[1].grid(), ax[2].grid(), ax[3].grid()\nax[1].set_xlim(-0.25,1.25), ax[1].set_ylim(-0.25,1.25)\nax[1].set_title(\"original data\"), ax[2].set_title(\"|> warp\"), ax[3].set_title(\"|> dot\")\nplt.gcf()","category":"page"},{"location":"examples/flow_tutorial/","page":"Normalizing Flows Tutorial","title":"Normalizing Flows Tutorial","text":"using StatsFuns: normcdf\nclassification_map = map((x) -> normcdf(dot([1,1],x)), map((x) -> ReactiveMP.forward(inferred_model, [x...]), collect(Iterators.product(0:0.01:1, 0:0.01:1))))\nfig, ax = plt.subplots(ncols = 3, figsize=(20,5))\nim1 = ax[1].scatter(data_x[:,1], data_x[:,2], c = data_y)\nim2 = ax[2].scatter(data_x[:,1], data_x[:,2], c = normcdf.(trans_data_x_2))\nax[3].contour(repeat(0:0.01:1, 1, 101), repeat(0:0.01:1, 1, 101)', classification_map)\nplt.colorbar(im1, ax=ax[1])\nplt.colorbar(im2, ax=ax[2])\nax[1].grid(), ax[2].grid(), ax[3].grid()\nax[1].set_xlabel(\"weight 1\"), ax[1].set_ylabel(\"weight 2\"), ax[2].set_xlabel(\"weight 1\"), ax[2].set_ylabel(\"weight 2\"), ax[3].set_xlabel(\"weight 1\"), ax[3].set_ylabel(\"weight 2\")\nax[1].set_title(\"original labels\"), ax[2].set_title(\"predicted labels\"), ax[3].set_title(\"Classification map\")\nplt.gcf()","category":"page"},{"location":"examples/univariate_normal_mixture/#examples-univariate-gaussian-mixture","page":"Univariate Normal Mixture","title":"Example: Univariate Gaussian Mixture","text":"","category":"section"},{"location":"examples/univariate_normal_mixture/","page":"Univariate Normal Mixture","title":"Univariate Normal Mixture","text":"The univariate Gaussian Mixture Model can be represented:","category":"page"},{"location":"examples/univariate_normal_mixture/","page":"Univariate Normal Mixture","title":"Univariate Normal Mixture","text":"beginalign*\n    p(s)   = mathrmBet(salpha_s beta_s) \n    p(m_l) =  mathcalN(m_lmu_l sigma_l)     \n    p(w_l) =  Gamma(w_lalpha_l beta_l) \n    p(z_i) =  mathrmBer(z_is) \n    p(y_i) = prod_l=1^L mathcalNleft(m_l w_lright)^z_i\nendalign*","category":"page"},{"location":"examples/univariate_normal_mixture/","page":"Univariate Normal Mixture","title":"Univariate Normal Mixture","text":"using Rocket, ReactiveMP, GraphPPL\nusing Distributions, Random, BenchmarkTools","category":"page"},{"location":"examples/univariate_normal_mixture/#Model-specification","page":"Univariate Normal Mixture","title":"Model specification","text":"","category":"section"},{"location":"examples/univariate_normal_mixture/","page":"Univariate Normal Mixture","title":"Univariate Normal Mixture","text":"@model [ default_factorisation = MeanField() ] function gaussian_mixture_model(n)\n    \n    s ~ Beta(1.0, 1.0)\n    \n    m1 ~ NormalMeanVariance(-2.0, 1e3)\n    w1 ~ GammaShapeRate(0.01, 0.01)\n    \n    m2 ~ NormalMeanVariance(2.0, 1e3)\n    w2 ~ GammaShapeRate(0.01, 0.01)\n    \n    z = randomvar(n)\n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        z[i] ~ Bernoulli(s)\n        y[i] ~ NormalMixture(z[i], (m1, m2), (w1, w2))\n    end\n    \n    return s, m1, w1, m2, w2, z, y\nend","category":"page"},{"location":"examples/univariate_normal_mixture/#Dataset","page":"Univariate Normal Mixture","title":"Dataset","text":"","category":"section"},{"location":"examples/univariate_normal_mixture/","page":"Univariate Normal Mixture","title":"Univariate Normal Mixture","text":"n = 50\n\nRandom.seed!(124)\n\nswitch = [ 1/3, 2/3 ]\nz      = rand(Categorical(switch), n)\ny      = Vector{Float64}(undef, n)\n\nμ1 = -10.0\nμ2 = 10.0\nw  = 1.777\n\ndists = [\n    Normal(μ1, sqrt(inv(w))),\n    Normal(μ2, sqrt(inv(w))),\n]\n\nfor i in 1:n\n    y[i] = rand(dists[z[i]])\nend","category":"page"},{"location":"examples/univariate_normal_mixture/#Inference","page":"Univariate Normal Mixture","title":"Inference","text":"","category":"section"},{"location":"examples/univariate_normal_mixture/","page":"Univariate Normal Mixture","title":"Univariate Normal Mixture","text":"model = Model(gaussian_mixture_model, length(y))\ndata  = (y = y,)\n\ninitmarginals = (\n    s  = vague(Beta), \n    m1 = NormalMeanVariance(-2.0, 1e3), \n    m2 = NormalMeanVariance(2.0, 1e3), \n    w1 = vague(GammaShapeRate), \n    w2 = vague(GammaShapeRate)\n)\n\nresult = inference(\n    model = model, \n    data  = data, \n    initmarginals = initmarginals, \n    iterations  = 10, \n    free_energy =true\n)","category":"page"},{"location":"examples/univariate_normal_mixture/","page":"Univariate Normal Mixture","title":"Univariate Normal Mixture","text":"mswitch = result.posteriors[:s]\nmm1 = result.posteriors[:m1]\nmm2 = result.posteriors[:m2]\nmw1 = result.posteriors[:w1]\nmw2 = result.posteriors[:w2]\nmz  = result.posteriors[:z]\nfe  = result.free_energy;\nnothing #hide","category":"page"},{"location":"examples/univariate_normal_mixture/","page":"Univariate Normal Mixture","title":"Univariate Normal Mixture","text":"using Plots\n\ndim(d) = (a) -> map(r -> r[d], a)\n\nmp = plot(mean.(mm1), ribbon = var.(mm1) .|> sqrt, label = \"m1 prediction\")\nmp = plot!(mean.(mm2), ribbon = var.(mm2) .|> sqrt, label = \"m2 prediction\")\nmp = plot!(mp, [ μ1 ], seriestype = :hline, label = \"real m1\")\nmp = plot!(mp, [ μ2 ], seriestype = :hline, label = \"real m2\")\n\nwp = plot(mean.(mw1), ribbon = var.(mw1) .|> sqrt, label = \"w1 prediction\", legend = :bottomleft, ylim = (-1, 3))\nwp = plot!(wp, [ w ], seriestype = :hline, label = \"real w1\")\nwp = plot!(wp, mean.(mw2), ribbon = var.(mw2) .|> sqrt, label = \"w2 prediction\")\nwp = plot!(wp, [ w ], seriestype = :hline, label = \"real w2\")\n\nswp = plot(mean.(mswitch), ribbon = var.(mswitch) .|> sqrt, label = \"Switch prediction\")\n\nswp = plot!(swp, [ switch[1] ], seriestype = :hline, label = \"switch[1]\")\nswp = plot!(swp, [ switch[2] ], seriestype = :hline, label = \"switch[2]\")\n\nfep = plot(fe[2:end], label = \"Free Energy\", legend = :bottomleft)\n\nplot(mp, wp, swp, fep, layout = @layout([ a b; c d ]))","category":"page"},{"location":"examples/univariate_normal_mixture/#Benchmark","page":"Univariate Normal Mixture","title":"Benchmark","text":"","category":"section"},{"location":"examples/univariate_normal_mixture/","page":"Univariate Normal Mixture","title":"Univariate Normal Mixture","text":"@benchmark inference(model = $model, data = $data, initmarginals = $initmarginals, iterations = 10, free_energy = true)","category":"page"},{"location":"examples/autoregressive/#examples-autoregressive","page":"Autoregressive Model","title":"Example: Autoregressive model","text":"","category":"section"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"In this example we are going to perform an automated Variational Bayesian Inference for autoregressive model that can be represented as following:","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"beginaligned\np(gamma) = mathrmGamma(gammaa b)\np(mathbftheta) = mathcalN(mathbfthetamathbfmu Sigma)\np(x_tmathbfx_t-1t-k) = mathcalN(x_tmathbftheta^Tmathbfx_t-1t-k gamma^-1)\np(y_tx_t) = mathcalN(y_tx_t tau^-1)\nendaligned","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"where x_t is a current state of our system, mathbfx_t-1t-k is a sequence of k previous states, k is an order of autoregression process, mathbftheta is a vector of transition coefficients, gamma is a precision of state transition process, y_k is a noisy observation of x_k with precision tau.","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"For a more rigorous introduction to Bayesian inference in Autoregressive models we refer to Albert Podusenko, Message Passing-Based Inference for Time-Varying Autoregressive Models.","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"We start with importing all needed packages:","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"using Rocket, ReactiveMP, GraphPPL\nusing Distributions, LinearAlgebra, Parameters, Random, Plots, BenchmarkTools","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"Lets generate some synthetic dataset, we use a predefined set of coeffcients for k = 5:","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"# The following coefficients correspond to stable poles\ncoefs_ar_5 = [0.10699399235785655, -0.5237303489793305, 0.3068897071844715, -0.17232255282458891, 0.13323964347539288]\nnothing #hide","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"function generate_ar_data(rng, n, θ, γ, τ)\n    order        = length(θ)\n    states       = Vector{Vector{Float64}}(undef, n + 3order)\n    observations = Vector{Float64}(undef, n + 3order)\n    \n    γ_std = sqrt(inv(γ))\n    τ_std = sqrt(inv(γ))\n    \n    states[1] = randn(rng, order)\n    \n    for i in 2:(n + 3order)\n        states[i]       = vcat(rand(rng, Normal(dot(θ, states[i - 1]), γ_std)), states[i-1][1:end-1])\n        observations[i] = rand(rng, Normal(states[i][1], τ_std))\n    end\n    \n    return states[1+3order:end], observations[1+3order:end]\nend","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"# Seed for reproducibility\nseed = 123\nrng  = MersenneTwister(seed)\n\n# Number of observations in synthetic dataset\nn = 500\n\n# AR process parameters\nreal_γ = 5.0\nreal_τ = 5.0\nreal_θ = coefs_ar_5\n\nstates, observations = generate_ar_data(rng, n, real_θ, real_γ, real_τ)\nnothing #hide","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"Lets plot our synthetic dataset:","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"plot(first.(states), label = \"Hidden states\")\nscatter!(observations, label = \"Observations\")","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"Next step is to specify probabilistic model and run inference procedure with ReactiveMP. We use GraphPPL.jl package to specify probabilistic model and additional constraints for variational Bayesian Inference. We also specify two different models for Multivariate AR with order k > 1 and for Univariate AR (reduces to simple State-Space-Model) with order k = 1.","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"@model function lar_model(T::Type, n, order, c, τ)\n    \n     \n    # We create a sequence of random variables for hidden states\n    x = randomvar(n)\n    # As well a sequence of observartions\n    y = datavar(Float64, n)\n    \n    ct = constvar(c)\n    # We assume observation noise to be known\n    cτ = constvar(τ)\n    \n    γ  = randomvar()\n    θ  = randomvar()\n    x0 = randomvar()\n    \n    # Prior for first state\n    if T === Multivariate\n        γ  ~ GammaShapeRate(1.0, 1.0)\n        θ  ~ MvNormalMeanPrecision(zeros(order), diageye(order))\n        x0 ~ MvNormalMeanPrecision(zeros(order), diageye(order))\n    else\n        γ  ~ GammaShapeRate(1.0, 1.0)\n        θ  ~ NormalMeanPrecision(0.0, 1.0)\n        x0 ~ NormalMeanPrecision(0.0, 1.0)\n    end\n    \n    x_prev = x0\n    \n    for i in 1:n\n        \n        x[i] ~ AR(x_prev, θ, γ) \n        \n        if T === Multivariate\n            y[i] ~ NormalMeanPrecision(dot(ct, x[i]), cτ)\n        else\n            y[i] ~ NormalMeanPrecision(ct * x[i], cτ)\n        end\n        \n        x_prev = x[i]\n    end\n    return x, y, θ, γ\nend","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"constraints = @constraints begin \n    q(x0, x, θ, γ) = q(x0, x)q(θ)q(γ)\nend","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"@meta function ar_meta(artype, order, stype)\n    AR(x, θ, γ) -> ARMeta(artype, order, stype)\nend","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"morder  = 5\nmartype = Multivariate\nmc      = ReactiveMP.ar_unit(martype, morder)\nmmeta   = ar_meta(martype, morder, ARsafe())\n\nmoptions = (limit_stack_depth = 100, )\n\nmmodel         = Model(lar_model, martype, length(observations), morder, mc, real_τ)\nmdata          = (y = observations, )\nminitmarginals = (γ = GammaShapeRate(1.0, 1.0), θ = MvNormalMeanPrecision(zeros(morder), diageye(morder)))\nmreturnvars    = (x = KeepLast(), γ = KeepEach(), θ = KeepEach())\n\n# First execution is slow due to Julia's initial compilation \nmresult = inference(\n    model = mmodel, \n    data  = mdata,\n    constraints   = constraints,\n    meta          = mmeta,\n    options       = moptions,\n    initmarginals = minitmarginals,\n    returnvars    = mreturnvars,\n    free_energy   = true,\n    iterations    = 100, \n    showprogress  = true\n)","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"@unpack x, γ, θ = mresult.posteriors\n\nfe = mresult.free_energy;","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"p1 = plot(first.(states), label=\"Hidden state\")\np1 = scatter!(p1, observations, label=\"Observations\")\np1 = plot!(p1, first.(mean.(x)), ribbon = first.(std.(x)), label=\"Inferred states\", legend = :bottomright)\n\np2 = plot(mean.(γ), ribbon = std.(γ), label = \"Inferred transition precision\", legend = :topright)\np2 = plot!([ real_γ ], seriestype = :hline, label = \"Real transition precision\")\n\np3 = plot(fe, label = \"Bethe Free Energy\")\n\nplot(p1, p2, p3, layout = @layout([ a; b c ]))","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"Lets also plot a subrange of our results:","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"subrange = div(n,5):(div(n, 5) + div(n, 5))\n\nplot(subrange, first.(states)[subrange], label=\"Hidden state\")\nscatter!(subrange, observations[subrange], label=\"Observations\")\nplot!(subrange, first.(mean.(x))[subrange], ribbon = sqrt.(first.(var.(x)))[subrange], label=\"Inferred states\", legend = :bottomright)","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"It is also interesting to see where our AR coefficients converge to:","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"let\n    pθ = plot()\n\n    θms = mean.(θ)\n    θvs = var.(θ)\n    \n    l = length(θms)\n\n    edim(e) = (a) -> map(r -> r[e], a)\n\n    for i in 1:length(first(θms))\n        pθ = plot!(pθ, θms |> edim(i), ribbon = θvs |> edim(i) .|> sqrt, label = \"Estimated θ[$i]\")\n    end\n    \n    for i in 1:length(real_θ)\n        pθ = plot!(pθ, [ real_θ[i] ], seriestype = :hline, label = \"Real θ[$i]\")\n    end\n    \n    plot(pθ, legend = :outertopright, size = (800, 300))\nend","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"println(\"$(length(real_θ))-order AR inference Bethe Free Energy: \", last(fe))\nnothing #hide","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"We can also run a 1-order AR inference on 5-order AR data:","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"uorder  = 1\nuartype = Univariate\nuc      = ReactiveMP.ar_unit(uartype, uorder)\numeta   = ar_meta(uartype, uorder, ARsafe())\n\nuoptions = (limit_stack_depth = 100, )\n\numodel         = Model(lar_model, uartype, length(observations), uorder, uc, real_τ)\nudata          = (y = observations, )\nuinitmarginals = (γ = GammaShapeRate(1.0, 1.0), θ = NormalMeanPrecision(0.0, 1.0))\nureturnvars    = (x = KeepLast(), γ = KeepEach(), θ = KeepEach())\n\nuresult = inference(\n    model = umodel, \n    data  = udata,\n    meta  = umeta,\n    constraints   = constraints,\n    initmarginals = uinitmarginals,\n    returnvars    = ureturnvars,\n    free_energy   = true,\n    iterations    = 15, \n    showprogress  = false\n)","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"println(\"1-order AR inference Bethe Free Energy: \", last(fe))\nnothing #hide","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"We can see that, according to final Bethe Free Energy value, in this example 5-order AR process can describe data better than 1-order AR.","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"We may be also interested in benchmarking our algorithm:","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"@benchmark inference(model = $umodel, constraints = $constraints, meta = $umeta, data = $udata, initmarginals = $uinitmarginals, free_energy = true, iterations = 15, showprogress = false)","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"@benchmark inference(model = $mmodel, constraints = $constraints, meta = $mmeta, data = $mdata, initmarginals = $minitmarginals, free_energy = true, iterations = 15, showprogress = false)","category":"page"},{"location":"examples/hidden_markov_model/#examples-hidden-markov-model","page":"Hidden Markov Model","title":"Example: Hidden Markov Model","text":"","category":"section"},{"location":"examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"In this demo we are interested in Bayesian inference of parameters of a hidden Markov model (HMM)., Specifically, we consider a first-order HMM with hidden states s_0 s_1 dots s_T and observations x_1 dots x_T governed by a state transition probability matrix A and an observation probability matrix B:,","category":"page"},{"location":"examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"beginalign*\n    s_t  sim mathcalCat(A s_t-1)\n    x_t  sim mathcalCat(B s_t)\nendalign*","category":"page"},{"location":"examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"We assume three possible states (\\\"red\\\", \\\"green\\\" and \\\"blue\\\"), and the goal is to estimate matrices A and B from a simulated data set. To have a full Bayesian treatment of the problem, both A and B are endowed with priors (Dirichlet distributions on the columns).\"","category":"page"},{"location":"examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"using Rocket, ReactiveMP, GraphPPL\nusing Random, BenchmarkTools, Distributions, MacroTools, LinearAlgebra\nusing Plots","category":"page"},{"location":"examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"function rand_vec(rng, distribution::Categorical) \n    k = ncategories(distribution)\n    s = zeros(k)\n    s[ rand(rng, distribution) ] = 1.0\n    s\nend\n\nfunction generate_data(n_samples; seed = 124)\n    \n    rng = MersenneTwister(seed)\n    \n    # Transition probabilities (some transitions are impossible)\n    A = [0.9 0.0 0.1; 0.1 0.9 0.0; 0.0 0.1 0.9] \n    # Observation noise\n    B = [0.9 0.05 0.05; 0.05 0.9 0.05; 0.05 0.05 0.9] \n    # Initial state\n    s_0 = [1.0, 0.0, 0.0] \n    # Generate some data\n    s = Vector{Vector{Float64}}(undef, n_samples) # one-hot encoding of the states\n    x = Vector{Vector{Float64}}(undef, n_samples) # one-hot encoding of the observations\n    \n    s_prev = s_0\n    \n    for t = 1:n_samples\n        a = A * s_prev\n        s[t] = rand_vec(rng, Categorical(a ./ sum(a)))\n        b = B * s[t]\n        x[t] = rand_vec(rng, Categorical(b ./ sum(b)))\n        s_prev = s[t]\n    end\n    \n    return x, s\nend","category":"page"},{"location":"examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"# Test data\nN = 100\n\nx_data, s_data = generate_data(N)\n\nscatter(argmax.(s_data))","category":"page"},{"location":"examples/hidden_markov_model/#Model-specification","page":"Hidden Markov Model","title":"Model specification","text":"","category":"section"},{"location":"examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"# Model specification\n@model function hidden_markov_model(n)\n    \n    A ~ MatrixDirichlet(ones(3, 3)) \n    B ~ MatrixDirichlet([ 10.0 1.0 1.0; 1.0 10.0 1.0; 1.0 1.0 10.0 ])\n    \n    s_0 ~ Categorical(fill(1.0 / 3.0, 3))\n    \n    s = randomvar(n)\n    x = datavar(Vector{Float64}, n)\n    \n    s_prev = s_0\n    \n    for t in 1:n\n        s[t] ~ Transition(s_prev, A) \n        x[t] ~ Transition(s[t], B)\n        s_prev = s[t]\n    end\n    \nend\n\n@constraints function hidden_markov_model_constraints()\n    q(s_0, s, A, B) = q(s_0, s)q(A)q(B)\nend","category":"page"},{"location":"examples/hidden_markov_model/#Inference","page":"Hidden Markov Model","title":"Inference","text":"","category":"section"},{"location":"examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"idata = (x = x_data, )\n\nimodel = Model(hidden_markov_model, N)\n\nimarginals = (\n    A = vague(MatrixDirichlet, 3, 3), \n    B = vague(MatrixDirichlet, 3, 3), \n    s = vague(Categorical, 3)\n)\n\nireturnvars = (\n    A = KeepLast(),\n    B = KeepLast(),\n    s = KeepLast()\n)\n\nresult = inference(\n    model         = imodel, \n    data          = idata,\n    constraints   = hidden_markov_model_constraints(),\n    initmarginals = imarginals, \n    returnvars    = ireturnvars, \n    iterations    = 20, \n    free_energy   = true\n)","category":"page"},{"location":"examples/hidden_markov_model/#Results","page":"Hidden Markov Model","title":"Results","text":"","category":"section"},{"location":"examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"mean(result.posteriors[:A])","category":"page"},{"location":"examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"mean(result.posteriors[:B])","category":"page"},{"location":"examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"p1 = scatter(argmax.(s_data), title=\"Inference results\", label = \"real\", ms = 6)\np1 = scatter!(p1, argmax.(ReactiveMP.probvec.(result.posteriors[:s])), label = \"inferred\", ms = 2)\np2 = plot(result.free_energy, label=\"Free energy\")\n\nplot(p1, p2, layout = @layout([ a; b ]))","category":"page"},{"location":"examples/hidden_markov_model/#Custom-inference","page":"Hidden Markov Model","title":"Custom inference","text":"","category":"section"},{"location":"examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"@model [ default_factorisation = MeanField() ] function custom_optimised_hidden_markov_model(n)\n    \n    A ~ MatrixDirichlet(ones(3, 3)) \n    B ~ MatrixDirichlet([ 10.0 1.0 1.0; 1.0 10.0 1.0; 1.0 1.0 10.0 ])\n    \n    s_0 ~ Categorical(fill(1.0 / 3.0, 3))\n    \n    s = randomvar(n)\n    x = datavar(Vector{Float64}, n)\n    \n    s_prev = s_0\n    \n    for t in 1:n\n        s[t] ~ Transition(s_prev, A) where { q = q(out, in)q(a) }\n        x[t] ~ Transition(s[t], B)\n        s_prev = s[t]\n    end\n    \n    return s, x, A, B\nend","category":"page"},{"location":"examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"function custom_optimised_inference(data, vmp_iters)\n    n = length(data)\n    \n    model, (s, x, A, B) = custom_optimised_hidden_markov_model(model_options(limit_stack_depth = 500), n)\n    \n    sbuffer = keep(Vector{Marginal})\n    Abuffer = keep(Marginal)\n    Bbuffer = keep(Marginal)\n    fe      = ScoreActor(Float64)\n    \n    ssub  = subscribe!(getmarginals(s), sbuffer)\n    Asub  = subscribe!(getmarginal(A), Abuffer)\n    Bsub  = subscribe!(getmarginal(B), Bbuffer)\n    fesub = subscribe!(score(Float64, BetheFreeEnergy(), model), fe)\n    \n    setmarginal!(A, vague(MatrixDirichlet, 3, 3))\n    setmarginal!(B, vague(MatrixDirichlet, 3, 3))\n    \n    foreach(s) do svar\n        setmarginal!(svar, vague(Categorical, 3))\n    end\n    \n    for i in 1:vmp_iters\n        update!(x, data)\n    end\n    \n    unsubscribe!(ssub)\n    unsubscribe!(Asub)\n    unsubscribe!(Bsub)\n    unsubscribe!(fesub)\n    \n    return map(getvalues, (sbuffer, Abuffer, Bbuffer, fe))\nend","category":"page"},{"location":"examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"sbuffer, Abuffer, Bbuffer, fe = custom_optimised_inference(x_data, 20)\n\n@assert mean.(last(sbuffer)) ≈ mean.(result.posteriors[:s])\nnothing #hide","category":"page"},{"location":"examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"p1 = scatter(argmax.(s_data), title=\"Inference results\", label = \"real\", ms = 6)\np1 = scatter!(p1, argmax.(ReactiveMP.probvec.(last(sbuffer))), label = \"inferred\", ms = 2)\np2 = plot(result.free_energy, label=\"Free energy\")\n\nplot(p1, p2, layout = @layout([ a; b ]))","category":"page"},{"location":"examples/hidden_markov_model/#Benchmark-timings","page":"Hidden Markov Model","title":"Benchmark timings","text":"","category":"section"},{"location":"examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"@benchmark inference(\n    model         = $imodel, \n    data          = $idata,\n    constraints   = hidden_markov_model_constraints(),\n    initmarginals = $imarginals, \n    returnvars    = $ireturnvars, \n    iterations    = 20, \n    free_energy   = true\n)","category":"page"},{"location":"examples/hidden_markov_model/","page":"Hidden Markov Model","title":"Hidden Markov Model","text":"@benchmark custom_optimised_inference($x_data, 20)","category":"page"},{"location":"extra/contributing/#Contribution-guidelines","page":"Contributing","title":"Contribution guidelines","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We welcome all possible contributors. This page details the some of the guidelines that should be followed when contributing to this package.","category":"page"},{"location":"extra/contributing/#Reporting-bugs","page":"Contributing","title":"Reporting bugs","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We track bugs using GitHub issues. We encourage you to write complete, specific, reproducible bug reports. Mention the versions of Julia and ReactiveMP for which you observe unexpected behavior. Please provide a concise description of the problem and complement it with code snippets, test cases, screenshots, tracebacks or any other information that you consider relevant. This will help us to replicate the problem and narrow the search space for solutions.","category":"page"},{"location":"extra/contributing/#Suggesting-features","page":"Contributing","title":"Suggesting features","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We welcome new feature proposals. However, before submitting a feature request, consider a few things:","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"Does the feature require changes in the core ReactiveMP.jl code? If it doesn't (for example, you would like to add a factor node for a particular application), you can add local extensions in your script/notebook or consider making a separate repository for your extensions.\nIf you would like to add an implementation of a feature that changes a lot in the core ReactiveMP.jl code, please open an issue on GitHub and describe your proposal first. This will allow us to discuss your proposal with you before you invest your time in implementing something that may be difficult to merge later on.","category":"page"},{"location":"extra/contributing/#Contributing-code","page":"Contributing","title":"Contributing code","text":"","category":"section"},{"location":"extra/contributing/#Installing-ReactiveMP","page":"Contributing","title":"Installing ReactiveMP","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We suggest that you use the dev command from the new Julia package manager to install ReactiveMP.jl for development purposes. To work on your fork of ReactiveMP.jl, use your fork's URL address in the dev command, for example:","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"] dev git@github.com:your_username/ReactiveMP.jl.git","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"The dev command clones ReactiveMP.jl to ~/.julia/dev/ReactiveMP. All local changes to ReactiveMP code will be reflected in imported code.","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"note: Note\nIt is also might be useful to install Revise.jl package as it allows you to modify code and use the changes without restarting Julia.","category":"page"},{"location":"extra/contributing/#Committing-code","page":"Contributing","title":"Committing code","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We use the standard GitHub Flow workflow where all contributions are added through pull requests. In order to contribute, first fork the repository, then commit your contributions to your fork, and then create a pull request on the master branch of the ReactiveMP.jl repository.","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"Before opening a pull request, please make sure that all tests pass without failing! All demos (can be found in /demo/ directory) and benchmarks (can be found in /benchmark/ directory) have to run without errors as well.","category":"page"},{"location":"extra/contributing/#Style-conventions","page":"Contributing","title":"Style conventions","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We use default Julia style guide. We list here a few important points and our modifications to the Julia style guide:","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"Use 4 spaces for indentation\nType names use UpperCamelCase. For example: AbstractFactorNode, RandomVariable, etc..\nFunction names are lowercase with underscores, when necessary. For example: activate!, randomvar, as_variable, etc..\nVariable names and function arguments use snake_case\nThe name of a method that modifies its argument(s) must end in !","category":"page"},{"location":"extra/contributing/#Unit-tests","page":"Contributing","title":"Unit tests","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We use the test-driven development (TDD) methodology for ReactiveMP.jl development. The test coverage should be as complete as possible. Please make sure that you write tests for each piece of code that you want to add.","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"All unit tests are located in the /test/ directory. The /test/ directory follows the structure of the /src/ directory. Each test file should have following filename format: test_*.jl. Some tests are also present in jldoctest docs annotations directly in the source code. See Julia's documentation about doctests.","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"The tests can be evaluated by running following command in the Julia REPL:","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"] test ReactiveMP","category":"page"},{"location":"lib/nodes/flow/#lib-nodes-flow","page":"Flow","title":"Flow node","text":"","category":"section"},{"location":"lib/nodes/flow/","page":"Flow","title":"Flow","text":"See also Flow tutorial for a comprehensive guide on using flows in ReactiveMP.","category":"page"},{"location":"lib/nodes/flow/","page":"Flow","title":"Flow","text":"PlanarFlow\nRadialFlow\nFlowModel\nCompiledFlowModel\ncompile\nAdditiveCouplingLayer\nInputLayer\nPermutationLayer\nFlowMeta","category":"page"},{"location":"lib/nodes/flow/#ReactiveMP.PlanarFlow","page":"Flow","title":"ReactiveMP.PlanarFlow","text":"The PlanarFlow function is defined as\n\nf(bfx) = bfx + bfu tanh(bfw^top bfx + b)\n\nwith input and output dimension D. Here bfxin mathbbR^D represents the input of the function. Furthermore bfuin mathbbR^D, bfwin mathbbR^D and binmathbbR represent the parameters of the function. The function contracts and expands the input space. \n\nThis function has been introduced in:\n\nRezende, Danilo, and Shakir Mohamed. \"Variational inference with normalizing flows.\" International conference on machine learning. PMLR, 2015.\n\n\n\n\n\n","category":"type"},{"location":"lib/nodes/flow/#ReactiveMP.RadialFlow","page":"Flow","title":"ReactiveMP.RadialFlow","text":"The RadialFlow function is defined as\n\nf(bfx) = bfx + fracbeta(bfz - bfz_0)alpha + bfz - bfz_0\n\nwith input and output dimension D. Here bfxin mathbbR^D represents the input of the function. Furthermore bfz_0in mathbbR^D, alphain mathbbR and betainmathbbR represent the parameters of the function. The function contracts and expands the input space. \n\nThis function has been introduced in:\n\nRezende, Danilo, and Shakir Mohamed. \"Variational inference with normalizing flows.\" International conference on machine learning. PMLR, 2015.\n\n\n\n\n\n","category":"type"},{"location":"lib/nodes/flow/#ReactiveMP.FlowModel","page":"Flow","title":"ReactiveMP.FlowModel","text":"The FlowModel structure is the most generic type of Flow model, in which the layers are not constrained to be of a specific type. The FlowModel structure contains the input dimensionality and a tuple of layers and can be constructed as FlowModel( dim, (layer1, layer2, ...) ).\n\nNote: this model can be specialized by constraining the types of layers. This potentially allows for more efficient specialized methods that can deal with specifics of these layers, such as triangular jacobian matrices.\n\n\n\n\n\n","category":"type"},{"location":"lib/nodes/flow/#ReactiveMP.CompiledFlowModel","page":"Flow","title":"ReactiveMP.CompiledFlowModel","text":"The CompiledFlowModel structure is the most generic type of compiled Flow model, in which the layers are not constrained to be of a specific type. The FlowModel structure contains the input dimension and a tuple of compiled layers. Do not manually create a CompiledFlowModel! Instead create a FlowModel first and compile it with compile(model::FlowModel). This will make sure that all layers/mappings are configured with the proper dimensionality and with randomly sampled parameters. Alternatively, if you would like to pass your own parameters, call compile(model::FlowModel, params::Vector).\n\nNote: this model can be specialized by constraining the types of layers. This potentially allows for more efficient specialized methods that can deal with specifics of these layers, such as triangular jacobian matrices.\n\n\n\n\n\n","category":"type"},{"location":"lib/nodes/flow/#ReactiveMP.compile","page":"Flow","title":"ReactiveMP.compile","text":"compile() compiles a model by setting its parameters. It randomly sets parameter values in the layers and flows such that inference in the model can be obtained.\n\nInput arguments\n\nmodel::FlowModel - a model of which the dimensionality of its layers/flows has been initialized, but its parameters have not been set.\n\nReturn arguments\n\n::CompiledFlowModel - a compiled model with set parameters, such that it can be used for processing data.\n\n\n\n\n\ncompile(model::FlowModel, params::Vector) lets you initialize a model model with a vector of parameters params.\n\nInput arguments\n\nmodel::FlowModel - a model of which the dimensionality of its layers/flows has been initialized, but its parameters have not been set.\nparams::Vector   - a vector of parameters with which the model should be compiled.\n\nReturn arguments\n\n::CompiledFlowModel - a compiled model with set parameters, such that it can be used for processing data.\n\n\n\n\n\n","category":"function"},{"location":"lib/nodes/flow/#ReactiveMP.AdditiveCouplingLayer","page":"Flow","title":"ReactiveMP.AdditiveCouplingLayer","text":"The additive coupling layer specifies an invertible function bfy = g(bfx) following the specific structure (for the mapping g mathbbR^2 rightarrow mathbbR^2):\n\n    beginalign\n        y_1 = x_1 \n        y_2 = x_2 + f(x_1)\n    endalign\n\nwhere f(cdot) denotes an arbitrary function with mapping f mathbbR rightarrow mathbbR. This function can be chosen arbitrarily complex. Non-linear functions (neural networks) are often chosen to model complex relationships. From the definition of the model, invertibility can be easily achieved as\n\n    beginalign\n        x_1 = y_1 \n        x_2 = y_2 - f(y_1)\n    endalign\n\nThe current implementation only allows for the mapping g mathbbR^2 rightarrow mathbbR^2, although this layer can be generalized for arbitrary input dimensions.\n\nAdditiveCouplingLayer(f <: AbstractCouplingFlow) creates the layer structure with function f.\n\nExample\n\nf = PlanarFlow()\nlayer = AdditiveCouplingLayer(f)\n\nThis layer structure has been introduced in:\n\nDinh, Laurent, David Krueger, and Yoshua Bengio. \"Nice: Non-linear independent components estimation.\" arXiv preprint arXiv:1410.8516 (2014).\n\n\n\n\n\n","category":"type"},{"location":"lib/nodes/flow/#ReactiveMP.InputLayer","page":"Flow","title":"ReactiveMP.InputLayer","text":"The input layer specifies the input dimension to a flow model.\n\nlayer = InputLayer(3)\n\n\n\n\n\n","category":"type"},{"location":"lib/nodes/flow/#ReactiveMP.PermutationLayer","page":"Flow","title":"ReactiveMP.PermutationLayer","text":"The permutation layer specifies an invertible mapping bfy = g(bfx) = Pbfx where P is a permutation matrix.\n\n\n\n\n\n","category":"type"},{"location":"lib/nodes/flow/#ReactiveMP.FlowMeta","page":"Flow","title":"ReactiveMP.FlowMeta","text":"The FlowMeta structure contains the meta data of the Flow factor node. More specifically, it contains the model of the Flow factor node. The FlowMeta structure can be constructed as FlowMeta(model). Make sure that the flow model has been compiled.\n\nThe FlowMeta structure is required for the Flow factor node and can be included with the Flow node as: y ~ Flow(x) where { meta = FlowMeta(...) }\n\n\n\n\n\n","category":"type"},{"location":"lib/prod/#lib-prod","page":"Prod implementation","title":"Prod implementation","text":"","category":"section"},{"location":"lib/prod/","page":"Prod implementation","title":"Prod implementation","text":"prod_analytical_rule\nresolve_prod_constraint\nprod(::ProdAnalytical, left, right)\nProdAnalytical\nProdFinal\nProdPreserveType\nProdPreserveTypeLeft\nProdPreserveTypeRight","category":"page"},{"location":"lib/prod/#ReactiveMP.prod_analytical_rule","page":"Prod implementation","title":"ReactiveMP.prod_analytical_rule","text":"prod_analytical_rule(::Type, ::Type)\n\nReturns either ProdAnalyticalRuleAvailable or ProdAnalyticalRuleUnknown for two given distribution types. Returns ProdAnalyticalRuleUnknown by default.\n\nSee also: prod, ProdAnalytical, ProdGeneric\n\n\n\n\n\n","category":"function"},{"location":"lib/prod/#ReactiveMP.resolve_prod_constraint","page":"Prod implementation","title":"ReactiveMP.resolve_prod_constraint","text":"resolve_prod_constraint(left, right)\n\nGiven two product constraints returns a single one that has a higher priority (if possible).\n\nSee also: prod, ProdAnalytical, ProdGeneric\n\n\n\n\n\n","category":"function"},{"location":"lib/prod/#Base.prod-Tuple{ProdAnalytical, Any, Any}","page":"Prod implementation","title":"Base.prod","text":"prod(strategy, left, right)\n\nprod function is used to find a product of two probability distrubution over same variable (e.g. 𝓝(x|μ1, σ1) × 𝓝(x|μ2, σ2)). There are multiple strategies for prod function, e.g. ProdAnalytical, ProdGeneric or ProdPreserveType.\n\nExamples:\n\nusing ReactiveMP\n\nproduct = prod(ProdAnalytical(), NormalMeanVariance(-1.0, 1.0), NormalMeanVariance(1.0, 1.0))\n\nmean(product), var(product)\n\n# output\n(0.0, 0.5)\n\nSee also: prod_analytical_rule, ProdAnalytical, ProdGeneric\n\n\n\n\n\n","category":"method"},{"location":"lib/prod/#ReactiveMP.ProdAnalytical","page":"Prod implementation","title":"ReactiveMP.ProdAnalytical","text":"ProdAnalytical\n\nProdAnalytical is one of the strategies for prod function. This strategy uses analytical prod methods but does not constraint a prod to be in any specific form. It fails if no analytical rules is available, use ProdGeneric prod strategy to fallback to approximation methods.\n\nSee also: prod, ProdPreserveType, ProdGeneric\n\n\n\n\n\n","category":"type"},{"location":"lib/prod/#ReactiveMP.ProdFinal","page":"Prod implementation","title":"ReactiveMP.ProdFinal","text":"ProdFinal{T}\n\nThe ProdFinal is a wrapper around a distribution. By passing it as a message along an edge of the graph the corresponding marginal is calculated as the distribution of the ProdFinal.  In a sense, the ProdFinal ignores any further prod with any other distribution for calculating the marginal and only check for variate types of two distributions. Trying to prod two instances of ProdFinal will result in an error. Note: ProdFinal is not a prod strategy, as opposed to ProdAnalytical and ProdGeneric.\n\nSee also: [BIFM]\n\n\n\n\n\n","category":"type"},{"location":"lib/prod/#ReactiveMP.ProdPreserveType","page":"Prod implementation","title":"ReactiveMP.ProdPreserveType","text":"ProdPreserveType{T}\n\nProdPreserveType is one of the strategies for prod function. This strategy constraint an output of a prod to be in some specific form. By default it fallbacks to a ProdAnalytical strategy and converts an output to a prespecified type but can be overwritten for some distributions for better performance.\n\nSee also: prod, ProdAnalytical, ProdPreserveTypeLeft, ProdPreserveTypeRight\n\n\n\n\n\n","category":"type"},{"location":"lib/prod/#ReactiveMP.ProdPreserveTypeLeft","page":"Prod implementation","title":"ReactiveMP.ProdPreserveTypeLeft","text":"ProdPreserveTypeLeft\n\nProdPreserveTypeLeft is one of the strategies for prod function. This strategy constraint an output of a prod to be in the functional form as left argument. By default it fallbacks to a ProdPreserveType strategy and converts an output to a prespecified type but can be overwritten for some distributions for better performance.\n\nSee also: prod, ProdPreserveType, ProdPreserveTypeRight\n\n\n\n\n\n","category":"type"},{"location":"lib/prod/#ReactiveMP.ProdPreserveTypeRight","page":"Prod implementation","title":"ReactiveMP.ProdPreserveTypeRight","text":"ProdPreserveTypeRight\n\nProdPreserveTypeRight is one of the strategies for prod function. This strategy constraint an output of a prod to be in the functional form as right argument. By default it fallbacks to a ProdPreserveType strategy and converts an output to a prespecified type but can be overwritten for some distributions for better performance.\n\nSee also: prod, ProdPreserveType, ProdPreserveTypeLeft\n\n\n\n\n\n","category":"type"},{"location":"lib/prod/#Dist-product","page":"Prod implementation","title":"Dist product","text":"","category":"section"},{"location":"lib/prod/","page":"Prod implementation","title":"Prod implementation","text":"DistProduct\nProdGeneric","category":"page"},{"location":"lib/prod/#ReactiveMP.DistProduct","page":"Prod implementation","title":"ReactiveMP.DistProduct","text":"DistProduct\n\nIf inference backend cannot return an analytical solution for a product of two distributions it may fallback to the DistProduct structure DistProduct is useful to propagate the exact forms of two messages until it hits some approximation method for form-constraint. However DistProduct cannot be used to compute statistics such as mean or variance.  It has to be approximated before using in actual inference procedure.\n\nBackend exploits form constraints specification which usually help to deal with intractable distributions products. \n\nSee also: prod, ProdGeneric\n\n\n\n\n\n","category":"type"},{"location":"lib/prod/#ReactiveMP.ProdGeneric","page":"Prod implementation","title":"ReactiveMP.ProdGeneric","text":"ProdGeneric{C}\n\nProdGeneric is one of the strategies for prod function. This strategy does not fail in case of no analytical rule is available, but simply creates a product tree, there all nodes represent the prod function and all leaves are valid Distribution object. This object does not define any statistical properties (such as mean or var etc) and cannot be used during the inference procedure. However this object plays imporant part in the functional form constraints implementation.  In a few words this object keeps all the information of a product of messages and propagates this information in the functional form constraint.\n\nProdGeneric has a \"fallback\" method, which it may or may not use under some circumstances. For example if the fallback method is ProdAnalytical (which is the default one) - ProdGeneric will try to optimize prod tree with analytical solutions where possible.\n\nSee also: prod, DistProduct, ProdAnalytical, ProdPreserveType, prod_analytical_rule\n\n\n\n\n\n","category":"type"},{"location":"examples/multivariate_normal_mixture/#examples-multivariate-gaussian-mixture","page":"Multivariate Normal Mixture","title":"Example: Multivariate Gaussian Mixture","text":"","category":"section"},{"location":"examples/multivariate_normal_mixture/","page":"Multivariate Normal Mixture","title":"Multivariate Normal Mixture","text":"The multivariate Gaussian Mixture Model can be represented as:","category":"page"},{"location":"examples/multivariate_normal_mixture/","page":"Multivariate Normal Mixture","title":"Multivariate Normal Mixture","text":"beginalign*\n    p(mathbfs)   = mathrmDir(mathbfsboldsymbolalpha) \n    p(mathbfm_l) =  mathcalN(m_lboldsymbolmu_l boldsymbolSigma_l)     \n    p(mathbfW_l) =  mathcalW(mathbfW_lmathbfV_l nu_l) \n    p(mathbfz_i) =  mathrmCat(mathbfz_imathbfs) \n    p(mathbfy_i) = prod_l=1^L mathcalNleft(mathbfm_l mathbfW_lright)^mathbfz_i\nendalign*","category":"page"},{"location":"examples/multivariate_normal_mixture/","page":"Multivariate Normal Mixture","title":"Multivariate Normal Mixture","text":"ENV[\"GKS_ENCODING\"]=\"utf8\"\n\nusing Rocket, ReactiveMP, GraphPPL\nusing Distributions, Plots\nusing Random, LinearAlgebra, BenchmarkTools","category":"page"},{"location":"examples/multivariate_normal_mixture/#Dataset","page":"Multivariate Normal Mixture","title":"Dataset","text":"","category":"section"},{"location":"examples/multivariate_normal_mixture/","page":"Multivariate Normal Mixture","title":"Multivariate Normal Mixture","text":"Random.seed!(125)\n\nL         = 50.0\nnmixtures = 6\nn_samples = 500\n\nprobvec = ones(nmixtures)\nprobvec = probvec ./ sum(probvec)\n\nswitch = Categorical(probvec)\n\nprintln(\"Switch distribution: \", Distributions.params(switch))\n\ngaussians = map(1:nmixtures) do index\n    angle      = 2π / nmixtures * (index - 1)\n    basis_v    = L * [ 1.0, 0.0 ]\n    rotationm  = [ cos(angle) -sin(angle); sin(angle) cos(angle) ]\n    mean       = rotationm * basis_v \n    covariance = Matrix(Hermitian(rotationm * [ 10.0 0.0; 0.0 20.0 ] * transpose(rotationm)))\n    return MvNormal(mean, covariance)\nend\n\nz = rand(switch, n_samples)\ny = Vector{Vector{Float64}}(undef, n_samples)\n\nfor i in 1:n_samples\n    y[i] = rand(gaussians[z[i]])\nend","category":"page"},{"location":"examples/multivariate_normal_mixture/","page":"Multivariate Normal Mixture","title":"Multivariate Normal Mixture","text":"sdim(n) = (a) -> map(d -> d[n], a)","category":"page"},{"location":"examples/multivariate_normal_mixture/","page":"Multivariate Normal Mixture","title":"Multivariate Normal Mixture","text":"global p = plot(xlim = (-1.5L, 1.5L), ylim = (-1.5L, 1.5L))\n\nfor (index, gaussian) in enumerate(gaussians)\n    global p\n    p = contour!(p, range(-2L, 2L, step = 0.25), range(-2L, 2L, step = 0.25), (x, y) -> pdf(gaussian, [ x, y ]), levels = 3, colorbar = false)\nend\n\np = scatter!(p, y |> sdim(1), y |> sdim(2), ms = 2, alpha = 0.4)\n\nplot(p, size = (600, 400), legend=false)","category":"page"},{"location":"examples/multivariate_normal_mixture/#Model-specification","page":"Multivariate Normal Mixture","title":"Model specification","text":"","category":"section"},{"location":"examples/multivariate_normal_mixture/","page":"Multivariate Normal Mixture","title":"Multivariate Normal Mixture","text":"@model [ default_factorisation = MeanField() ] function gaussian_mixture_model(rng, nmixtures, n, priors_mean, priors_cov)\n    \n    z = randomvar(n)\n    m = randomvar(nmixtures)\n    w = randomvar(nmixtures)\n    \n    basis_v = [ 1.0, 0.0 ]\n    \n    for i in 1:nmixtures        \n        m[i] ~ MvNormalMeanCovariance(priors_mean[i], priors_cov[i])\n        w[i] ~ Wishart(3, [ 1e2 0.0; 0.0 1e2 ])\n    end\n    \n    s ~ Dirichlet(ones(nmixtures))\n\n    y = datavar(Vector{Float64}, n)\n    \n    means = tuple(m...)\n    precs = tuple(w...)\n    \n    for i in 1:n\n        z[i] ~ Categorical(s) \n        y[i] ~ NormalMixture(z[i], means, precs)\n    end\n    \n    return s, z, m, w, y\nend","category":"page"},{"location":"examples/multivariate_normal_mixture/#Inference","page":"Multivariate Normal Mixture","title":"Inference","text":"","category":"section"},{"location":"examples/multivariate_normal_mixture/","page":"Multivariate Normal Mixture","title":"Multivariate Normal Mixture","text":"rng = MersenneTwister(11)\n\nbasis_v       = [ 1.0, 0.0 ]\napproximate_L = 50rand(rng)\npriors_mean   = []\n\nfor i in 1:nmixtures\n    approximate_angle_prior = ((2π + rand(rng)) / nmixtures) * (i - 1)\n    approximate_basis_v  = approximate_L / 2 * (basis_v .+ rand(rng, 2))\n    approximate_rotation = [ cos(approximate_angle_prior) -sin(approximate_angle_prior); sin(approximate_angle_prior) cos(approximate_angle_prior) ]\n    push!(priors_mean,  approximate_rotation * approximate_basis_v)\nend\n\npriors_cov = [ [ 1e6 0.0; 0.0 1e6 ] for _ in 1:nmixtures ]\n\ninitmarginals = (\n    s = vague(Dirichlet, nmixtures), \n    m = [ MvNormalMeanCovariance(prior[1], prior[2]) for prior in zip(priors_mean, priors_cov) ], \n    w = Wishart(3, [ 1e2 0.0; 0.0 1e2 ])\n)\n\nresult = inference(\n    model = Model(gaussian_mixture_model, rng, nmixtures, length(y), priors_mean, priors_cov), \n    data  = (y = y,), \n    initmarginals = initmarginals, \n    iterations  = 15, \n    free_energy = true\n)","category":"page"},{"location":"examples/multivariate_normal_mixture/","page":"Multivariate Normal Mixture","title":"Multivariate Normal Mixture","text":"s = result.posteriors[:s]\nz = result.posteriors[:z]\nm = result.posteriors[:m]\nw = result.posteriors[:w]\nfe = result.free_energy;\nnothing #hide","category":"page"},{"location":"examples/multivariate_normal_mixture/","page":"Multivariate Normal Mixture","title":"Multivariate Normal Mixture","text":"global pe = plot(xlim = (-1.5L, 1.5L), ylim = (-1.5L, 1.5L))\n\nrp = scatter(y |> sdim(1), y |> sdim(2), xlim = (-1.5L, 1.5L), ylim = (-1.5L, 1.5L), legend=false, title=\"Generated\", ms = 2)\n\ne_means = mean.(m[end])\ne_precs = mean.(w[end])\n\nfor (e_m, e_w) in zip(e_means, e_precs)\n    global pe\n    gaussian = MvNormal(e_m, Matrix(Hermitian(inv(e_w))))\n    pe = contour!(pe, range(-2L, 2L, step = 0.25), range(-2L, 2L, step = 0.25), (x, y) -> pdf(gaussian, [ x, y ]), title=\"Inference result\", legend=false, levels = 7, colorbar = false)\nend\n\npfe = plot(fe[2:end], label = \"Free Energy\")\n\nplot(rp, pe, pfe, layout = @layout([ a b; c ]))","category":"page"},{"location":"examples/multivariate_normal_mixture/#Benchmark","page":"Multivariate Normal Mixture","title":"Benchmark","text":"","category":"section"},{"location":"examples/multivariate_normal_mixture/","page":"Multivariate Normal Mixture","title":"Multivariate Normal Mixture","text":"@benchmark inference(\n    model = $(Model(gaussian_mixture_model, rng, nmixtures, length(y), priors_mean, priors_cov)), \n    data = $((y = y,)), \n    initmarginals = $initmarginals, \n    iterations = 15, \n    free_energy = true\n)","category":"page"},{"location":"man/inference-execution/#user-guide-inference-execution","page":"Inference execution","title":"Inference execution","text":"","category":"section"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"This section explains how to use ReactiveMP reactive API for running inference on probabilistic models that were created with GraphPPL package as explained in Model Specification section.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"The ReactiveMP inference API supports different types of message-passing algorithms (including hybrid algorithms combining several different types):","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"Belief Propagation\nVariational Message Passing","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"Whereas belief propagation computes exact inference for the random variables of interest, the variational message passing (VMP) in an approximation method that can be applied to a larger range of models.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"The ReactiveMP engine itself isn't aware of different algorithm types and simply does message passing between nodes, however during model specification stage user may specifiy different factorisation constraints around factor nodes by using where { q = ... } syntax or with the help of the @constraints macro. Different factorisation constraints lead to a different message passing update rules.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"Inference with ReactiveMP usually consists of the same simple building blocks and designed in such a way to support both static and real-time infinite datasets:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"Create a model with @model macro and get a references to random variables and data inputs\nSubscribe to random variable posterior marginal updates \nSubscribe to Bethe Free Energy updates (optional)\nFeed model with observations \nUnsubscribe from posterior marginal updates (optional)","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"It is worth to note that Step 5 is optional and in case where observations come from an infinite real-time data stream (e.g. from the internet) it may be justified to never unsubscribe and perform real-time Bayesian inference in a reactive manner as soon as data arrives.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"ReactiveMP.jl provides generic inference function to simplify these steps and test models faster. However, this function does not suppor the full range of ReactiveMP.jl's package  feature. Read about both automatic and manual approaches below.","category":"page"},{"location":"man/inference-execution/#user-guide-inference-execution-automatic-specification","page":"Inference execution","title":"Automatic inference specification","text":"","category":"section"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"ReactiveMP.jl exports user friendly inference function to quickly run and test you model with static datasets. Note, however, that this function does not use all capabilities of ReactiveMP.jl library and for advanced use cases you may want to resort to the manual inference specification section and Advanced Tutorial section.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"inference\nInferenceResult","category":"page"},{"location":"man/inference-execution/#ReactiveMP.inference","page":"Inference execution","title":"ReactiveMP.inference","text":"inference(\n    model::ModelGenerator; \n    data,\n    initmarginals = nothing,\n    initmessages  = nothing,\n    constraints   = nothing,\n    meta          = nothing,\n    options       = (;),\n    returnvars    = nothing, \n    iterations    = 1,\n    free_energy   = false,\n    showprogress  = false,\n    callbacks     = nothing,\n)\n\nThis function provides a generic (but somewhat limited) way to perform probabilistic inference in ReactiveMP.jl. Returns InferenceResult.\n\nArguments\n\nFor more information about some of the arguments, please check below.\n\nmodel::ModelGenerator: specifies a model generator, with the help of the Model function\ndata: NamedTuple or Dict with data, required\ninitmarginals = nothing: NamedTuple or Dict with initial marginals, optional, defaults to nothing\ninitmessages = nothing: NamedTuple or Dict with initial messages, optional, defaults to nothing\nconstraints = nothing: constraints specification object, optional, see @constraints\nmeta  = nothing: meta specification object, optional, may be required for some models, see @meta\noptions = (;): model creation options, optional, see model_options\nreturnvars = nothing: return structure info, optional, defaults to return everything at each iteration, see below for more information\niterations = 1: number of iterations, optional, defaults to 1, we do not distinguish between variational message passing or Loopy belief propagation or expectation propagation iterations\nfree_energy = false: compute the Bethe free energy, optional, defaults to false. Can be passed a floating point type, e.g. Float64, for better efficiency, but disables automatic differentiation packages, such as ForwardDiff.jl\nshowprogress = false: show progress module, optional, defaults to false\ncallbacks = nothing: inference cycle callbacks, optional, see below for more info\nwarn = true: enables/disables warnings\n\nExtended information about some of the arguments\n\nmodel\n\nThe model argument accepts a ModelGenerator as its input. The easiest way to create the ModelGenerator is to use the Model function. The Model function accepts a model name as its first argument and the rest is passed directly to the model constructor. For example:\n\nresult = inference(\n    # Creates `coin_toss(some_argument, some_keyword_argument = 3)`\n    model = Model(coin_toss, some_argument; some_keyword_argument = 3)\n)\n\nNote: The model keyword argument does not accept a FactorGraphModel instance as a value, as it needs to inject constraints and meta during the inference procedure.\n\ninitmarginals\n\nIn general for variational message passing every marginal distribution in a model needs to be pre-initialised. In practice, however, for many models it is sufficient enough to initialise only a small subset of variables in the model.\n\ninitmessages\n\nLoopy belief propagation may need some messages in a model to be pre-initialised.\n\noptions\n\nSee ?model_options.\n\nreturnvars\n\nreturnvars specifies the variables of interests and the amount of information to return about their posterior updates. By default the inference function tracks and returns every update for each iteration for every random variable in the model. returnvars accepts a NamedTuple or Dict or return var specification. There are two specifications:\n\nKeepLast: saves the last update for a variable, ignoring any intermediate results during iterations\nKeepEach: saves all updates for a variable for all iterations\n\nExample: \n\nresult = inference(\n    ...,\n    returnvars = (\n        x = KeepLast(),\n        τ = KeepEach()\n    )\n)\n\nfree_energy\n\nThis setting specifies whenever the inference function should return Bethe Free Energy (BFE) values.  Note, however, that it may be not possible to compute BFE values for every model. \n\nAdditionally, the argument may accept a floating point type, instead of a Bool value. Using his option, e.g.Float64, improves performance of Bethe Free Energy computation, but restricts using automatic differentiation packages.\n\ncallbacks\n\nThe inference function has its own lifecycle. The user is free to provide some (or none) of the callbacks to inject some extra logging or other procedures in the inference function, e.g.\n\nresult = inference(\n    ...,\n    callbacks = (\n        on_marginal_update = (model, name, update) -> println(\"$(name) has been updated: $(update)\"),\n        after_inference    = (args...) -> println(\"Inference has been completed\")\n    )\n)\n\nThe list of all possible callbacks is present below:\n\n:on_marginal_update:    args: (model::FactorGraphModel, name::Symbol, update)\n:before_model_creation: args: ()\n:after_model_creation:  args: (model::FactorGraphModel)\n:before_inference:      args: (model::FactorGraphModel)\n:before_iteration:      args: (model::FactorGraphModel, iteration::Int)\n:before_data_update:    args: (model::FactorGraphModel, data)\n:after_data_update:     args: (model::FactorGraphModel, data)\n:after_iteration:       args: (model::FactorGraphModel, iteration::Int)\n:after_inference:       args: (model::FactorGraphModel)\n\nSee also: InferenceResult\n\n\n\n\n\n","category":"function"},{"location":"man/inference-execution/#ReactiveMP.InferenceResult","page":"Inference execution","title":"ReactiveMP.InferenceResult","text":"InferenceResult\n\nThis structure is used as a return value from the inference function. \n\nFields\n\nposteriors: Dict or NamedTuple of 'random variable' - 'posterior' pairs. See the returnvars argument for inference.\nfree_energy: (optional) An array of Bethe Free Energy values per VMP iteration. See the free_energy argument for inference.\nmodel: FactorGraphModel object reference.\nreturnval: Return value from executed @model.\n\nSee also: inference\n\n\n\n\n\n","category":"type"},{"location":"man/inference-execution/#user-guide-inference-execution-manual-specification","page":"Inference execution","title":"Manual inference specification","text":"","category":"section"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"For advanced use cases such as online real-time Bayesian inference it is advised to use manual inference specification. ","category":"page"},{"location":"man/inference-execution/#user-guide-inference-execution-model-creation","page":"Inference execution","title":"Model creation","text":"","category":"section"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"During model specification stage user decides on variables of interesets in a model and returns (optionally) them using a return ... statement. As an example consider that we have a simple hierarchical model in which the mean of a Normal distribution is represented by another Normal distribution whose mean is modelled by another Normal distribution.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random\n\n@model function my_model()\n    m2 ~ NormalMeanVariance(0.0, 1.0)\n    m1 ~ NormalMeanVariance(m2, 1.0)\n\n    y = datavar(Float64)\n    y ~ NormalMeanVariance(m1, 1.0)\n\n    # Return variables of interests, optional\n    return m1, y\nend","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"And later on we may create our model and obtain references for variables of interests:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"model, (m1, y) = my_model()\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"Alternatively, it is possible to query variables using squared brackets on model object:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"model, _ = my_model()\n\nmodel[:m1] # m1\nmodel[:y]  # y\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"@model macro also return a reference for a factor graph as its first return value. Factor graph object (named model in previous example) contains all information about all factor nodes in a model as well as random variables and data inputs. See Advanced Tutorial section.","category":"page"},{"location":"man/inference-execution/#user-guide-inference-execution-marginal-updates","page":"Inference execution","title":"Posterior marginal updates","text":"","category":"section"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"The ReactiveMP package has a reactive API and operates in terms of Observables and Actors. For detailed information about these concepts we refer to Rocket.jl documentation.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"We use getmarginal function to get a posterior marginal updates observable:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"m1_posterior_updates = getmarginal(m1)\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"After that we can subscribe on new updates and perform some actions based on new values:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"m1_posterior_subscription = subscribe!(m1_posterior_updates, (new_posterior) -> begin\n    println(\"New posterior for m1: \", new_posterior)\nend)\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"Sometimes it is usefull to return an array of random variables from model specification, in this case we may use getmarginals() function that transform an array of observables to an observable of arrays.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"@model function my_model()\n    ...\n    m_n = randomvar(n)\n    ...\n    return m_n, ...\nend\n\nmodel, (m_n, ...) = my_model()\n\nm_n_updates = getmarginals(m_n)","category":"page"},{"location":"man/inference-execution/#user-guide-inference-execution-observations","page":"Inference execution","title":"Feeding observations","text":"","category":"section"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"By default (without any extra factorisation constraints) model specification implies Belief Propagation message passing update rules. In case of BP algorithm ReactiveMP package computes an exact Bayesian posteriors with a single message passing iteration. To enforce Belief Propagation message passing update rule for some specific factor node user may use where { q = FullFactorisation() } option. Read more in Model Specification section. To perform a message passing iteration we need to pass some data to all our data inputs that were created with datavar function during model specification.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"To feed an observation for a specific data input we use update! function:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"update!(y, 0.0)\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"As you can see after we passed a single value to our data input we got a posterior marginal update from our subscription and printed it with println function. In case of BP if observations do not change it should not affect posterior marginal results:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"update!(y, 0.0) # Observation didn't change, should result in the same posterior\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"If y is an array of data inputs it is possible to pass an array of observation to update! function:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"for i in 1:length(data)\n    update!(y[i], data[i])\nend\n# is an equivalent of\nupdate!(y, data)","category":"page"},{"location":"man/inference-execution/#user-guide-inference-vmp","page":"Inference execution","title":"Variational Message Passing","text":"","category":"section"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"Variational message passing (VMP) algorithms are generated much in the same way as the belief propagation algorithm we saw in the previous section. There is a major difference though: for VMP algorithm generation we need to define the factorization properties of our approximate distribution. A common approach is to assume that all random variables of the model factorize with respect to each other. This is known as the mean field assumption. In ReactiveMP, the specification of such factorization properties is defined during model specification stage using the where { q = ... } syntax or with the @constraints macro (see Constraints specification section for more info about the @constraints macro). Let's take a look at a simple example to see how it is used. In this model we want to learn the mean and precision of a Normal distribution, where the former is modelled with a Normal distribution and the latter with a Gamma.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"real_mean      = -4.0\nreal_precision = 0.2\nrng            = MersenneTwister(1234)\n\nn    = 100\ndata = rand(rng, Normal(real_mean, sqrt(inv(real_precision))), n)\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"@model function normal_estimation(n)\n    m ~ NormalMeanVariance(0.0, 10.0)\n    w ~ Gamma(0.1, 10.0)\n\n    y = datavar(Float64, n)\n\n    for i in 1:n\n        y[i] ~ NormalMeanPrecision(m, w) where { q = MeanField() }\n    end\n\n    return m, w, y\nend","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"We create our model as usual, however in order to start VMP inference procedure we need to set initial posterior marginals for all random variables in the model:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"model, (m, w, y) = normal_estimation(n)\n\n# We use vague initial marginals\nsetmarginal!(m, vague(NormalMeanVariance)) \nsetmarginal!(w, vague(Gamma))\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"To perform a single VMP iteration it is enough to feed all data inputs with some values. To perform multiple VMP iterations we should feed our all data inputs with the same values multiple times:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"m_marginals = []\nw_marginals = []\n\nsubscriptions = subscribe!([\n    (getmarginal(m), (marginal) -> push!(m_marginals, marginal)),\n    (getmarginal(w), (marginal) -> push!(w_marginals, marginal)),\n])\n\nvmp_iterations = 10\n\nfor _ in 1:vmp_iterations\n    update!(y, data)\nend\n\nunsubscribe!(subscriptions)\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"As we process more VMP iterations, our beliefs about the possible values of m and w converge and become more confident.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"using Plots\n\np1    = plot(title = \"'Mean' posterior marginals\")\ngrid1 = -6.0:0.01:4.0\n\nfor iter in [ 1, 2, 10 ]\n\n    estimated = Normal(mean(m_marginals[iter]), std(m_marginals[iter]))\n    e_pdf     = (x) -> pdf(estimated, x)\n\n    plot!(p1, grid1, e_pdf, fill = true, opacity = 0.3, label = \"Estimated mean after $iter VMP iterations\")\n\nend\n\nplot!(p1, [ real_mean ], seriestype = :vline, label = \"Real mean\", color = :red4, opacity = 0.7)","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"p2    = plot(title = \"'Precision' posterior marginals\")\ngrid2 = 0.01:0.001:0.35\n\nfor iter in [ 2, 3, 10 ]\n\n    estimated = Gamma(shape(w_marginals[iter]), scale(w_marginals[iter]))\n    e_pdf     = (x) -> pdf(estimated, x)\n\n    plot!(p2, grid2, e_pdf, fill = true, opacity = 0.3, label = \"Estimated precision after $iter VMP iterations\")\n\nend\n\nplot!(p2, [ real_precision ], seriestype = :vline, label = \"Real precision\", color = :red4, opacity = 0.7)","category":"page"},{"location":"man/inference-execution/#user-guide-inference-vmp-bfe","page":"Inference execution","title":"Computing Bethe Free Energy","text":"","category":"section"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"VMP inference boils down to finding the member of a family of tractable probability distributions that is closest in KL divergence to an intractable posterior distribution. This is achieved by minimizing a quantity known as Variational Free Energy. ReactiveMP uses Bethe Free Energy approximation to the real Variational Free Energy. Free energy is particularly useful to test for convergence of the VMP iterative procedure.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"The ReactiveMP package exports score function for an observable of free energy values:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"fe_observable = score(BetheFreeEnergy(), model)\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"# Reset posterior marginals for `m` and `w`\nsetmarginal!(m, vague(NormalMeanVariance))\nsetmarginal!(w, vague(Gamma))\n\nfe_values = []\n\nfe_subscription = subscribe!(fe_observable, (v) -> push!(fe_values, v))\n\nvmp_iterations = 10\n\nfor _ in 1:vmp_iterations\n    update!(y, data)\nend\n\nunsubscribe!(fe_subscription)","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"plot(fe_values, label = \"Bethe Free Energy\", xlabel = \"Iteration #\")","category":"page"},{"location":"custom/custom-functional-form/#custom-functional-form","page":"Custom functional form","title":"Custom Functional Form Specification","text":"","category":"section"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"In a nutshell, functional form constraints defines a function that approximates the product of colliding messages and computes posterior marginal that can be used later on during the inference procedure. An important part of the functional forms constraint implementation is the prod function. More information about prod function is present in the Prod Implementation section. For example, if we refer to our CustomFunctionalForm as to f we can see the whole functional form constraints pipeline as:","category":"page"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"q(x) = fleft(fracoverrightarrowmu(x)overleftarrowmu(x)int overrightarrowmu(x)overleftarrowmu(x) mathrmdxright)","category":"page"},{"location":"custom/custom-functional-form/#Interface","page":"Custom functional form","title":"Interface","text":"","category":"section"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"ReactiveMP.jl, however, uses some extra utility functions to define functional form constraint behaviour. Here we briefly describe all utility function. If you are only interested in the concrete example, you may directly head to the Custom Functional Form example at the end of this section.","category":"page"},{"location":"custom/custom-functional-form/#Abstract-super-type","page":"Custom functional form","title":"Abstract super type","text":"","category":"section"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"AbstractFormConstraint","category":"page"},{"location":"custom/custom-functional-form/#ReactiveMP.AbstractFormConstraint","page":"Custom functional form","title":"ReactiveMP.AbstractFormConstraint","text":"AbstractFormConstraint\n\nEvery functional form constraint is a subtype of AbstractFormConstraint abstract type.\n\nNote: this is not strictly necessary, but it makes automatic dispatch easier and compatible with the CompositeFormConstraint.\n\nSee also: CompositeFormConstraint\n\n\n\n\n\n","category":"type"},{"location":"custom/custom-functional-form/#Form-check-strategy","page":"Custom functional form","title":"Form check strategy","text":"","category":"section"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"Every custom functional form must implement a new method for the default_form_check_strategy function that returns either FormConstraintCheckEach or FormConstraintCheckLast.","category":"page"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"FormConstraintCheckLast: q(x) = f(μ1(x) * μ2(x) * μ3(x))\nFormConstraintCheckEach: q(x) = f(f(μ1(x) * μ2(x)) * μ3(x))","category":"page"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"default_form_check_strategy\nFormConstraintCheckEach\nFormConstraintCheckLast","category":"page"},{"location":"custom/custom-functional-form/#ReactiveMP.default_form_check_strategy","page":"Custom functional form","title":"ReactiveMP.default_form_check_strategy","text":"default_form_check_strategy(form_constraint)\n\nReturns a default check strategy (e.g. FormConstraintCheckEach or FormConstraintCheckEach) for a given form constraint object.\n\nSee also: FormConstraintCheckEach, FormConstraintCheckLast, constrain_form\n\n\n\n\n\n","category":"function"},{"location":"custom/custom-functional-form/#ReactiveMP.FormConstraintCheckEach","page":"Custom functional form","title":"ReactiveMP.FormConstraintCheckEach","text":"FormConstraintCheckEach\n\nThis form constraint check strategy checks functional form of the messages product after each product in an equality chain.  Usually if a variable has been connected to multiple nodes we want to perform multiple prod to obtain a posterior marginal. With this form check strategy constrain_form function will be executed after each subsequent prod function.\n\nSee also: FormConstraintCheckLast, default_form_check_strategy, constrain_form, multiply_messages\n\n\n\n\n\n","category":"type"},{"location":"custom/custom-functional-form/#ReactiveMP.FormConstraintCheckLast","page":"Custom functional form","title":"ReactiveMP.FormConstraintCheckLast","text":"FormConstraintCheckEach\n\nThis form constraint check strategy checks functional form of the last messages product in the equality chain.  Usually if a variable has been connected to multiple nodes we want to perform multiple prod to obtain a posterior marginal. With this form check strategy constrain_form function will be executed only once after all subsequenct prod functions have been executed.\n\nSee also: FormConstraintCheckLast, default_form_check_strategy, constrain_form, multiply_messages\n\n\n\n\n\n","category":"type"},{"location":"custom/custom-functional-form/#Prod-constraint","page":"Custom functional form","title":"Prod constraint","text":"","category":"section"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"Every custom functional form must implement a new method for the default_prod_constraint function that returns a proper prod_constraint object.","category":"page"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"default_prod_constraint","category":"page"},{"location":"custom/custom-functional-form/#ReactiveMP.default_prod_constraint","page":"Custom functional form","title":"ReactiveMP.default_prod_constraint","text":"default_prod_constraint(form_constraint)\n\nReturns a default prod constraint needed to apply a given form_constraint. For most form constraints this function returns ProdGeneric.\n\nSee also: ProdAnalytical, ProdGeneric\n\n\n\n\n\n","category":"function"},{"location":"custom/custom-functional-form/#Constrain-form,-a.k.a-f","page":"Custom functional form","title":"Constrain form, a.k.a f","text":"","category":"section"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"The main function that a custom functional form must implement, which we referred to as f in the beginning in this section, is the constrain_form function.","category":"page"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"constrain_form","category":"page"},{"location":"custom/custom-functional-form/#ReactiveMP.constrain_form","page":"Custom functional form","title":"ReactiveMP.constrain_form","text":"constrain_form(form_constraint, distribution)\n\nThis function must approximate distribution object in a form that satisfies form_constraint.\n\nSee also: FormConstraintCheckEach, FormConstraintCheckLast, default_form_check_strategy, is_point_mass_form_constraint\n\n\n\n\n\n","category":"function"},{"location":"custom/custom-functional-form/#Is-point-mass-form-constraint-(optional)","page":"Custom functional form","title":"Is point mass form constraint (optional)","text":"","category":"section"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"Every custom functional form may implement a new method for the is_point_mass_form_constraint function that returns either true or false. This is an utility function that simplifes computation of the Bethe Free Energy and is not strictly necessary.","category":"page"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"is_point_mass_form_constraint","category":"page"},{"location":"custom/custom-functional-form/#ReactiveMP.is_point_mass_form_constraint","page":"Custom functional form","title":"ReactiveMP.is_point_mass_form_constraint","text":"is_point_mass_form_constraint(form_constraint)\n\nSpecifies whether form constraint always returns PointMass estimates or not. For a given form_constraint returns either true or false.\n\nSee also: FormConstraintCheckEach, FormConstraintCheckLast, constrain_form\n\n\n\n\n\n","category":"function"},{"location":"custom/custom-functional-form/#Compatibility-with-@constraints-macro-(optional)","page":"Custom functional form","title":"Compatibility with @constraints macro (optional)","text":"","category":"section"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"To make custom functional form constraint compatible with the @constraints macro, it must implement a new method for the make_form_constraint function.","category":"page"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"make_form_constraint","category":"page"},{"location":"custom/custom-functional-form/#ReactiveMP.make_form_constraint","page":"Custom functional form","title":"ReactiveMP.make_form_constraint","text":"make_form_constraint(::Type, args...; kwargs...)\n\nCreates form constraint object based on passed type with given args and kwargs. Used to simplify form constraint specification.\n\nAs an example:\n\nmake_form_constraint(PointMass)\n\ncreates an instance of PointMassFormConstraint and \n\nmake_form_constraint(SampleList, 5000, LeftProposal())\n\nshould create an instance of SampleListFormConstraint.\n\nSee also: AbstractFormConstraint\n\n\n\n\n\n","category":"function"},{"location":"custom/custom-functional-form/#custom-functional-form-example","page":"Custom functional form","title":"Custom Functional Form Example","text":"","category":"section"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"In this demo we show how to build a custom functional form constraint that is compatible with the ReactiveMP.jl inference backend. An important part of the functional forms constraint implementation is the prod function. More information about prod function is present in the Prod Implementation section. We show a relatively simple use-case, which might not be very useful in practice, but serves as a simple step-by-step guide. Assume that we want a specific posterior marginal of some random variable in our model to have a specific Gaussian parametrisation, for example mean-precision. We can use built-in NormalMeanPrecision distribution, but we still need to define our custom functional form constraint:","category":"page"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"using ReactiveMP, GraphPPL\n\n# First we define our functional form structure with no fields\nstruct MeanPrecisionFormConstraint <: AbstractFormConstraint end","category":"page"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"Next we define the behaviour of our functional form constraint:","category":"page"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"ReactiveMP.is_point_mass_form_constraint(::MeanPrecisionFormConstraint) = false\nReactiveMP.default_form_check_strategy(::MeanPrecisionFormConstraint)   = FormConstraintCheckLast()\nReactiveMP.default_prod_constraint(::MeanPrecisionFormConstraint)       = ProdGeneric()\n\nfunction ReactiveMP.constrain_form(::MeanPrecisionFormConstraint, distribution) \n    # This is quite a naive assumption, that a given `dsitribution` object has `mean` and `precision` defined\n    # However this quantities might be approximated with some other external method, e.g. Laplace approximation\n    m = mean(distribution)      # or approximate with some other method\n    p = precision(distribution) # or approximate with some other method\n    return NormalMeanPrecision(m, p)\nend\n\nfunction ReactiveMP.constrain_form(::MeanPrecisionFormConstraint, distribution::DistProduct)\n    # DistProduct is the special case, read about this type more in the corresponding documentation section\n    # ... \nend","category":"page"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"At this point we already can use our functional form constraint in the inference backend, however, lets also make our functional form constraint compatible with the @constraints macro from GraphPPL.jl package.","category":"page"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"ReactiveMP.make_form_constraint(::Type{ NormalMeanPrecision }, args...; kwargs...) = MeanPrecisionFormConstraint()","category":"page"},{"location":"custom/custom-functional-form/","page":"Custom functional form","title":"Custom functional form","text":"@constraints begin \n    q(x) :: NormalMeanPrecision\nend","category":"page"},{"location":"examples/gamma_mixture/#examples-gamma-mixture","page":"Gamma Mixture","title":"Example: Gamma Mixture","text":"","category":"section"},{"location":"examples/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"This example implements one of the experiments outlined in https://biaslab.github.io/publication/mp-based-inference-in-gmm/.","category":"page"},{"location":"examples/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"To model this process in ReactiveMP, first, we start with importing all needed packages:","category":"page"},{"location":"examples/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"using Rocket, ReactiveMP, GraphPPL\nusing Distributions, Random, StableRNGs\nusing StatsPlots","category":"page"},{"location":"examples/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"# create custom structure for model parameters for simplicity\nstruct GammaMixtureModelParameters\n    nmixtures   # number of mixtures\n    priors_as   # tuple of priors for variable a\n    priors_bs   # tuple of priors for variable b\n    prior_s     # prior of variable s\nend","category":"page"},{"location":"examples/gamma_mixture/#Model-specification","page":"Gamma Mixture","title":"Model specification","text":"","category":"section"},{"location":"examples/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"@model function gamma_mixture_model(nobservations, parameters::GammaMixtureModelParameters)\n\n    # fetch information from struct\n    nmixtures = parameters.nmixtures\n    priors_as = parameters.priors_as\n    priors_bs = parameters.priors_bs\n    prior_s   = parameters.prior_s\n\n    # set prior on global selection variable\n    s ~ Dirichlet(probvec(prior_s))\n\n    # allocate vectors of random variables\n    as = randomvar(nmixtures)\n    bs = randomvar(nmixtures)\n\n    # set priors on variables of mixtures\n    for i in 1:nmixtures\n        as[i] ~ GammaShapeRate(shape(priors_as[i]), rate(priors_as[i]))\n        bs[i] ~ GammaShapeRate(shape(priors_bs[i]), rate(priors_bs[i]))\n    end\n\n    # introduce random variables for local selection variables and data\n    z = randomvar(nobservations)\n    y = datavar(Float64, nobservations)\n\n    # convert vector to tuples for proper functioning of GammaMixture node\n    tas = tuple(as...)\n    tbs = tuple(bs...)\n\n    # specify local selection variable and data generating process\n    for i in 1:nobservations\n        z[i] ~ Categorical(s)\n        y[i] ~ GammaMixture(z[i], tas, tbs)\n    end\n\n    # return random variables\n    return s, as, bs, z, y\n    \nend","category":"page"},{"location":"examples/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"constraints = @constraints begin \n    q(as) :: PointMass(starting_point = (args...) -> [ 1.0 ])\nend","category":"page"},{"location":"examples/gamma_mixture/#Generate-test-dataset-for-verification","page":"Gamma Mixture","title":"Generate test dataset for verification","text":"","category":"section"},{"location":"examples/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"# specify seed and number of data points\nrng = StableRNG(43)\nn_samples = 2500\n\n# specify parameters of mixture model that generates the data\n# Note that mixture components have exactly the same means\nmixtures  = [ Gamma(9.0, inv(27.0)), Gamma(90.0, inv(270.0)) ]\nnmixtures = length(mixtures)\nmixing    = rand(rng, nmixtures)\nmixing    = mixing ./ sum(mixing)\nmixture   = MixtureModel(mixtures, mixing)\n\n# generate data set\ndataset = rand(rng, mixture, n_samples)\nnothing #hide","category":"page"},{"location":"examples/gamma_mixture/#Inference","page":"Gamma Mixture","title":"Inference","text":"","category":"section"},{"location":"examples/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"# specify priors of probabilistic model\n# NOTE: As the means of the mixtures \"collide\", we specify informative prior for selector variable\nnmixtures = 2\ngpriors = GammaMixtureModelParameters(\n    nmixtures,                                                    # number of mixtures\n    [ Gamma(1.0, 0.1), Gamma(1.0, 1.0) ],                         # priors on variables a\n    [ GammaShapeRate(10.0, 2.0), GammaShapeRate(1.0, 3.0) ],      # priors on variables b\n    Dirichlet(1e3*mixing)                                         # prior on variable s\n)\n\ngmodel         = Model(gamma_mixture_model, length(dataset), gpriors)\ngdata          = (y = dataset, )\nginitmarginals = (s = gpriors.prior_s, z = vague(Categorical, gpriors.nmixtures), bs = GammaShapeRate(1.0, 1.0))\ngreturnvars    = (s = KeepLast(), z = KeepLast(), as = KeepEach(), bs = KeepEach())\n\ngoptions = (\n    limit_stack_depth     = 100, \n    default_factorisation = MeanField() # Mixture models require Mean-Field assumption currently\n)\n\ngresult = inference(\n    model       = gmodel, \n    data        = gdata,\n    constraints = constraints,\n    options     = goptions,\n    initmarginals = ginitmarginals,\n    returnvars    = greturnvars,\n    free_energy   = true,\n    iterations    = 250, \n    showprogress  = true\n)","category":"page"},{"location":"examples/gamma_mixture/#Verification","page":"Gamma Mixture","title":"Verification","text":"","category":"section"},{"location":"examples/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"# extract inferred parameters\n_as, _bs = mean.(gresult.posteriors[:as][end]), mean.(gresult.posteriors[:bs][end])\n_dists   = map(g -> Gamma(g[1], inv(g[2])), zip(_as, _bs))\n_mixing = mean(gresult.posteriors[:s])\n\n# create model from inferred parameters\n_mixture   = MixtureModel(_dists, _mixing);\n\n# report on outcome of inference\nprintln(\"Generated means: $(mean(mixtures[1])) and $(mean(mixtures[2]))\")\nprintln(\"Inferred means: $(mean(_dists[1])) and $(mean(_dists[2]))\")\nprintln(\"========\")\nprintln(\"Generated mixing: $(mixing)\")\nprintln(\"Inferred mixing: $(_mixing)\")\nnothing #hide","category":"page"},{"location":"examples/gamma_mixture/#Results","page":"Gamma Mixture","title":"Results","text":"","category":"section"},{"location":"examples/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"# plot results\np1 = histogram(dataset, ylim = (0, 13), xlim = (0, 1), normalize=:pdf, label=\"observations\")\np1 = plot!(mixture, label=false, title=\"Generated mixtures\")\n\np2 = histogram(dataset, ylim = (0, 13), xlim = (0, 1), normalize=:pdf, label=\"data\", opacity=0.3)\np2 = plot!(_mixture, label=false, title=\"Inferred mixtures\", linewidth=3.0)\n\n# evaluate the convergence of the algorithm by monitoring the BFE\np3 = plot(gresult.free_energy, label=false, xlabel=\"iterations\", title=\"Bethe FE\")\n\nplot(p1, p2, layout = @layout([ a; b ]))","category":"page"},{"location":"examples/gamma_mixture/","page":"Gamma Mixture","title":"Gamma Mixture","text":"plot(p3)","category":"page"},{"location":"#ReactiveMP.jl","page":"Introduction","title":"ReactiveMP.jl","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Julia package for automatic Bayesian inference on a factor graph with reactive message passing.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Given a probabilistic model, ReactiveMP allows for an efficient message-passing based Bayesian inference. It uses the model structure to generate an algorithm that consists of a sequence of local computations on a Forney-style factor graph (FFG) representation of the model.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"ReactiveMP.jl has been designed with a focus on efficiency, scalability and maximum performance for running inference with message passing. It is worth noting that this package is aimed to run Bayesian inference for conjugate state-space models. For these types of models, ReactiveMP.jl takes advantage of the conjugate pairings and beats general-purpose probabilistic programming packages easily in terms of computational load, speed, memory  and accuracy. On the other hand, sampling-based packages like Turing.jl are generic Bayesian inference solutions and are capable of running inference for a broader set of models. ","category":"page"},{"location":"#Package-Features","page":"Introduction","title":"Package Features","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"User friendly syntax for specification of probabilistic models.\nAutomatic generation of message passing algorithms including\nBelief propagation\nVariational message passing\nExpectation maximization\nSupport for hybrid models combining discrete and continuous latent variables.\nSupport for hybrid distinct message passing inference algorithm under a unified paradigm.\nEvaluation of Bethe free energy as a model performance measure.\nSchedule-free reactive message passing API.\nHigh performance.\nScalability for large models with millions of parameters and observations.\nInference procedure is differentiable.\nEasy to extend with custom nodes.","category":"page"},{"location":"#Resources","page":"Introduction","title":"Resources","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"For an introduction to message passing and FFGs, see The Factor Graph Approach to Model-Based Signal Processing by Loeliger et al. (2007).","category":"page"},{"location":"#How-to-get-started?","page":"Introduction","title":"How to get started?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Head to the Getting started section to get up and running with ForneyLab. Alternatively, explore various examples in the documentation. For advanced extensive tutorial take a look on Advanced Tutorial.","category":"page"},{"location":"#Table-of-Contents","page":"Introduction","title":"Table of Contents","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Pages = [\n  \"man/getting-started.md\",\n  \"man/advanced-tutorial.md\",\n  \"man/model-specification.md\",\n  \"man/constraints-specification.md\",\n  \"man/meta-specification.md\",\n  \"examples/overview.md\",\n  \"lib/message.md\",\n  \"lib/node.md\",\n  \"lib/math.md\",\n  \"extra/contributing.md\"\n]\nDepth = 2","category":"page"},{"location":"#Index","page":"Introduction","title":"Index","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"lib/helpers/#lib-helpers","page":"Helper utils","title":"Helper utilities","text":"","category":"section"},{"location":"lib/helpers/","page":"Helper utils","title":"Helper utils","text":"ReactiveMP implements various structures/function/method as \"helper\" structures that might be usefull in various contexts.","category":"page"},{"location":"lib/helpers/#lib-helpers-skip-index-iterator","page":"Helper utils","title":"SkipIndexIterator","text":"","category":"section"},{"location":"lib/helpers/","page":"Helper utils","title":"Helper utils","text":"ReactiveMP.SkipIndexIterator\nReactiveMP.skipindex","category":"page"},{"location":"lib/helpers/#ReactiveMP.SkipIndexIterator","page":"Helper utils","title":"ReactiveMP.SkipIndexIterator","text":"SkipIndexIterator\n\nA special type of iterator that simply iterates over internal iterator, but skips index skip.\n\nArguments\n\niterator: internal iterator\nskip: index to skip (integer)\n\nSee also: skipindex\n\n\n\n\n\n","category":"type"},{"location":"lib/helpers/#ReactiveMP.skipindex","page":"Helper utils","title":"ReactiveMP.skipindex","text":"skipindex(iterator, skip)\n\nCreation operator for SkipIndexIterator.\n\njulia> s = ReactiveMP.skipindex(1:3, 2)\n2-element ReactiveMP.SkipIndexIterator{Int64, UnitRange{Int64}}:\n 1\n 3\n\njulia> collect(s)\n2-element Vector{Int64}:\n 1\n 3\n\nSee also: SkipIndexIterator\n\n\n\n\n\n","category":"function"},{"location":"lib/helpers/#lib-helpers-inf-counting-real","page":"Helper utils","title":"InfCountingReal","text":"","category":"section"},{"location":"lib/helpers/","page":"Helper utils","title":"Helper utils","text":"ReactiveMP.InfCountingReal\nReactiveMP.∞","category":"page"},{"location":"lib/helpers/#ReactiveMP.InfCountingReal","page":"Helper utils","title":"ReactiveMP.InfCountingReal","text":"InfCountingReal\n\nInfCountingReal implements a \"number\" that counts infinities in a separate field. Used to cancel out infinities in BFE computations. Infinity implemented as InfCountingReal(zero(Float64), 1).\n\nArguments\n\nvalue::T: value of type <: Real\ninfs::Int: number of added/subtracted infinities\n\njulia> r = ReactiveMP.InfCountingReal(0.0, 0)\nInfCountingReal(0.0, 0∞)\n\njulia> float(r)\n0.0\n\njulia> r = r + ReactiveMP.∞\nInfCountingReal(0.0, 1∞)\n\njulia> float(r)\nInf\n\njulia> r = r - ReactiveMP.∞\nInfCountingReal(0.0, 0∞)\n\njulia> float(r)\n0.0\n\nSee also: ∞\n\n\n\n\n\n","category":"type"},{"location":"lib/helpers/#ReactiveMP.∞","page":"Helper utils","title":"ReactiveMP.∞","text":"∞\n\nA singleton object that implements \"infinity\" that can be added or subtracted to/from InfCountingReal structure. Internally implemented as InfCountingReal(zero(Float64), 0).\n\nSee also: InfCountingReal\n\n\n\n\n\n","category":"constant"},{"location":"lib/helpers/#lib-helpers-deep-eltype","page":"Helper utils","title":"deep_eltype","text":"","category":"section"},{"location":"lib/helpers/","page":"Helper utils","title":"Helper utils","text":"ReactiveMP.deep_eltype","category":"page"},{"location":"lib/helpers/#ReactiveMP.deep_eltype","page":"Helper utils","title":"ReactiveMP.deep_eltype","text":"deep_eltype\n\nReturns the eltype of the first container in the nested hierarchy.\n\njulia> ReactiveMP.deep_eltype([ [1, 2], [2, 3] ])\nInt64\n\njulia> ReactiveMP.deep_eltype([[[ 1.0, 2.0 ], [ 3.0, 4.0 ]], [[ 5.0, 6.0 ], [ 7.0, 8.0 ]]])\nFloat64\n\n\n\n\n\n","category":"function"},{"location":"lib/helpers/#lib-helpers-functional-index","page":"Helper utils","title":"FunctionalIndex","text":"","category":"section"},{"location":"lib/helpers/","page":"Helper utils","title":"Helper utils","text":"ReactiveMP.FunctionalIndex","category":"page"},{"location":"lib/helpers/#ReactiveMP.FunctionalIndex","page":"Helper utils","title":"ReactiveMP.FunctionalIndex","text":"FunctionalIndex\n\nA special type of an index that represents a function that can be used only in pair with a collection.  An example of a FunctionalIndex can be firstindex or lastindex, but more complex use cases are possible too,  e.g. firstindex + 1. Important part of the implementation is that the resulting structure is isbitstype(...) = true, that allows to store it in parametric type as valtype.\n\nOne use case for this structure is to dispatch on and to replace begin or end (or more complex use cases, e.g. begin + 1) markers in constraints specification language.\n\n\n\n\n\n","category":"type"}]
}
