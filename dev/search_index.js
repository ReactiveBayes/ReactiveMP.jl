var documenterSearchIndex = {"docs":
[{"location":"lib/message/#lib-message","page":"Messages","title":"Messages implementation","text":"","category":"section"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"In message passing framework one of the most important concept is (wow!) messages. Messages flow on edges of a factor graph and usually hold some information in a form of probability distribution. In ReactiveMP.jl we distinguish two major types of messages: Belief Propagation and Variational.  ","category":"page"},{"location":"lib/message/#Abstract-message-type","page":"Messages","title":"Abstract message type","text":"","category":"section"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"Both belief propagation and variational messages are subtypes of a AbstractMessage supertype.","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"AbstractMessage","category":"page"},{"location":"lib/message/#ReactiveMP.AbstractMessage","page":"Messages","title":"ReactiveMP.AbstractMessage","text":"AbstractMessage\n\nAn abstract supertype for all concrete message types.\n\nSee also: Message\n\n\n\n\n\n","category":"type"},{"location":"lib/message/#lib-belief-propagation-message","page":"Messages","title":"Belief-Propagation (or Sum-Product) message","text":"","category":"section"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"Belief propagation messages are encoded with type Message. ","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"(Image: message) Belief propagation message","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"Message","category":"page"},{"location":"lib/message/#ReactiveMP.Message","page":"Messages","title":"ReactiveMP.Message","text":"Message{D} <: AbstractMessage\n\nMessage structure encodes a Belief Propagation message, which holds some data that usually a probability distribution, but can also be an arbitrary object. Message acts as a proxy structure to data object and proxies most of the statistical functions, e.g. mean, mode, cov etc.\n\nArguments\n\ndata::D: message always holds some data object associated with it\nis_clamped::Bool, specifies if this message is clamped\nis_initial::Bool, specifies if this message is initial\n\nExample\n\njulia> distribution = Gamma(10.0, 2.0)\nGamma{Float64}(α=10.0, θ=2.0)\n\njulia> message = Message(distribution, false, true)\nMessage(Gamma{Float64}(α=10.0, θ=2.0))\n\njulia> mean(message) \n20.0\n\njulia> getdata(message)\nGamma{Float64}(α=10.0, θ=2.0)\n\njulia> is_clamped(message)\nfalse\n\njulia> is_initial(message)\ntrue\n\n\nSee also: AbstractMessage, materialize!\n\n\n\n\n\n","category":"type"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"From implementation point a view Message structure does nothing but holds some data object and redirects most of the statistical related functions to that data object. However it used extensively in Julia's multiple dispatch. Implementation also uses extra is_initial and is_clamped fields to determine if product of two messages results in is_initial or is_clamped posterior marginal.","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"distribution = NormalMeanPrecision(0.0, 1.0)\nmessage      = Message(distribution, false, true)","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"mean(message), precision(message)","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"logpdf(message, 1.0)","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"is_clamped(message), is_initial(message)","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"User should not really interact with Message structure while working with ReactiveMP unless doing some advanced inference procedures that involves prediction.","category":"page"},{"location":"lib/message/#lib-variational-message","page":"Messages","title":"Variational message","text":"","category":"section"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"Variational messages are encoded with type VariationalMessage.","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"(Image: message) Variational message with structured factorisation q(x, y)q(z) assumption","category":"page"},{"location":"examples/linear_gaussian_state_space_model/#examples-linear-gaussian-state-space-model","page":"Linear Gaussian Dynamical System","title":"Example: Linear Gaussian State Space Model","text":"","category":"section"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"In this example the goal is to estimate hidden states of a Linear Dynamical process where all hidden states are Gaussians. A simple multivariate Linear Gaussian State Space Model can be described with the following equations:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"beginaligned\n p(x_ix_i - 1)  = mathcalN(x_iA * x_i - 1 mathcalP)\n p(y_ix_i)  = mathcalN(y_iB * x_i mathcalQ)\nendaligned","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"where x_i are hidden states, y_i are noisy observations, A, B are state transition and observational matrices, mathcalP and mathcalQ are state transition noise and observation noise covariance matrices. For a more rigorous introduction to Linear Gaussian Dynamical systems we refer to Simo Sarkka, Bayesian Filtering and Smoothing book.","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"To model this process in ReactiveMP, first, we start with importing all needed packages:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"using Rocket, ReactiveMP, GraphPPL, Distributions\nusing BenchmarkTools, Random, LinearAlgebra, Plots","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"Next step, is to generate some synthetic data:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"function generate_data(rng, A, B, Q, P)\n    x_prev = [ 10.0, -10.0 ]\n\n    x = Vector{Vector{Float64}}(undef, n)\n    y = Vector{Vector{Float64}}(undef, n)\n\n    for i in 1:n\n        x[i] = rand(rng, MvNormal(A * x_prev, Q))\n        y[i] = rand(rng, MvNormal(B * x[i], P))\n        x_prev = x[i]\n    end\n    \n    return x, y\nend","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"# Seed for reproducibility\nseed = 1234\n\nrng = MersenneTwister(seed)\n\n# We will model 2-dimensional observations with rotation matrix `A`\n# To avoid clutter we also assume that matrices `A`, `B`, `P` and `Q`\n# are known and fixed for all time-steps\nθ = π / 35\nA = [ cos(θ) -sin(θ); sin(θ) cos(θ) ]\nB = diageye(2)\nQ = diageye(2)\nP = 25.0 .* diageye(2)\n\n# Number of observations\nn = 300\n\nnothing #hide","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"note: Note\nFor large number of observations you will need yo use limit_stack_depth = 100 option during model creation, e.g. model, (x, y) = create_model(..., options = (limit_stack_depth = 100, ))","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"x, y = generate_data(rng, A, B, Q, P)\nnothing #hide","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"Lets plot our synthetic dataset. Lines represent our hidden states we want to estimate using noisy observations, which are represented as dots.","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"slicedim(dim) = (a) -> map(e -> e[dim], a)\n\npx = plot()\n\npx = plot!(px, x |> slicedim(1), label = \"Hidden Signal (dim-1)\", color = :orange)\npx = scatter!(px, y |> slicedim(1), label = false, markersize = 2, color = :orange)\npx = plot!(px, x |> slicedim(2), label = \"Hidden Signal (dim-2)\", color = :green)\npx = scatter!(px, y |> slicedim(2), label = false, markersize = 2, color = :green)\n\nplot(px)","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"To create a model we use GraphPPL package and @model macro:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"@model function rotate_ssm(n, x0, A, B, Q, P)\n    \n    # We create constvar references for better efficiency\n    cA = constvar(A)\n    cB = constvar(B)\n    cQ = constvar(Q)\n    cP = constvar(P)\n    \n    # `x` is a sequence of hidden states\n    x = randomvar(n)\n    # `y` is a sequence of \"clamped\" observations\n    y = datavar(Vector{Float64}, n)\n    \n    x_prior ~ MvNormalMeanCovariance(mean(x0), cov(x0))\n    x_prev = x_prior\n    \n    for i in 1:n\n        x[i] ~ MvNormalMeanCovariance(cA * x_prev, cQ)\n        y[i] ~ MvNormalMeanCovariance(cB * x[i], cP)\n        x_prev = x[i]\n    end\n    \n    return x, y\nend","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"Also for convenience we create an inference function to infer hidden states of our system:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"function inference(data, x0, A, B, Q, P)\n\n    # We create a model and get references for \n    # hidden states and observations\n    model, (x, y) = rotate_ssm(n, x0, A, B, Q, P);\n\n    xbuffer   = buffer(Marginal, n)\n    bfe       = nothing\n    \n    # We subscribe on posterior marginals of `x`\n    xsubscription = subscribe!(getmarginals(x), xbuffer)\n    # We are also intereset in BetheFreeEnergy functional,\n    # which in this case is equal to minus log evidence\n    fsubcription = subscribe!(score(BetheFreeEnergy(), model), (v) -> bfe = v)\n\n    # `update!` updates our clamped datavars\n    update!(y, data)\n\n    # It is important to always unsubscribe\n    unsubscribe!((xsubscription, fsubcription))\n    \n    return xbuffer, bfe\nend","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"To run inference we also specify prior for out first time-step:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"x0 = MvNormalMeanCovariance(zeros(2), 100.0 * diageye(2))\nnothing # hide","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"xmarginals, bfe = inference(y, x0, A, B, Q, P)\nnothing #hide","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"px = plot()\n\npx = plot!(px, x |> slicedim(1), label = \"Hidden Signal (dim-1)\", color = :orange)\npx = plot!(px, x |> slicedim(2), label = \"Hidden Signal (dim-2)\", color = :green)\n\npx = plot!(px, mean.(xmarginals) |> slicedim(1), ribbon = var.(xmarginals) |> slicedim(1) .|> sqrt, fillalpha = 0.5, label = \"Estimated Signal (dim-1)\", color = :teal)\npx = plot!(px, mean.(xmarginals) |> slicedim(2), ribbon = var.(xmarginals) |> slicedim(2) .|> sqrt, fillalpha = 0.5, label = \"Estimated Signal (dim-1)\", color = :violet)\n\nplot(px)","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"As we can see from our plot, estimated signal resembles closely to the real hidden states with small variance. We maybe also interested in the value for minus log evidence:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"bfe","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"We may be also interested in performance of our resulting Belief Propagation algorithm:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"@benchmark inference($y, $x0, $A, $B, $Q, $P)","category":"page"},{"location":"lib/node/#lib-node","page":"Overview","title":"Nodes implementation","text":"","category":"section"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"In message passing framework one of the most important concept is factor node.  Factor node represents a local function in a factorised representation of a generative model.","category":"page"},{"location":"lib/node/#lib-node-traits","page":"Overview","title":"Node traits","text":"","category":"section"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"Each factor node has to define as_node_functional_form trait function and to specify ValidNodeFunctionalForm singleton as a return object. By default as_node_functional_form returns UndefinedNodeFunctionalForm. Objects that do not specify this property correctly cannot be used in model specification.","category":"page"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"note: Note\n@node macro does that automatically","category":"page"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"ValidNodeFunctionalForm\nUndefinedNodeFunctionalForm\nas_node_functional_form","category":"page"},{"location":"lib/node/#ReactiveMP.ValidNodeFunctionalForm","page":"Overview","title":"ReactiveMP.ValidNodeFunctionalForm","text":"ValidNodeFunctionalForm\n\nTrait specification for an object that can be used in model specification as a factor node.\n\nSee also: as_node_functional_form, UndefinedNodeFunctionalForm\n\n\n\n\n\n","category":"type"},{"location":"lib/node/#ReactiveMP.UndefinedNodeFunctionalForm","page":"Overview","title":"ReactiveMP.UndefinedNodeFunctionalForm","text":"UndefinedNodeFunctionalForm\n\nTrait specification for an object that can not be used in model specification as a factor node.\n\nSee also: as_node_functional_form, ValidNodeFunctionalForm\n\n\n\n\n\n","category":"type"},{"location":"lib/node/#ReactiveMP.as_node_functional_form","page":"Overview","title":"ReactiveMP.as_node_functional_form","text":"as_node_functional_form(object)\n\nDetermines object node functional form trait specification. Returns either ValidNodeFunctionalForm() or UndefinedNodeFunctionalForm().\n\nSee also: ValidNodeFunctionalForm, UndefinedNodeFunctionalForm\n\n\n\n\n\n","category":"function"},{"location":"lib/node/#lib-node-types","page":"Overview","title":"Node types","text":"","category":"section"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"We distinguish different types of factor nodes to have a better control over Bethe Free Energy computation. Each factor node has either Deterministic or Stochastic functional form type.","category":"page"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"Deterministic\nStochastic\nisdeterministic\nisstochastic\nsdtype","category":"page"},{"location":"lib/node/#ReactiveMP.Deterministic","page":"Overview","title":"ReactiveMP.Deterministic","text":"Deterministic\n\nDeterministic object used to parametrize factor node object with determinstic type of relationship between variables.\n\nSee also: Stochastic, isdeterministic, isstochastic, sdtype\n\n\n\n\n\n","category":"type"},{"location":"lib/node/#ReactiveMP.Stochastic","page":"Overview","title":"ReactiveMP.Stochastic","text":"Stochastic\n\nStochastic object used to parametrize factor node object with stochastic type of relationship between variables.\n\nSee also: Deterministic, isdeterministic, isstochastic, sdtype\n\n\n\n\n\n","category":"type"},{"location":"lib/node/#ReactiveMP.isdeterministic","page":"Overview","title":"ReactiveMP.isdeterministic","text":"isdeterministic(node)\n\nFunction used to check if factor node object is deterministic or not. Returns true or false.\n\nSee also: Deterministic, Stochastic, isstochastic, sdtype\n\n\n\n\n\n","category":"function"},{"location":"lib/node/#ReactiveMP.isstochastic","page":"Overview","title":"ReactiveMP.isstochastic","text":"isstochastic(node)\n\nFunction used to check if factor node object is stochastic or not. Returns true or false.\n\nSee also: Deterministic, Stochastic, isdeterministic, sdtype\n\n\n\n\n\n","category":"function"},{"location":"lib/node/#ReactiveMP.sdtype","page":"Overview","title":"ReactiveMP.sdtype","text":"sdtype(object)\n\nReturns either Deterministic or Stochastic for a given object (if defined).\n\nSee also: Deterministic, Stochastic, isdeterministic, isstochastic\n\n\n\n\n\n","category":"function"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"using ReactiveMP","category":"page"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"For example + node has the Deterministic type:","category":"page"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"plus_node = make_node(+)\n\nprintln(\"Is `+` node deterministic: \", isdeterministic(plus_node))\nprintln(\"Is `+` node stochastic: \", isstochastic(plus_node))\nnothing #hide","category":"page"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"On the other hand Bernoulli node has the Stochastic type:","category":"page"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"bernoulli_node = make_node(Bernoulli)\n\nprintln(\"Is `Bernoulli` node deterministic: \", isdeterministic(bernoulli_node))\nprintln(\"Is `Bernoulli` node stochastic: \", isstochastic(bernoulli_node))","category":"page"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"To get an actual instance of the type object we use sdtype function:","category":"page"},{"location":"lib/node/","page":"Overview","title":"Overview","text":"println(\"sdtype() of `+` node is \", sdtype(plus_node))\nprintln(\"sdtype() of `Bernoulli` node is \", sdtype(bernoulli_node))\nnothing #hide","category":"page"},{"location":"lib/node/#lib-node-factorisation-constraints","page":"Overview","title":"Node factorisation constraints","text":"","category":"section"},{"location":"man/getting-started/#user-guide-getting-started","page":"Getting Started","title":"Getting started","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"ReactiveMP.jl is a Julia package for Bayesian Inference on Factor Graphs by Message Passing. It supports both exact and variational inference algorithms.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"ReactiveMP package is a successor of the ForneyLab package. It follows the same ideas and concepts for message-passing based inference, but uses new reactive and efficient message passing implementation under the hood. The API between two packages is different due to a better flexibility, performance and new reactive approach for solving inference problems.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"This page provides the necessary information you need to get started with ReactiveMP. We will show the general approach to solving inference problems with ReactiveMP by means of a running example: inferring the bias of a coin.","category":"page"},{"location":"man/getting-started/#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Install ReactiveMP through the Julia package manager:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"] add ReactiveMP","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"note: Note\nFor best user experience you also need to install GraphPPL, Rocket and Distributions packages.","category":"page"},{"location":"man/getting-started/#Example:-Inferring-the-bias-of-a-coin","page":"Getting Started","title":"Example: Inferring the bias of a coin","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"The ReactiveMP approach to solving inference problems consists of three phases:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Model specification: ReactiveMP uses GraphPPL package for model specification part. It offers a domain-specific language to specify your probabilistic model.\nInference specification: ReactiveMP inference API has been designed to be as flexible as possible and it is compatible both with asynchronous infinite data streams and with static datasets. For most of the use cases it consists of the same simple building blocks. In this example we will show one of the many possible ways to infer your quantities of interest.\nInference execution: Given model specification and inference procedure it is pretty straightforward to use reactive API from Rocket to pass data to the inference backend and to run actual inference.","category":"page"},{"location":"man/getting-started/#Coin-flip-simulation","page":"Getting Started","title":"Coin flip simulation","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Let's start by creating some dataset. One approach could be flipping a coin N times and recording each outcome. For simplicity in this example we will use static pre-generated dataset. Each sample can be thought of as the outcome of single flip which is either heads or tails (1 or 0). We will assume that our virtual coin is biased, and lands heads up on 75% of the trials (on average).","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"First lets setup our environment by importing all needed packages:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Next, lets define our dataset:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"rng = MersenneTwister(42)\nn = 10\np = 0.75\ndistribution = Bernoulli(p)\n\ndataset = float.(rand(rng, Bernoulli(p), n))","category":"page"},{"location":"man/getting-started/#Model-specification","page":"Getting Started","title":"Model specification","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"In a Bayesian setting, the next step is to specify our probabilistic model. This amounts to specifying the joint probability of the random variables of the system.","category":"page"},{"location":"man/getting-started/#Likelihood","page":"Getting Started","title":"Likelihood","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"We will assume that the outcome of each coin flip is governed by the Bernoulli distribution, i.e.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"y_i sim mathrmBernoulli(theta)","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"where y_i = 1 represents \"heads\", y_i = 0 represents \"tails\". The underlying probability of the coin landing heads up for a single coin flip is theta in 01.","category":"page"},{"location":"man/getting-started/#Prior","page":"Getting Started","title":"Prior","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"We will choose the conjugate prior of the Bernoulli likelihood function defined above, namely the beta distribution, i.e.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"theta sim Beta(a b)","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"where a and b are the hyperparameters that encode our prior beliefs about the possible values of theta. We will assign values to the hyperparameters in a later step.   ","category":"page"},{"location":"man/getting-started/#Joint-probability","page":"Getting Started","title":"Joint probability","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"The joint probability is given by the multiplication of the likelihood and the prior, i.e.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"P(y_1N θ) = P(θ) prod_i=1^N P(y_i  θ)","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Now let's see how to specify this model using GraphPPL's package syntax.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"\n# GraphPPL.jl export `@model` macro for model specification\n# It accepts a regular Julia function and builds an FFG under the hood\n@model function coin_model(n)\n\n    # `datavar` creates data 'inputs' in our model\n    # We will pass data later on to these inputs\n    # In this example we create a sequence of inputs that accepts Float64\n    y = datavar(Float64, n)\n    \n    # We endow θ parameter of our model with some prior\n    θ ~ Beta(2.0, 7.0)\n    \n    # We assume that outcome of each coin flip is governed by the Bernoulli distribution\n    for i in 1:n\n        y[i] ~ Bernoulli(θ)\n    end\n    \n    # We return references to our data inputs and θ parameter\n    # We will use these references later on during inference step\n    return y, θ\nend\n","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"As you can see, GraphPPL offers a model specification syntax that resembles closely to the mathematical equations defined above. We use datavar function to create \"clamped\" variables that take specific values at a later date. θ ~ Beta(2.0, 7.0) expression creates random variable θ and assigns it as an output of Beta node in the corresponding FFG. ","category":"page"},{"location":"man/getting-started/#Inference-specification","page":"Getting Started","title":"Inference specification","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Once we have defined our model, the next step is to use ReactiveMP API to infer quantities of interests. To do this, we need to specify inference procedure. ReactiveMP API is flexible in terms of inference specification and is compatible both with real-time inference processing and with static datasets. In most of the cases for static datasets, as in our example, it consists of same basic building blocks:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Return variables of interests from model specification\nSubscribe on variables of interests posterior marginal updates\nPass data to the model\nUnsubscribe ","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Here is an example of inference procedure:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"function inference(data)\n    n = length(data)\n\n    # `coin_model` function from `@model` macro returns a reference to the model object and \n    # the same output as in `return` statement in the original function specification\n    model, (y, θ) = coin_model(n)\n    \n    # Reference for future posterior marginal \n    mθ = nothing\n\n    # `getmarginal` function returns an observable of future posterior marginal updates\n    # We use `Rocket.jl` API to subscribe on this observable\n    # As soon as posterior marginal update is available we just save it in `mθ`\n    subscription = subscribe!(getmarginal(θ), (m) -> mθ = m)\n    \n    # `update!` function passes data to our data inputs\n    update!(y, data)\n    \n    # It is always a good practice to unsubscribe and to \n    # free computer resources held by the subscription\n    unsubscribe!(subscription)\n    \n    # Here we return our resulting posterior marginal\n    return mθ\nend","category":"page"},{"location":"man/getting-started/#Inference-execution","page":"Getting Started","title":"Inference execution","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Here after everything is ready we just call our inference function to get a posterior marginal distribution over θ parameter in the model.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"θestimated = inference(dataset)","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"println(\"mean: \", mean(θestimated))\nprintln(\"std:  \", std(θestimated))\nnothing #hide","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"using Plots, LaTeXStrings; theme(:default)\n\nrθ = range(0, 1, length = 1000)\n\np1 = plot(rθ, (x) -> pdf(Beta(2.0, 7.0), x), title=\"Prior\", fillalpha=0.3, fillrange = 0, label=L\"P\\:(\\theta)\", c=1,)\np2 = plot(rθ, (x) -> pdf(θestimated, x), title=\"Posterior\", fillalpha=0.3, fillrange = 0, label=L\"P\\:(\\theta|y)\", c=3)\n\nplot(p1, p2, layout = @layout([ a; b ]))","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"In our dataset we used 10 coin flips to estimate the bias of a coin. It resulted in a vague posterior distribution, however ReactiveMP scales very well for large models and factor graphs. We may use more coin flips in our dataset for better posterior distribution estimates:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"dataset_100   = float.(rand(rng, Bernoulli(p), 100))\ndataset_1000  = float.(rand(rng, Bernoulli(p), 1000))\ndataset_10000 = float.(rand(rng, Bernoulli(p), 10000))\nnothing # hide","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"θestimated_100   = inference(dataset_100)\nθestimated_1000  = inference(dataset_1000)\nθestimated_10000 = inference(dataset_10000)\nnothing #hide","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"p3 = plot(title = \"Posterior\", legend = :topleft)\n\np3 = plot!(p3, rθ, (x) -> pdf(θestimated_100, x), fillalpha = 0.3, fillrange = 0, label = L\"P\\:(\\theta\\:|y_{1:100})\", c = 4)\np3 = plot!(p3, rθ, (x) -> pdf(θestimated_1000, x), fillalpha = 0.3, fillrange = 0, label = L\"P\\:(\\theta\\:|y_{1:1000})\", c = 5)\np3 = plot!(p3, rθ, (x) -> pdf(θestimated_10000, x), fillalpha = 0.3, fillrange = 0, label = L\"P\\:(\\theta\\:|y_{1:10000})\", c = 6)\n\nplot(p1, p3, layout = @layout([ a; b ]))","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"With larger dataset our posterior marginal estimate becomes more and more accurate and represents real value of the bias of a coin.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"println(\"mean: \", mean(θestimated_10000))\nprintln(\"std:  \", std(θestimated_10000))\nnothing #hide","category":"page"},{"location":"man/getting-started/#Where-to-go-next?","page":"Getting Started","title":"Where to go next?","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"There are a set of demos available in ReactiveMP repository that demonstrate the more advanced features of the package and also Examples section in the documentation. Alternatively, you can head to the Model specification which provides more detailed information of how to use ReactiveMP and GraphPPL to specify probabilistic models. Inference execution section provides a documentation about ReactiveMP API for running reactive Bayesian inference.","category":"page"},{"location":"examples/overview/#examples-overview","page":"Overview","title":"Examples overview","text":"","category":"section"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"This section contains a set of examples for Bayesian Inference with ReactiveMP package in various probabilistic models.","category":"page"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"note: Note\nThis section is WIP and more examples will be added over time. More examples can be found in demo/ folder at GitHub repository.","category":"page"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"Gaussian Linear Dynamical System: An example of inference procedure for Gaussian Linear Dynamical System with multivariate noisy observations using Belief Propagation (Sum Product) algorithm. Reference: Simo Sarkka, Bayesian Filtering and Smoothing.\nHierarchical Gaussian Filter: An example of online inference procedure for Hierarchical Gaussian Filter with univariate noisy observations using Variational Message Passing algorithm. Reference: Ismail Senoz, Online Message Passing-based Inference in the Hierarchical Gaussian Filter\nAutoregressive Model: An example of variational Bayesian Inference on full graph for Autoregressive model. Reference: Albert Podusenko, Message Passing-Based Inference for Time-Varying Autoregressive Models.","category":"page"},{"location":"man/model-specification/#user-guide-model-specification","page":"Model Specification","title":"Model Specification","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Probabilistic models incorporate elements of randomness to describe an event or phenomenon by using random variables and probability theory. A probabilistic model can be represented visually by using probabilistic graphical models (PGMs). A factor graph is a type of PGM that is well suited to cast inference tasks in terms of graphical manipulations.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"GraphPPL.jl is a Julia package presenting a model specification language for probabilistic models.","category":"page"},{"location":"man/model-specification/#@model-macro","page":"Model Specification","title":"@model macro","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"The ReactiveMP uses GraphPPL library to simplify model specification. It is not necessary but highly recommended to use ReactiveMP in a combination with GraphPPL model specification library. The GraphPPL library exports a single @model macro for model specification. The @model macro accepts two arguments: model options (optionally) and the model specification itself in a form of regular Julia function. ","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"For example: ","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"# `@model` macro accepts an array of named options as a first argument and\n# a regular Julia function body as its second argument\n@model [ option1 = ..., option2 = ... ] function model_name(model_arguments...)\n    # model specification goes here\n    return ...\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Model options are optional and may be omitted:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"@model function model_name(model_arguments...)\n    # model specification here\n    return ...\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"that is equivalent to ","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"# Empty options if ommited\n@model [] function model_name(model_arguments...)\n    # model specification here\n    return ...\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"The @model macro returns a regular Julia function (in this example model_name(model_arguments...)) that has the same signature and can be executed as usual. It returns a reference to a model object itself and a tuple of a user specified return variables, e.g:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"@model function my_model(model_arguments...)\n    # model specification here\n    # ...\n    return x, y\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"model, (x, y) = my_model(model_arguments...)","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"It is also important to note that any model should return something, such as variables or nodes. If a model doesn't return anything then an error will be raised during runtime.  model object might be useful to inspect model's factor graph and/or factor nodes and variables. It is also used in Bethe Free Energy score computation. If not needed it can be ommited with _ placeholder, eg:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"_, (x, y) = my_model(model_arguments...)","category":"page"},{"location":"man/model-specification/#A-full-example-before-diving-in","page":"Model Specification","title":"A full example before diving in","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Before presenting the details of the model specification syntax, we show an example of a simple probabilistic model. Here we create a linear gaussian state space model with latent random variables x and noisy observations y:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"@model [ options... ] function state_space_model(n_observations, noise_variance)\n\n    x = randomvar(n_observations)\n    y = datavar(Float64, n_observations)\n\n    x[1] ~ NormalMeanVariance(0.0, 100.0)\n\n    for i in 2:n_observations\n       x[i] ~ x[i - 1] + 1.0\n       y[i] ~ NormalMeanVariance(x[i], noise_variance)\n    end\n\n    return x, y\nend","category":"page"},{"location":"man/model-specification/#Model-variables","page":"Model Specification","title":"Model variables","text":"","category":"section"},{"location":"man/model-specification/#Constants","page":"Model Specification","title":"Constants","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Any runtime constant passed to a model as a model argument will be automatically converted to a fixed constant in the graph model. This convertion happens every time when model specification identifies a constant. Sometimes it might be useful to create constants by hand (e.g. to avoid copying large matrices across the model and to avoid extensive memory allocations).","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"You can create a constant within a model specification macro with constvar() function. For example:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"c = constvar(1.0)\n\nfor i in 2:n\n    x[i] ~ x[i - 1] + c # Reuse the same reference to a constant 1.0\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Additionally you can specify an extra ::ConstVariable type for some of the model arguments. In this case macro automatically converts them to a single constant using constvar() function. E.g.:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"@model function model_name(nsamples::Int, c::ConstVariable)\n    # ...\n    # no need to call for a constvar() here\n    for i in 2:n\n        x[i] ~ x[i - 1] + c # Reuse the same reference to a constant `c`\n    end\n    # ...\n    return ...\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"note: Note\n::ConstVariable does not restrict an input type of an argument and does not interfere with multiple dispatch. In this example c can have any type, e.g. Int.","category":"page"},{"location":"man/model-specification/#user-guide-model-specification-data-variables","page":"Model Specification","title":"Data variables","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"It is important to have a mechanism to pass data values to the model. You can create data inputs with datavar() function. As a first argument it accepts a type specification and optional dimensionality (as additional arguments or as a tuple).","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Examples: ","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"y = datavar(Float64) # Creates a single data input with `y` as identificator\ny = datavar(Float64, n) # Returns a vector of  `y_i` data input objects with length `n`\ny = datavar(Float64, n, m) # Returns a matrix of `y_i_j` data input objects with size `(n, m)`\ny = datavar(Float64, (n, m)) # It is also possible to use a tuple for dimensionality, it is an equivalent of the previous line","category":"page"},{"location":"man/model-specification/#Random-variables","page":"Model Specification","title":"Random variables","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"There are several ways to create random variables. The first one is an explicit call to randomvar() function. By default it doesn't accept any argument, creates a single random variable in the model and returns it. It is also possible to pass dimensionality arguments to randomvar() function in the same way as for the datavar() function.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Examples: ","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"x = randomvar() # Returns a single random variable which can be used later in the model\nx = randomvar(n) # Returns an vector of random variables with length `n`\nx = randomvar(n, m) # Returns a matrix of random variables with size `(n, m)`\nx = randomvar((n, m)) # It is also possible to use a tuple for dimensionality, it is an equivalent of the previous line","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"The second way to create a random variable is to use the ~ operator. If the random variable hasn't been created yet, ~ operator will be creat it automatically during the creation of the node. Read more about the ~ operator in the next section.","category":"page"},{"location":"man/model-specification/#Factor-nodes","page":"Model Specification","title":"Factor nodes","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Factor nodes (or local functions) are used to define a relationship between random variables and/or constants and data inputs. In most of the cases a factor node defines a probability distribution over selected random variables. ","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"We model a random variable by a probability distribution using the ~ operator. For example, to create a random variable y which is modeled by a Normal distribution, where its mean and variance are controlled by the random variables m and v respectively, we define","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"m = randomvar()\nv = randomvar()\ny ~ NormalMeanVariance(m, v) # Creates a `y` random variable automatically","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"It is also possible to use a deterministic relationships between random variables:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"a = randomvar()\nb = randomvar()\nc ~ a + b # Here with the help of `~` operator we explictly say that `c` is a random variable too","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"note: Note\nThe GraphPPL.jl package uses the ~ operator for modelling both stochastic and deterministic relationships between random variables.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"The @model macro automatically resolves any inner function calls into anonymous extra nodes. It is also worth to note that inference backend will try to optimize inner deterministic function calls in the case where all arguments are constants or data inputs. For example:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"noise ~ NormalMeanVariance(mean, inv(precision)) # Will create a non-linear node `inv` in case if `precision` is a random variable. Won't create an additional non-linear node in case if `precision` is a constant or data input.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"It is possible to use any functional expression within the ~ operator arguments list. The only one exception is the ref expression (e.g x[i] or x[i, j]). In principle x[i] expression is equivalent to getindex(x, i) and therefore might be treated as a factor node with getindex as local function, however all ref expressions within the ~ operator arguments list are left untouched during model parsing. This means that the model parser will not create unnecessary nodes when only simple indexing is involved.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"y ~ NormalMeanVariance(x[i - 1], variance) # While in principle `x[i - 1]` is equivalent to (`getindex(x, -(i, 1))`) model parser will leave it untouched and won't create any anonymous nodes for this expression.\n\ny ~ NormalMeanVariance(A * x[i - 1], variance) # This example will create a `*` anonymous node (in case if x[i - 1] is a random variable) and leave `x[i - 1]` untouched.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"It is also possible to return a node reference from the ~ operator with the following syntax:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"node, y ~ NormalMeanVariance(mean, var)","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Having a node reference can be useful in case the user wants to return it from a model and to use it later on to specify initial joint marginal distributions.","category":"page"},{"location":"man/model-specification/#Node-creation-options","page":"Model Specification","title":"Node creation options","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"To pass optional arguments to the node creation constructor the user can use the where { options...  } specification syntax.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Example:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"y ~ NormalMeanVariance(y_mean, y_var) where { q = q(y_mean)q(y_var)q(y) } # mean-field factorisation over q","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"A list of all available options is presented below:","category":"page"},{"location":"man/model-specification/#Factorisation-constraint-option","page":"Model Specification","title":"Factorisation constraint option","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Users can specify a factorisation constraint over the approximate posterior q for variational inference. The general syntax for factorisation constraints over q is the following:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"variable ~ Node(node_arguments...) where { q = RecognitionFactorisationConstraint }","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"where RecognitionFactorisationConstraint can be one the following:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"MeanField()","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Automatically specifies a mean-field factorisation","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Example:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"y ~ NormalMeanVariance(y_mean, y_var) where { q = MeanField() }","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"FullFactorisation()","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Automatically specifies a full factorisation (this is the default)","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Example:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"y ~ NormalMeanVariance(y_mean, y_var) where { q = FullFactorisation() }","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"q(μ)q(v)q(out) or q(μ) * q(v) * q(out)","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"A user can specify any factorisation he wants as the multiplication of q(interface_names...) factors. As interface names the user can use the interface names of an actual node (read node's documentation), its aliases (if available) or actual random variable names present in the ~ operator expression.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Examples: ","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"# Using interface names of a `NormalMeanVariance` node for factorisation constraint. \n# Call `?NormalMeanVariance` to know more about interface names for some node\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(μ)q(v)q(out) }\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(μ, v)q(out) }\n\n# Using interface names aliases of a `NormalMeanVariance` node for factorisation constraint. \n# Call `?NormalMeanVariance` to know more about interface names aliases for some node\n# In general aliases correspond to the function names for distribution parameters\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(mean)q(var)q(out) }\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(mean, var)q(out) }\n\n# Using random variables names from `~` operator expression\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(y_mean)q(y_var)q(y) }\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(y_mean, y_var)q(y) }\n\n# All methods can be combined easily\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(μ)q(y_var)q(out) }\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(y_mean, v)q(y) }","category":"page"},{"location":"man/model-specification/#Metadata-option","page":"Model Specification","title":"Metadata option","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Is is possible to pass any extra metadata to a factor node with the meta option (if node supports it, read node's documentation). Metadata can be later accessed in message computation rules:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"z ~ f(x, y) where { meta = ... }","category":"page"},{"location":"man/model-specification/#Pipeline-option","page":"Model Specification","title":"Pipeline option","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"To assign a factor node's local pipeline we use a pipeline option:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"y ~ NormalMeanVariance(m, v) where { pipeline = LoggerPipelineStage() } # Logs all outbound messages with `LoggerPipelineStage`","category":"page"},{"location":"lib/math/#lib-math","page":"Math utils","title":"Math utilities","text":"","category":"section"},{"location":"lib/math/","page":"Math utils","title":"Math utils","text":"ReactiveMP package exports tiny and huge objects to represent tiny and huge numbers. These objects aren't really numbers and behave differently depending on the context. They do support any operation that is defined for Real numbers. For more info see Julia's documentation about promotion.","category":"page"},{"location":"lib/math/","page":"Math utils","title":"Math utils","text":"tiny\nhuge\nTinyNumber\nHugeNumber","category":"page"},{"location":"lib/math/#ReactiveMP.tiny","page":"Math utils","title":"ReactiveMP.tiny","text":"tiny\n\nAn instance of a TinyNumber. Behaviour and actual value of the tiny number depends on the context.\n\nExample\n\njulia> tiny\ntiny\n\njulia> 1 + tiny\n1.000000000001\n\njulia> tiny + 1\n1.000000000001\n\njulia> 1f0 + tiny\n1.000001f0\n\njulia> big\"1.0\" + tiny\n1.000000000000000000000001\n\njulia> big\"1\" + tiny\n1.000000000000000000000001\n\nSee also: huge, TinyNumber, HugeNumber\n\n\n\n\n\n","category":"constant"},{"location":"lib/math/#ReactiveMP.huge","page":"Math utils","title":"ReactiveMP.huge","text":"huge\n\nAn instance of a HugeNumber. Behaviour and actual value of the huge number depends on the context.\n\nExample\n\njulia> huge\nhuge\n\njulia> 1 + huge\n1.000000000001e12\n\njulia> huge + 1\n1.000000000001e12\n\njulia> 1f0 + huge\n1.000001f6\n\njulia> big\"1.0\" + huge\n1.000000000000000000000001e+24\n\njulia> big\"1\" + huge\n1.000000000000000000000001e+24\n\nSee also: tiny, TinyNumber, HugeNumber\n\n\n\n\n\n","category":"constant"},{"location":"lib/math/#ReactiveMP.TinyNumber","page":"Math utils","title":"ReactiveMP.TinyNumber","text":"TinyNumber <: Real\n\nTinyNumber represents (wow!) tiny number that can be used in a various computations without unnecessary type promotions.\n\nSee also: HugeNumber\n\n\n\n\n\n","category":"type"},{"location":"lib/math/#ReactiveMP.HugeNumber","page":"Math utils","title":"ReactiveMP.HugeNumber","text":"HugeNumber <: Real\n\nHugeNumber represents (wow!) huge number that can be used in a various computations without unnecessary type promotions.\n\nSee also: TinyNumber\n\n\n\n\n\n","category":"type"},{"location":"examples/hierarchical_gaussian_filter/#examples-hgf","page":"Hierarchical Gaussian Filter","title":"Example: Hierarchical Gaussian Filter","text":"","category":"section"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"In this demo the goal is to perform approximate variational Bayesian Inference for Univariate Hierarchical Gaussian Filter (HGF).","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Simple HGF model can be defined as:","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"beginaligned\n    x^(j)_k  sim  mathcalN(x^(j)_k - 1 f_k(x^(j - 1)_k)) \n    y_k  sim  mathcalN(x^(j)_k tau_k)\nendaligned","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"where j is an index of layer in hierarchy, k is a time step and f_k is a variance activation function. ReactiveMP.jl export Gaussian Controlled Variance (GCV) node with f_k = exp(kappa x + omega) variance activation function. By default uses Gauss-Hermite cubature with a prespecified number of approximation points in the cubature. We can change the number of points in Gauss-Hermite cubature with the help of metadata structures in ReactiveMP.jl. ","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":" beginaligned\n    z_k  sim  mathcalN(z_k - 1 mathcaltau_z) \n    x_k  sim  mathcalN(x_k - 1 exp(kappa z_k + omega)) \n    y_k  sim  mathcalN(x_k mathcaltau_y)\nendaligned","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"In this experiment we will create a single time step of the graph and perform variational message passing filtering alrogithm to estimate hidden states of the system. For a more rigorous introduction to Hierarchical Gaussian Filter we refer to Ismail Senoz, Online Message Passing-based Inference in the Hierarchical Gaussian Filter paper.","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"For simplicity we will consider tau_z, tau_y, kappa and omega known and fixed.","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"To model this process in ReactiveMP, first, we start with importing all needed packages:","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"using Rocket, ReactiveMP, GraphPPL, Distributions\nusing BenchmarkTools, Random, Plots","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Next step, is to generate some synthetic data:","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"function generate_data(rng, k, w, zv, yv)\n    z_prev = 0.0\n    x_prev = 0.0\n\n    z = Vector{Float64}(undef, n)\n    v = Vector{Float64}(undef, n)\n    x = Vector{Float64}(undef, n)\n    y = Vector{Float64}(undef, n)\n\n    for i in 1:n\n        z[i] = rand(rng, Normal(z_prev, sqrt(zv)))\n        v[i] = exp(k * z[i] + w)\n        x[i] = rand(rng, Normal(x_prev, sqrt(v[i])))\n        y[i] = rand(rng, Normal(x[i], sqrt(yv)))\n\n        z_prev = z[i]\n        x_prev = x[i]\n    end \n    \n    return z, x, y\nend","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"# Seed for reproducibility\nseed = 123\n\nrng = MersenneTwister(seed)\n\n# Parameters of HGF process\nreal_k = 1.0\nreal_w = 0.0\nz_variance = abs2(0.5)\ny_variance = abs2(1.0)\n\n# Number of observations\nn = 300\n\nz, x, y = generate_data(rng, real_k, real_w, z_variance, y_variance)\nnothing #hide","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Lets plot our synthetic dataset. Lines represent our hidden states we want to estimate using noisy observations.","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"let \n    pz = plot(title = \"Hidden States Z\")\n    px = plot(title = \"Hidden States X\")\n    \n    plot!(pz, 1:n, z, label = \"z_i\", color = :orange)\n    plot!(px, 1:n, x, label = \"x_i\", color = :green)\n    \n    plot(pz, px, layout = @layout([ a; b ]))\nend","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"To create a model we use GraphPPL package and @model macro:","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"# We create a single-time step of corresponding state-space process to\n# perform online learning (filtering)\n@model [ default_factorisation = MeanField() ] function hgf(real_k, real_w, z_variance, y_variance)\n    \n    # Priors from previous time step for `z`\n    zt_min_mean = datavar(Float64)\n    zt_min_var  = datavar(Float64)\n    \n    # Priors from previous time step for `x`\n    xt_min_mean = datavar(Float64)\n    xt_min_var  = datavar(Float64)\n\n    zt_min ~ NormalMeanVariance(zt_min_mean, zt_min_var)\n    xt_min ~ NormalMeanVariance(xt_min_mean, xt_min_var)\n\n    # Higher layer is modelled as a random walk \n    zt ~ NormalMeanVariance(zt_min, z_variance) where { q = q(zt, zt_min)q(z_variance) }\n    \n    # Lower layer is modelled with `GCV` node\n    gcv_node, xt ~ GCV(xt_min, zt, real_k, real_w) where { q = q(xt, xt_min)q(zt)q(κ)q(ω) }\n    \n    # Noisy observations \n    y = datavar(Float64)\n    y ~ NormalMeanVariance(xt, y_variance)\n    \n    return zt, xt, y, gcv_node, xt_min_mean, xt_min_var, zt_min_mean, zt_min_var\nend","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"function inference(data, vmp_iters, real_k, real_w, z_variance, y_variance)\n    n = length(data)\n    \n    # We don't want to save all marginals from all VMP iterations\n    # but only last one after all VMP iterations per time step\n    # Rocket.jl exports PendingScheduler() object that postpones \n    # any update unless manual `resolve!()` has been called\n    ms_scheduler = PendingScheduler()\n    \n    mz = keep(Marginal)\n    mx = keep(Marginal)\n    fe = ScoreActor(Float64)\n\n    model, (zt, xt, y, gcv_node, xt_min_mean, xt_min_var, zt_min_mean, zt_min_var) = hgf(real_k, real_w, z_variance, y_variance)\n\n    # Initial priors\n    current_zt_mean, current_zt_var = 0.0, 10.0\n    current_xt_mean, current_xt_var = 0.0, 10.0\n    \n    s_mz = subscribe!(getmarginal(zt) |> schedule_on(ms_scheduler), mz)\n    s_mx = subscribe!(getmarginal(xt) |> schedule_on(ms_scheduler), mx)\n    s_fe = subscribe!(score(Float64, BetheFreeEnergy(), model), fe)\n\n    # Initial marginals to start VMP procedire\n    setmarginal!(gcv_node, :y_x, MvNormalMeanCovariance([ 0.0, 0.0 ], [ 5.0, 5.0 ]))\n    setmarginal!(gcv_node, :z, NormalMeanVariance(0.0, 5.0))\n    \n    # For each observations we perofrm `vmp_iters` VMP iterations\n    for i in 1:n\n        \n        for _ in 1:vmp_iters\n            update!(y, data[i])\n            update!(zt_min_mean, current_zt_mean)\n            update!(zt_min_var, current_zt_var)\n            update!(xt_min_mean, current_xt_mean)\n            update!(xt_min_var, current_xt_var)\n        end\n        \n        # After all VMP iterations we release! `PendingScheduler`\n        # as well as release! `ScoreActor` to indicate new time step\n        release!(ms_scheduler)\n        release!(fe)\n        \n        current_zt_mean, current_zt_var = mean_var(last(mz))::Tuple{Float64, Float64}\n        current_xt_mean, current_xt_var = mean_var(last(mx))::Tuple{Float64, Float64}\n    end\n    \n    # It is important to unsubscribe at the end of the inference procedure\n    unsubscribe!((s_mz, s_mx, s_fe))\n    \n    return map(getvalues, (mz, mx, fe))\nend","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"To run inference we also specify number of VMP iterations we want to perform as well as an approximation method for GCV node:","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"vmp_iters = 10\nnothing #hide","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"mz, mx, fe = inference(y, vmp_iters, real_k, real_w, z_variance, y_variance)\nnothing #hide","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"let \n    pz = plot(title = \"Hidden States Z\")\n    px = plot(title = \"Hidden States X\")\n    \n    plot!(pz, 1:n, z, label = \"z_i\", color = :orange)\n    plot!(pz, 1:n, mean.(mz), ribbon = std.(mz), label = \"estimated z_i\", color = :teal)\n    \n    plot!(px, 1:n, x, label = \"x_i\", color = :green)\n    plot!(px, 1:n, mean.(mx), ribbon = std.(mx), label = \"estimated x_i\", color = :violet)\n    \n    plot(pz, px, layout = @layout([ a; b ]))\nend","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"As we can see from our plot, estimated signal resembles closely to the real hidden states with small variance. We maybe also interested in the values for Bethe Free Energy functional:","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"plot(fe, label = \"Bethe Free Energy\")","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"As we can see BetheFreeEnergy converges nicely to a stable point.","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"We may be also interested in performance of our resulting Variational Message Passing algorithm:","category":"page"},{"location":"examples/hierarchical_gaussian_filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"@benchmark inference($y, $vmp_iters, $real_k, $real_w, $z_variance, $y_variance)","category":"page"},{"location":"man/fundamentals/#user-guide-fundamentals","page":"Fundamentals","title":"Fundamentals","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"This tutorials covers the fundamentals of the ReactiveMP.jl package. For a more advanced usage we refer the interested reader to the other sections of the documentation.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"This tutorial also exists in the form of a Jupyter notebook in demo/ folder at GitHub repository.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"First lets setup our environment by importing all needed packages:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random","category":"page"},{"location":"man/fundamentals/#user-guide-fundamentals-model-syntax","page":"Fundamentals","title":"General model specification syntax","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"We use the @model macro from the GraphPPL.jl package to create a probabilistic model p(s y) and to specify extra constraints on the variational family of distributions mathcalQ, used for approximating intractable posterior distributions. Below there is a simple example of the general syntax for model specification. In this tutorial we do not cover all possible ways to create models or advanced features of GraphPPL.jl.  Instead we refer the interested reader to the Model specification section for a more rigorous explanation and illustrative examples.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# the `@model` macro accepts a regular Julia function\n@model function test_model1(s_mean, s_precision)\n    \n    # We use the `randomvar` function to create \n    # a random variable in our model\n    s = randomvar()\n    \n    # the `tilde` operator creates a functional dependency\n    # between variables in our model and can be read as \n    # `sampled from` or `is modeled by`\n    s ~ GaussianMeanPrecision(s_mean, s_precision)\n    \n    # We use the `datavar` function to create \n    # observed data variables in our models\n    # We also need to specify the type of our data \n    # In this example it is `Float64`\n    y = datavar(Float64)\n    \n    y ~ GaussianMeanPrecision(s, 1.0)\n    \n    return s, y\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"The @model macro creates a function with the same name and with the same set of input arguments as the original function (test_model1(s_mean, s_precision) in this example). However, the return value is modified in such a way to contain a reference to the model object as the first value and to the user specified variables in the form of a tuple as the second value.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"model, (s, y) = test_model1(0.0, 1.0)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Later on we can examine our model structure with the help of some utility functions such as: ","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getnodes(): returns an array of factor nodes in a correposning factor graph\ngetrandom(): returns an array of random variable in the model\ngetdata(): returns an array of data inputs in the model\ngetconstant(): return an array of constant values in the model","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getnodes(model)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getrandom(model) .|> name","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getdata(model) .|> name","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getconstant(model) .|> getconst","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"It is also possible to use control flow statements such as if or for blocks in the model specification function. In general, any valid snippet of Julia code can be used inside the @model block. As an example consider the following (valid!) model:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random #hide\n\n@model function test_model2(n)\n    \n    if n <= 1\n        error(\"`n` argument must be greater than one.\")\n    end\n    \n    # `randomvar(n)` creates a dense sequence of \n    # random variables\n    s = randomvar(n)\n    \n    # `datavar(Float64, n)` creates a dense sequence of \n    # observed data variables of type `Float64`\n    y = datavar(Float64, n)\n    \n    s[1] ~ GaussianMeanPrecision(0.0, 0.1)\n    y[1] ~ GaussianMeanPrecision(s[1], 1.0)\n    \n    for i in 2:n\n        s[i] ~ GaussianMeanPrecision(s[i - 1], 1.0)\n        y[i] ~ GaussianMeanPrecision(s[i], 1.0)\n    end\n    \n    return s, y\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"model, (s, y) = test_model2(10)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# An amount of factor nodes in generated Factor Graph\ngetnodes(model) |> length","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# An amount of random variables\ngetrandom(model) |> length","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# An amount of data inputs\ngetdata(model) |> length","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# An amount of constant values\ngetconstant(model) |> length","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"It is also possible to use complex expression inside the functional dependency expressions","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"y ~ NormalMeanPrecision(2.0 * (s + 1.0), 1.0)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"The ~ operator automatically creates a random variable if none was created before with the same name and throws an error if this name already exists","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# s = randomvar() here is optional\n# `~` creates random variables automatically\ns ~ NormalMeanPrecision(0.0, 1.0)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"An example model which will throw an error:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@model function error_model1()\n    s = 1.0\n    s ~ NormalMeanPrecision(0.0, 1.0)\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"LoadError: Invalid name 's' for new random variable. 's' was already initialized with '=' operator before.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"By default the GraphPPL.jl package creates new references for constants (literals like 0.0 or 1.0) in a model. In some situations this may not be efficient, especially when these constants represent large matrices. GraphPPL.jl will by default create new copies of some constant (e.g. matrix) in a model every time it uses it. However it is possible to use constvar() function to create and reuse similar constants in the model specification syntax as","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# Creates constant reference in a model with a prespecified value\nc = constvar(0.0)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"An example:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random #hide\n\n@model function test_model5(dim::Int, n::Int, A::Matrix, P::Matrix, Q::Matrix)\n    \n    s = randomvar(n)\n    \n    y = datavar(Vector{Float64}, n)\n    \n    # Here we create constant references\n    # for constant matrices in our model \n    # to make inference more memory efficient\n    cA = constvar(A)\n    cP = constvar(P)\n    cQ = constvar(Q)\n    \n    s[1] ~ MvGaussianMeanCovariance(zeros(dim), cP)\n    y[1] ~ MvGaussianMeanCovariance(s[1], cQ)\n    \n    for i in 2:n\n        s[i] ~ MvGaussianMeanCovariance(cA * s[i - 1], cP)\n        y[i] ~ MvGaussianMeanCovariance(s[i], cQ)\n    end\n    \n    return s, y\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"The ~ expression can also return a reference to a newly created node in a corresponding factor graph for convenience in later usage:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@model function test_model()\n\n    # In this example `ynode` refers to the corresponding \n    # `GaussianMeanVariance` node created in the factor graph\n    ynode, y ~ GaussianMeanVariance(0.0, 1.0)\n    \n    return ynode, y\nend","category":"page"},{"location":"man/fundamentals/#user-guide-fundamentals-inference","page":"Fundamentals","title":"Probabilistic inference in ReactiveMP.jl","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"ReactiveMP.jl uses the Rocket.jl package API for inference routines. Rocket.jl is a reactive programming extension for Julia that is higly inspired by RxJS and similar libraries from the Rx ecosystem. It consists of observables, actors, subscriptions and operators. For more infromation and rigorous examples see Rocket.jl github page.","category":"page"},{"location":"man/fundamentals/#Observables","page":"Fundamentals","title":"Observables","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Observables are lazy push-based collections and they deliver their values over time.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# Timer that emits a new value every second and has an initial one second delay \nobservable = timer(1000, 1000)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"A subscription allows us to subscribe on future values of some observable, and actors specify what to do with these new values:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"actor = (value) -> println(value)\nsubscription1 = subscribe!(observable, actor)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"0\n1\n2\n3\n...","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# We always need to unsubscribe from some observables\nunsubscribe!(subscription1)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# We can modify our observables\nmodified = observable |> filter(d -> rem(d, 2) === 1) |> map(Int, d -> d ^ 2)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"subscription2 = subscribe!(modified, (value) -> println(value))","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"1\n9\n25\n...","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"unsubscribe!(subscription2)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"The ReactiveMP.jl package returns posterior marginal distributions in our specified model in the form of an observable. It is possible to subscribe on its future updates, but for convenience ReactiveMP.jl only caches the last obtained values of all marginals in a model. To get a reference for the posterior marginal of some random variable in a model ReactiveMP.jl exports two functions: ","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getmarginal(x): for a single random variable x\ngetmarginals(xs): for a dense sequence of random variables sx","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Lets see how it works in practice. Here we create a simple coin toss model. We assume that observations are governed by the Bernoulli distribution with unknown bias parameter θ. To have a fully Bayesian treatment of this problem we endow θ with the Beta prior.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random #hide\n\n@model function coin_toss_model(n)\n\n    # `datavar` creates data 'inputs' in our model\n    # We will pass data later on to these inputs\n    # In this example we create a sequence of inputs that accepts Float64\n    y = datavar(Float64, n)\n    \n    # We endow θ parameter of our model with some prior\n    θ ~ Beta(2.0, 7.0)\n    \n    # We assume that the outcome of each coin flip \n    # is modeled by a Bernoulli distribution\n    for i in 1:n\n        y[i] ~ Bernoulli(θ)\n    end\n    \n    # We return references to our data inputs and θ parameter\n    # We will use these references later on during the inference step\n    return y, θ\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"_, (y, θ) = coin_toss_model(500)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# As soon as we have a new value for the marginal posterior over the `θ` variable\n# we simply print the first two statistics of it\nθ_subscription = subscribe!(getmarginal(θ), (marginal) -> println(\"New update: mean(θ) = \", mean(marginal), \", std(θ) = \", std(marginal)));\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Next, lets define our dataset:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"p       = 0.75 # Bias of a coin\ndataset = float.(rand(Bernoulli(p), 500));","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"To pass data to our model we use update! function","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"update!(y, dataset)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# It is necessary to always unsubscribe from running observables\nunsubscribe!(θ_subscription)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# The ReactiveMP.jl inference backend is lazy and does not compute posterior marginals if no-one is listening for them\n# At this moment we have already unsubscribed from the new posterior updates so this `update!` does nothing\nupdate!(y, dataset)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Rocket.jl provides some useful built-in actors for obtaining posterior marginals especially with static datasets.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# the `keep` actor simply keeps all incoming updates in an internal storage, ordered\nθvalues = keep(Marginal)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# `getmarginal` always emits last cached value as its first value\nsubscribe!(getmarginal(θ) |> take(1), θvalues)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getvalues(θvalues)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# `getmarginal` always emits last cached value as its first value\nsubscribe!(getmarginal(θ) |> take(1), θvalues)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getvalues(θvalues)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# the `buffer` actor keeps very last incoming update in an internal storage and can also store \n# an array of updates for a sequence of random variables\nθbuffer = buffer(Marginal, 1)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"subscribe!(getmarginals([ θ ]) |> take(1), θbuffer);\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getvalues(θbuffer)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"subscribe!(getmarginals([ θ ]) |> take(1), θbuffer);\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getvalues(θbuffer)","category":"page"},{"location":"man/fundamentals/#user-guide-fundamentals-reactive-inference","page":"Fundamentals","title":"Reactive inference in ReactiveMP.jl","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"ReactiveMP.jl naturally supports reactive streams of data and it is possible to run reactive inference with some external datasource.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random #hide\n\n@model function online_coin_toss_model()\n    \n    # We create datavars for the prior \n    # over `θ` variable\n    θ_a = datavar(Float64)\n    θ_b = datavar(Float64)\n    \n    θ ~ Beta(θ_a, θ_b)\n    \n    y = datavar(Float64)\n    y ~ Bernoulli(θ)\n\n    return θ_a, θ_b, θ, y\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"_, (θ_a, θ_b, θ, y) = online_coin_toss_model()\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# In this example we subscribe on posterior marginal of θ variable and use it as a prior for our next observation\n# We also print into stdout for convenience\nθ_subscription = subscribe!(getmarginal(θ), (m) -> begin \n    m_a, m_b = params(m)\n    update!(θ_a, m_a)\n    update!(θ_b, m_b)\n    println(\"New posterior for θ: mean = \", mean(m), \", std = \", std(m))\nend)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# Initial priors\nupdate!(θ_a, 10.0 * rand())\nupdate!(θ_b, 10.0 * rand())","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"data_source = timer(500, 500) |> map(Float64, (_) -> float(rand(Bernoulli(0.75)))) |> tap((v) -> println(\"New observation: \", v))\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"data_subscription = subscribe!(data_source |> take(5), (data) -> update!(y, data))\nsleep(5) #hide\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# It is important to unsubscribe from running observables to release computer resources\nunsubscribe!(data_subscription)\nunsubscribe!(θ_subscription)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"That was an example of exact Bayesian inference with Sum-Product (or Belief Propagation) algorithm. However, ReactiveMP.jl is not limited to only the sum-product algorithm but it also supports variational message passing with Constrained Bethe Free Energy Minimisation.","category":"page"},{"location":"man/fundamentals/#user-guide-fundamentals-vmp-inference","page":"Fundamentals","title":"Variational inference in ReactiveMP.jl","text":"","category":"section"},{"location":"man/fundamentals/#Factorisation-constraints","page":"Fundamentals","title":"Factorisation constraints","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"On a very high-level, ReactiveMP.jl is aimed to solve the Constrained Bethe Free Energy minimisation problem. For this task we approximate our exact posterior marginal distribution by some family of distributions q in mathcalQ. Often this involves assuming some factorization over q. For this purpose the @model macro supports optional where { ... } clauses for every ~ expression in a model specification.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random #hide\n\n@model function test_model6(n)\n    τ ~ GammaShapeRate(1.0, 1.0) \n    μ ~ NormalMeanVariance(0.0, 100.0)\n    \n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        # Here we assume a mean-field assumption on our \n        # variational family of distributions locally for the current node\n        y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) }\n    end\n    \n    return μ, τ, y\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"In this example we specified an extra constraints for q_a for Bethe factorisation:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"q(s) = prod_a in mathcalV q_a(s_a) prod_i in mathcalE q_i^-1(s_i)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"There are several options to specify the mean-field factorisation constraint. ","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) } # With names from model specification\ny[i] ~ NormalMeanPrecision(μ, τ) where { q = q(out)q(mean)q(precision) } # With names from node specification\ny[i] ~ NormalMeanPrecision(μ, τ) where { q = MeanField() } # With alias name","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"It is also possible to use local structured factorisation:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i], μ)q(τ) } # With names from model specification\ny[i] ~ NormalMeanPrecision(μ, τ) where { q = q(out, mean)q(precision) } # With names from node specification","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"As an option the @model macro accepts optional arguments for model specification, one of which is default_factorisation that accepts MeanField() as its argument for better convenience","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@model [ default_factorisation = MeanField() ] function test_model(...)\n    ...\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"This will autatically impose a mean field factorization constraint over all marginal distributions in our model.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"To run inference in this model we again need to create a synthetic dataset:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"real_mean = -3.0\nreal_prec = 5.0\nn         = 1000\n\ndataset = rand(Normal(real_mean, inv(sqrt(real_prec))), n)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"model, (μ, τ, y) = test_model6(length(dataset))\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"For variational inference we also usually need to set initial marginals for our inference procedure. For that purpose ReactiveMP.jl export the setmarginal! function:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"setmarginal!(μ, vague(NormalMeanPrecision))\nsetmarginal!(τ, vague(GammaShapeRate))","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"μ_values = keep(Marginal)\nτ_values = keep(Marginal)\n\nμ_subscription = subscribe!(getmarginal(μ), μ_values)\nτ_subscription = subscribe!(getmarginal(τ), τ_values)\n\nfor i in 1:10\n    update!(y, dataset)\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getvalues(μ_values)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getvalues(τ_values)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"println(\"μ: mean = \", mean(last(μ_values)), \", std = \", std(last(μ_values)))\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"println(\"τ: mean = \", mean(last(τ_values)), \", std = \", std(last(τ_values)))\nnothing #hide","category":"page"},{"location":"man/fundamentals/#Form-constraints","page":"Fundamentals","title":"Form constraints","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"In order to support form constraints, the randomvar() function also supports a where { ... } clause with some optional arguments. One of these arguments is form_constraint that allows us to specify a form constraint to the random variables in our model. Another one is prod_constraint that allows to specify an additional constraints during computation of product of two colliding messages. For example we can perform the EM algorithm if we assign a point mass contraint on some variables in our model.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random #hide\n\n@model function test_model7(n)\n    τ ~ GammaShapeRate(1.0, 1.0) \n    \n    # In case of form constraints `randomvar()` call is necessary\n    μ = randomvar() where { form_constraint = PointMassFormConstraint() }\n    μ ~ NormalMeanVariance(0.0, 100.0)\n    \n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) }\n    end\n    \n    return μ, τ, y\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"In this example we specified an extra constraints for q_i for Bethe factorisation:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"q(s) = prod_a in mathcalV q_a(s_a) prod_i in mathcalE q_i^-1(s_i)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"real_mean = -3.0\nreal_prec = 5.0\nn         = 1000\n\ndataset = rand(Normal(real_mean, inv(sqrt(real_prec))), n)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"model, (μ, τ, y) = test_model7(length(dataset))\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"setmarginal!(μ, vague(NormalMeanPrecision))\nsetmarginal!(τ, PointMass(1.0))\n\nμ_values = keep(Marginal)\nτ_values = keep(Marginal)\n\nμ_subscription = subscribe!(getmarginal(μ), μ_values)\nτ_subscription = subscribe!(getmarginal(τ), τ_values)\n\nfor i in 1:10\n    update!(y, dataset)\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getvalues(μ_values)","category":"page"},{"location":"man/fundamentals/#Product-constraints","page":"Fundamentals","title":"Product constraints","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"By default ReactiveMP.jl tries to compute an analytical product of two colliding messages and throws an error if no analytical solution is known. However, it is possible to fall back to a generic product that does not require an analytical solution to be known. In this case the inference backend will simply propagate the product of two message in a form of a tuple. It is not possible to use such a tuple-product during an inference and in this case it is mandatory to use some form constraint to approximate this product.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"μ = randomvar() where { \n    prod_constraint = ProdGeneric(),\n    form_constraint = SampleListFormConstraint() \n}","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Sometimes it is useful to preserve a specific parametrisation of the resulting product later on in an inference procedure. ReactiveMP.jl exports a special prod_constraint called ProdPreserveType especially for that purpose:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"μ = randomvar() where { prod_constraint = ProdPreserveType(NormalWeightedMeanPrecision) }","category":"page"},{"location":"man/fundamentals/#Free-Energy-Computation","page":"Fundamentals","title":"Free Energy Computation","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"During variational inference ReactiveMP.jl optimises a special functional called the Bethe Free Energy functional. It is possible to obtain its values for all VMP iterations with the score function.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"model, (μ, τ, y) = test_model6(length(dataset))\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"bfe_observable   = score(BetheFreeEnergy(), model)\nbfe_subscription = subscribe!(bfe_observable, (fe) -> println(\"Current BFE value: \", fe))\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# Reset the model with vague marginals\nsetmarginal!(μ, vague(NormalMeanPrecision))\nsetmarginal!(τ, vague(GammaShapeRate))\n\nfor i in 1:10\n    update!(y, dataset)\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# It always necessary to unsubscribe and release computer resources\nunsubscribe!([ μ_subscription, τ_subscription, bfe_subscription ])","category":"page"},{"location":"man/fundamentals/#Meta-data-specification","page":"Fundamentals","title":"Meta data specification","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"During model specification some functional dependencies may accept an optional meta object in the where { ... } clause. The purpose of the meta object is to adjust, modify or supply some extra information to the inference backend during the computations of the messages. The meta object for example may contain an approximation method that needs to be used during various approximations or it may specify the tradeoff between accuracy and performance:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# In this example the `meta` object for the autoregressive `AR` node specifies the variate type of \n# the autoregressive process and its order. In addition it specifies that the message computation rules should\n# respect accuracy over speed with the `ARsafe()` strategy. In contrast, `ARunsafe()` strategy tries to speedup computations\n# by cost of possible numerical instabilities during an inference procedure\ns[i] ~ AR(s[i - 1], θ, γ) where { q = q(s[i - 1], s[i])q(θ)q(γ), meta = ARMeta(Multivariate, order, ARsafe()) }\n...\ns[i] ~ AR(s[i - 1], θ, γ) where { q = q(s[i - 1], s[i])q(θ)q(γ), meta = ARMeta(Univariate, order, ARunsafe()) }","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Another example with GaussianControlledVariance, or simply GCV [see Hierarchical Gaussian Filter], node:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# In this example we specify structured factorisation and flag meta with `GaussHermiteCubature` \n# method with `21` sigma points for approximation of non-lineariety between hierarchy layers\nxt ~ GCV(xt_min, zt, real_k, real_w) where { q = q(xt, xt_min)q(zt)q(κ)q(ω), meta = GCVMetadata(GaussHermiteCubature(21)) }","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"The Meta object is useful to pass any extra information to a node that is not a random variable or constant model variable. It may include extra approximation methods, differentiation methods, optional non-linear functions, extra inference parameters etc.","category":"page"},{"location":"man/fundamentals/#user-guide-fundamentals-custom-nodes-rules","page":"Fundamentals","title":"Creating custom nodes and message computation rules","text":"","category":"section"},{"location":"man/fundamentals/#Custom-nodes","page":"Fundamentals","title":"Custom nodes","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"To create a custom functional form and to make it available during model specification ReactiveMP.jl exports the @node macro:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# `@node` macro accepts a name of the functional form, its type, either `Stochastic` or `Deterministic` and an array of interfaces:\n@node NormalMeanVariance Stochastic [ out, μ, v ]\n\n# Interfaces may have aliases for their names that might be convenient for factorisation constraints specification\n@node NormalMeanVariance Stochastic [ out, (μ, aliases = [ mean ]), (v, aliases = [ var ]) ]\n\n# `NormalMeanVariance` structure declaration must exist, otherwise `@node` macro will throw an error\nstruct NormalMeanVariance end \n\n@node NormalMeanVariance Stochastic [ out, μ, v ]\n\n# It is also possible to use function objects as a node functional form\nfunction dot end\n\n# Syntax for functions is a bit differet, as it is necesssary to use `typeof(...)` function for them \n# out = dot(x, a)\n@node typeof(dot) Deterministic [ out, x, a ]","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"note: Note\nDeterministic nodes do not support factorisation constraints with the where { q = ... } clause.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"After that it is possible to use the newly created node during model specification:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@model function test_model()\n    ...\n    y ~ dot(x, a)\n    ...\nend","category":"page"},{"location":"man/fundamentals/#Custom-messages-computation-rules","page":"Fundamentals","title":"Custom messages computation rules","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"ReactiveMP.jl exports the @rule macro to create custom message computation rules. For example let us create a simple + node to be available for usage in the model specification usage. We refer to A Factor Graph Approach to Signal Modelling , System Identification and Filtering [ Sascha Korl, 2005, page 32 ] for a rigorous explanation of the + node in factor graphs. According to Korl, assuming that inputs are Gaussian Sum-Product message computation rule for + node is the following:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"mu_z = mu_x + mu_y \nV_z = V_x + V_y","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"To specify this in ReactiveMP.jl we use the @node and @rule macros:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@node typeof(+) Deterministic  [ z, x, y ]\n\n@rule typeof(+)(:z, Marginalisation) (m_x::UnivariateNormalDistributionsFamily, m_y::UnivariateNormalDistributionsFamily) = begin\n    x_mean, x_var = mean_var(m_x)\n    y_mean, y_var = mean_var(m_y)\n    return NormalMeanVariance(x_mean + y_mean, x_var + y_var)\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"In this example, for the @rule macro, we specify a type of our functional form: typeof(+). Next, we specify an edge we are going to compute an outbound message for. Marginalisation indicates that the corresponding message respects the marginalisation constraint for posterior over corresponding edge:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"q(z) = int q(z x y) mathrmdxmathrmdy","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"If we look on difference between sum-product rules and variational rules with mean-field assumption we notice that they require different local information to compute an outgoing message:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"mu(z) = int f(x y z)mu(x)mu(y)mathrmdxmathrmdy","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"nu(z) = exp int log f(x y z)q(x)q(y)mathrmdxmathrmdy ","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"The @rule macro supports both cases with special prefixes during rule specification:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"m_ prefix corresponds to the incoming message on a specific edge\nq_ prefix corresponds to the posterior marginal of a specific edge","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Example of a Sum-Product rule with m_ messages used:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@rule NormalMeanPrecision(:μ, Marginalisation) (m_out::UnivariateNormalDistributionsFamily, m_τ::PointMass) = begin \n    m_out_mean, m_out_cov = mean_cov(m_out)\n    return NormalMeanPrecision(m_out_mean, inv(m_out_cov + inv(mean(m_τ))))\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Example of a Variational rule with Mean-Field assumption with q_ posteriors used:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@rule NormalMeanPrecision(:μ, Marginalisation) (q_out::Any, q_τ::Any) = begin \n    return NormalMeanPrecision(mean(q_out), mean(q_τ))\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"ReactiveMP.jl also supports structured rules. It is possible to obtain joint marginal over a set of edges:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@rule NormalMeanPrecision(:τ, Marginalisation) (q_out_μ::Any, ) = begin\n    m, V = mean_cov(q_out_μ)\n    θ = 2 / (V[1,1] - V[1,2] - V[2,1] + V[2,2] + abs2(m[1] - m[2]))\n    α = convert(typeof(θ), 1.5)\n    return Gamma(α, θ)\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"note: Note\nIn the @rule specification the messages or marginals arguments must be in order with interfaces specification from @node macro:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# Inference backend expects arguments in `@rule` macro to be in the same order\n@node NormalMeanPrecision Stochastic [ out, μ, τ ]","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Any rule always has access to the meta information with hidden the meta::Any variable:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any) = begin \n    ...\n    println(meta)\n    ...\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"It is also possible to dispatch on a specific type of a meta object:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::LaplaceApproximation) = begin \n    ...\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"or","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::GaussHermiteCubature) = begin \n    ...\nend","category":"page"},{"location":"man/fundamentals/#user-guide-fundamentals-pipeline","page":"Fundamentals","title":"Customizing messages computational pipeline","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"In certain situations it might be convenient to customize the default message computational pipeline. GrahpPPL.jl supports the pipeline keyword in the where { ... } clause to add some extra steps after a message has been computed. A use case might be an extra approximation method to preserve conjugacy in the model, debugging or simple printing.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# Logs all outbound messages\ny[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = LoggerPipelineStage() }\n# Initialise messages to be vague\ny[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = InitVaguePipelineStage() }\n# In principle, it is possible to approximate outbound messages with Laplace Approximation\ny[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = LaplaceApproximation() }","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Let us return to the coin toss model, but this time we want to print flowing messages:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random #hide\n\n@model function coin_toss_model_log(n)\n\n    y = datavar(Float64, n)\n\n    θ ~ Beta(2.0, 7.0) where { pipeline = LoggerPipelineStage(\"θ\") }\n\n    for i in 1:n\n        y[i] ~ Bernoulli(θ)  where { pipeline = LoggerPipelineStage(\"y[$i]\") }\n    end\n    \n    return y, θ\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"_, (y, θ) = coin_toss_model_log(5)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"θ_subscription = subscribe!(getmarginal(θ), (value) -> println(\"New posterior marginal for θ: \", value))\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"coinflips = float.(rand(Bernoulli(0.5), 5))\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"update!(y, coinflips)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"unsubscribe!(θ_subscription)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# Inference is lazy and does not send messages if no one is listening for them\nupdate!(y, coinflips)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"This tutorials covered the fundamentals of the ReactiveMP.jl package. For a more advanced usage we refer the interested reader to the other sections of the documentation.","category":"page"},{"location":"examples/flow_tutorial/#examples-flow","page":"Flow Tutorial","title":"Example: Flow tutorial","text":"","category":"section"},{"location":"examples/flow_tutorial/#Normalizing-flows:-a-tutorial","page":"Flow Tutorial","title":"Normalizing flows: a tutorial","text":"","category":"section"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"Table of contents","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"Introduction\nModel specification\nModel compilation\nProbabilistic inference\nParameter estimation","category":"page"},{"location":"examples/flow_tutorial/#examples-flow-introduction","page":"Flow Tutorial","title":"Introduction","text":"","category":"section"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"Normalizing flows are parameterized mappings of random variables, which map simple base distributions to more complex distributions. These mappings are constrained to be invertible and differentiable and can be composed of multiple simpler mappings for improved expressivity.","category":"page"},{"location":"examples/flow_tutorial/#Load-required-packages","page":"Flow Tutorial","title":"Load required packages","text":"","category":"section"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"Before we can start, we need to import some packages:","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"using ReactiveMP\nusing Rocket\nusing GraphPPL\nusing Random \n\nRandom.seed!(123)\n\nusing LinearAlgebra     # only used for some matrix specifics\nusing PyPlot            # only used for visualisation\nusing Distributions     # only used for sampling from multivariate distributions\nusing Optim             # only used for parameter optimisation","category":"page"},{"location":"examples/flow_tutorial/#examples-flow-model-specification","page":"Flow Tutorial","title":"Model specification","text":"","category":"section"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"Specifying a flow model is easy. The general recipe looks like follows: model = FlowModel(input_dim, (layer1(options), layer2(options), ...)). Here the first argument corresponds to the input dimension of the model and the second argument is a tuple of layers. An example flow model can be defined as","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"model = FlowModel(2,\n    (\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n)\nnothing #hide","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"Alternatively, the input_dim can also be passed as an InputLayer layer as ","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"model = FlowModel(\n    (\n        InputLayer(2),\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n)\nnothing #hide","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"In the above AdditiveCouplingLayer layers the input bfx = x_1 x_2 ldots x_N is partitioned into chunks of unit length. These partitions are additively coupled to an output bfy = y_1 y_2 ldots y_N as ","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"beginalign*\n    y_1 = x_1 \n    y_2 = x_2 + f_1(x_1) \n    vdots \n    y_N = x_N + f_N-1(x_N-1)\nendalign*","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"math","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"Importantly, this structure can easily be converted as ","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"beginalign*\n    x_1 = y_1 \n    x_2 = y_2 - f_1(x_1) \n    vdots \n    x_N = y_N - f_N-1(x_N-1)\nendalign*","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"f_n","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"is an arbitrarily complex function, here chosen to be a PlanarFlow, but this can be interchanged for any function or neural network. The permute keyword argument (which defaults to true) specifies whether the output of this layer should be randomly permuted or shuffled. This makes sure that the first element is also transformed in consecutive layers.","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"A permutation layer can also be added by itself as a PermutationLayer layer with a custom permutation matrix if desired.","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"model = FlowModel(\n    (\n        InputLayer(2),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false),\n        PermutationLayer(PermutationMatrix(2)),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n)\nnothing #hide","category":"page"},{"location":"examples/flow_tutorial/#examples-flow-model-compilation","page":"Flow Tutorial","title":"Model compilation","text":"","category":"section"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"In the current models, the layers are setup to work with the passed input dimension. This means that the function f_n is repeated input_dim-1 times for each of the partitions. Furthermore the permutation layers are set up with proper permutation matrices. If we print the model we get","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"model","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"The text below describes the terms above. Please note the distinction in typing and elements, i.e. FlowModel{types}(elements):","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"FlowModel - specifies that we are dealing with a flow model.\n3 - Number of layers.\nTuple{AdditiveCouplingLayerEmpty{...},PermutationLayer{Int64},AdditiveCouplingLayerEmpty{...}} - tuple of layer types.\nTuple{ReactiveMP.PlanarFlowEmpty{1},ReactiveMP.PlanarFlowEmpty{1}} - tuple of functions f_n.\nPermutationLayer{Int64}(2, [0 1; 1 0]) - permutation layer with input dimension 2 and permutation matrix [0 1; 1 0].","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"From inspection we can see that the AdditiveCouplingLayerEmpty and PlanarFlowEmpty objects are different than before. They are initialized for the correct dimension, but they do not have any parameters registered to them. This is by design to allow for separating the model specification from potential optimization procedures. Before we perform inference in this model, the parameters should be initialized. We can randomly initialize the parameters as","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"compiled_model = compile(model)","category":"page"},{"location":"examples/flow_tutorial/#examples-flow-probabilistic-inference","page":"Flow Tutorial","title":"Probabilistic inference","text":"","category":"section"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"We can perform inference in our compiled model through standard usage of ReactiveMP. Let's first generate some random 2D data which has been sampled from a standard normal distribution and is consecutively passed through a normalizing flow. Using the forward(model, data) function we can propagate data in the forward direction through the flow.","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"function generate_data(nr_samples::Int64, model::CompiledFlowModel)\n\n    # specify latent sampling distribution\n    dist = MvNormal([1.5, 0.5], I)\n\n    # sample from the latent distribution\n    x = rand(dist, nr_samples)\n\n    # transform data\n    y = zeros(Float64, size(x))\n    for k = 1:nr_samples\n        y[:,k] .= ReactiveMP.forward(model, x[:,k])\n    end\n\n    # return data\n    return y, x\n\nend","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"# generate data\ny, x = generate_data(1000, compiled_model)\n\n# plot generated data\n_, ax = plt.subplots(ncols=2, figsize=(15,5))\nax[1].scatter(x[1,:], x[2,:], alpha=0.3)\nax[2].scatter(y[1,:], y[2,:], alpha=0.3)\nax[1].set_title(\"Original data\")\nax[2].set_title(\"Transformed data\")\nax[1].grid(), ax[2].grid()\nplt.gcf()","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"The probabilistic model for doing inference can be described as ","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"@model function normalizing_flow(nr_samples::Int64, compiled_model::CompiledFlowModel)\n    \n    # initialize variables\n    z_μ   = randomvar()\n    z_Λ   = randomvar()\n    x     = randomvar(nr_samples)\n    y_lat = randomvar(nr_samples)\n    y     = datavar(Vector{Float64}, nr_samples)\n\n    # specify prior\n    z_μ ~ MvNormalMeanCovariance(zeros(2), huge*diagm(ones(2)))\n    z_Λ ~ Wishart(2.0, tiny*diagm(ones(2)))\n\n    # specify model\n    meta = FlowMeta(compiled_model) # defaults to FlowMeta(compiled_model; approximation=Linearization()). \n                                    # other approximation methods can be e.g. FlowMeta(compiled_model; approximation=Unscented(input_dim))\n\n    # specify observations\n    for k = 1:nr_samples\n\n        # specify latent state\n        x[k] ~ MvNormalMeanPrecision(z_μ, z_Λ) where { q = MeanField() }\n\n        # specify transformed latent value\n        y_lat[k] ~ Flow(x[k]) where { meta = meta }\n\n        # specify observations\n        y[k] ~ MvNormalMeanCovariance(y_lat[k], tiny*diagm(ones(2)))\n\n    end\n\n    # return variables\n    return z_μ, z_Λ, x, y_lat, y\n\nend","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"Here the flow model is passed inside a meta data object of the flow node. Inference then resorts to","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"function inference_flow(data_y::Array{Array{Float64,1},1}, compiled_model::CompiledFlowModel; nr_iterations::Int64=10)\n    \n    # fetch number of samples\n    nr_samples = length(data_y)\n\n    # define model\n    model, (z_μ, z_Λ, x, y_lat, y) = normalizing_flow(nr_samples, compiled_model)\n    \n    # initialize buffer for latent states\n    mzμ = keep(Marginal)\n    mzΛ = keep(Marginal)\n    mx  = buffer(Marginal, nr_samples)\n    my  = buffer(Marginal, nr_samples)\n\n    # initialize free energy\n    fe_values = Vector{Float64}()\n    \n    # subscribe to z\n    zμ_sub = subscribe!(getmarginal(z_μ), mzμ)\n    zΛ_sub = subscribe!(getmarginal(z_Λ), mzΛ)\n    x_sub  = subscribe!(getmarginals(x), mx)\n    y_sub  = subscribe!(getmarginals(y_lat), my)\n    fe_sub = subscribe!(score(BetheFreeEnergy(), model), (fe) -> push!(fe_values, fe))\n\n    # set initial marginals\n    setmarginal!(z_μ, MvNormalMeanCovariance(zeros(2), huge*diagm(ones(2))))\n    setmarginal!(z_Λ, Wishart(2.0, tiny*diagm(ones(2))))\n\n    # update y according to observations (i.e. perform inference)\n    for it = 1:nr_iterations\n        ReactiveMP.update!(y, data_y)\n    end\n\n    # unsubscribe\n    unsubscribe!([zμ_sub, zΛ_sub, x_sub, y_sub, fe_sub])\n    \n    # return the marginal values\n    return getvalues(mzμ)[end], getvalues(mzΛ)[end], getvalues(mx), getvalues(my), fe_values\n\nend;","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"The following line of code then executes the inference algorithm.","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"zμ_flow, zΛ_flow, x_flow, y_flow, fe_flow = inference_flow([y[:,k] for k=1:size(y,2)], compiled_model)\nnothing #hide","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"As we can see, the variational free energy decreases inside of our model.","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"plt.figure()\nplt.plot(1:10, fe_flow/size(y,2))\nplt.grid()\nplt.xlim(1,10)\nplt.xlabel(\"iteration\")\nplt.ylabel(\"normalized variational free energy [nats/sample]\")\nplt.gcf()","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"If we plot a random noisy observation and its approximated transformed uncertainty we obtain:","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"# pick a random observation\nid = rand(1:size(y,2))\nrand_observation = MvNormal(y[:,id], 5e-1*diagm(ones(2)))\nwarped_observation = MvNormal(ReactiveMP.backward(compiled_model, y[:,id]), ReactiveMP.inv_jacobian(compiled_model, y[:,id])*5e-1*diagm(ones(2))*ReactiveMP.inv_jacobian(compiled_model, y[:,id])');\n\n# plot inferred means and transformed point\nfig, ax = plt.subplots(ncols = 2, figsize=(15,5))\nax[1].scatter(x[1,:], x[2,:], alpha=0.1, label=\"generated data\")\nax[1].contour(repeat(-5:0.1:5, 1, 101), repeat(-5:0.1:5, 1, 101)', map( (x) -> pdf(MvNormal([1.5, 0.5], I), [x...]), collect(Iterators.product(-5:0.1:5, -5:0.1:5))), label=\"true distribution\")\nax[1].scatter(mean(zμ_flow)[1], mean(zμ_flow)[2], color=\"red\", marker=\"x\", label=\"inferred mean\")\nax[1].contour(repeat(-10:0.01:10, 1, 2001), repeat(-10:0.01:10, 1, 2001)', map( (x) -> pdf(warped_observation, [x...]), collect(Iterators.product(-10:0.01:10, -10:0.01:10))), colors=\"red\", levels=1)\nax[1].scatter(mean(warped_observation)..., color=\"red\", s=10, label=\"transformed noisy observation\")\nax[2].scatter(y[1,:], y[2,:], alpha=0.1, label=\"generated data\")\nax[2].scatter(ReactiveMP.forward(compiled_model, mean(zμ_flow))..., color=\"red\", marker=\"x\", label=\"inferred mean\")\nax[2].contour(repeat(-10:0.1:10, 1, 201), repeat(-10:0.1:10, 1, 201)', map( (x) -> pdf(MvNormal([1.5, 0.5], I), ReactiveMP.backward(compiled_model, [x...])), collect(Iterators.product(-10:0.1:10, -10:0.1:10))))\nax[2].contour(repeat(-10:0.1:10, 1, 201), repeat(-10:0.1:10, 1, 201)', map( (x) -> pdf(rand_observation, [x...]), collect(Iterators.product(-10:0.1:10, -10:0.1:10))), colors=\"red\", levels=1, label=\"random noisy observation\")\nax[2].scatter(mean(rand_observation)..., color=\"red\", s=10, label=\"random noisy observation\")\nax[1].grid(), ax[2].grid()\nax[1].set_xlim(-4,4), ax[1].set_ylim(-4,4), ax[2].set_xlim(-10,10), ax[2].set_ylim(-10,10)\nax[1].legend(), ax[2].legend()\nfig.suptitle(\"Generated data\")\nax[1].set_title(\"Latent distribution\"), ax[2].set_title(\"Observed distribution\")\nplt.gcf()","category":"page"},{"location":"examples/flow_tutorial/#examples-flow-parameter-estimation","page":"Flow Tutorial","title":"Parameter estimation","text":"","category":"section"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"The flow model is often used to learn unknown probabilistic mappings. Here we will demonstrate it as follows for a binary classification task with the following data:","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"function generate_data(nr_samples::Int64)\n\n    # sample weights\n    w = rand(nr_samples,2)\n\n    # sample appraisal\n    y = zeros(Float64, nr_samples)\n    for k = 1:nr_samples\n        y[k] = 1.0*(w[k,1] > 0.5)*(w[k,2] < 0.5)\n    end\n\n    # return data\n    return y, w\n\nend","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"data_y, data_x = generate_data(200);\nplt.figure()\nplt.scatter(data_x[:,1], data_x[:,2], c=data_y)\nplt.grid()\nplt.xlabel(\"w1\")\nplt.ylabel(\"w2\")\nplt.gcf()","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"We will then specify a possible flow model as","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"# specify flow model\nmodel = FlowModel(2,\n    (\n        AdditiveCouplingLayer(PlanarFlow()), # defaults to AdditiveCouplingLayer(PlanarFlow(); permute=true)\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"The corresponding probabilistic model for the binary classification task can be created as","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"@model function flow_classifier(nr_samples::Int64, model::FlowModel, params)\n    \n    # initialize variables\n    x_lat  = randomvar(nr_samples)\n    y_lat1 = randomvar(nr_samples)\n    y_lat2 = randomvar(nr_samples)\n    y      = datavar(Float64, nr_samples)\n    x      = datavar(Vector{Float64}, nr_samples)\n\n    # compile flow model\n    meta  = FlowMeta(compile(model, params)) # default: FlowMeta(model, Linearization())\n\n    # specify observations\n    for k = 1:nr_samples\n\n        # specify latent state\n        x_lat[k] ~ MvNormalMeanPrecision(x[k], 1e3*diagm(ones(2)))\n\n        # specify transformed latent value\n        y_lat1[k] ~ Flow(x_lat[k]) where { meta = meta }\n        y_lat2[k] ~ dot(y_lat1[k], [1, 1])\n\n        # specify observations\n        y[k] ~ Probit(y_lat2[k]) # default: where { pipeline = RequireInbound(in = NormalMeanPrecision(0, 1.0)) }\n\n    end\n\n    # return variables\n    return x_lat, x, y_lat1, y_lat2, y\n\nend","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"Here we see that the compilation occurs inside of our probabilistic model. As a result we can pass parameters (and a model) to this function which we wish to opmize for some criterium, such as the variational free energy. Inference can be described as","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"function inference_flow_classifier(data_y::Array{Float64,1}, data_x::Array{Array{Float64,1},1}, model::FlowModel, params)\n    \n    # fetch number of samples\n    nr_samples = length(data_y)\n\n    # define model\n    model, (x_lat, x, y_lat1, y_lat2, y) = flow_classifier(nr_samples, model, params)\n\n    # initialize free energy\n    fe_buffer = nothing\n    \n    # subscribe\n    fe_sub = subscribe!(score(BetheFreeEnergy(), model), (fe) -> fe_buffer = fe)\n\n    # update y and x according to observations (i.e. perform inference)\n    ReactiveMP.update!(y, data_y)\n    ReactiveMP.update!(x, data_x)\n\n    # unsubscribe\n    unsubscribe!(fe_sub)\n    \n    # return the marginal values\n    return fe_buffer\n\nend","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"For the optimization procedure, we will simplify our inference loop, such that it only accepts parameters as an argument (which is wishes to optimize) and outputs a performance metric.","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"function f(params)\n    fe = inference_flow_classifier(data_y, [data_x[k,:] for k=1:size(data_x,1)], model, params)\n    return fe\nend","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"Optimization can be performed using the Optim package. Alternatively, other (custom) optimizers can be implemented, such as:","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"res = optimize(f, randn(nr_params(model)), LBFGS(), Optim.Options(g_tol = 1e-3, iterations = 100, store_trace = true, show_trace = true)) - uses finitediff and is slower/less accurate.","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"or","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"# create gradient function\ng = (x) -> ForwardDiff.gradient(f, x);\n\n# specify initial params\nparams = randn(nr_params(model))\n\n# create custom optimizer (here Adam)\noptimizer = Adam(params; λ=1e-1)\n\n# allocate space for gradient\n∇ = zeros(nr_params(model))\n\n# perform optimization\nfor it = 1:10000\n\n    # backward pass\n    ∇ .= ForwardDiff.gradient(f, optimizer.x)\n\n    # gradient update\n    ReactiveMP.update!(optimizer, ∇)\n\nend\n","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"res = optimize(f, randn(nr_params(model)), LBFGS(), Optim.Options(store_trace = true, show_trace = true), autodiff=:forward)\nnothing #hide","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"optimization results are then given as","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"params = Optim.minimizer(res)\ninferred_model = compile(model, params)\ntrans_data_x_1 = hcat(map((x) -> ReactiveMP.forward(inferred_model, x), [data_x[k,:] for k=1:size(data_x,1)])...)'\ntrans_data_x_2 = map((x) -> dot([1, 1], x), [trans_data_x_1[k,:] for k=1:size(data_x,1)])\ntrans_data_x_2_split = [trans_data_x_2[data_y .== 1.0], trans_data_x_2[data_y .== 0.0]]\nfig, ax = plt.subplots(ncols = 3, figsize=(15,5))\nax[1].scatter(data_x[:,1], data_x[:,2], c = data_y)\nax[2].scatter(trans_data_x_1[:,1], trans_data_x_1[:,2], c = data_y)\nax[3].hist(trans_data_x_2_split; stacked=true, bins=50, color = [\"gold\", \"purple\"])\nax[1].grid(), ax[2].grid(), ax[3].grid()\nax[1].set_xlim(-0.25,1.25), ax[1].set_ylim(-0.25,1.25)\nax[1].set_title(\"original data\"), ax[2].set_title(\"|> warp\"), ax[3].set_title(\"|> dot\")\nplt.gcf()","category":"page"},{"location":"examples/flow_tutorial/","page":"Flow Tutorial","title":"Flow Tutorial","text":"using StatsFuns: normcdf\nclassification_map = map((x) -> normcdf(dot([1,1],x)), map((x) -> ReactiveMP.forward(inferred_model, [x...]), collect(Iterators.product(0:0.01:1, 0:0.01:1))))\nfig, ax = plt.subplots(ncols = 3, figsize=(20,5))\nim1 = ax[1].scatter(data_x[:,1], data_x[:,2], c = data_y)\nim2 = ax[2].scatter(data_x[:,1], data_x[:,2], c = normcdf.(trans_data_x_2))\nax[3].contour(repeat(0:0.01:1, 1, 101), repeat(0:0.01:1, 1, 101)', classification_map)\nplt.colorbar(im1, ax=ax[1])\nplt.colorbar(im2, ax=ax[2])\nax[1].grid(), ax[2].grid(), ax[3].grid()\nax[1].set_xlabel(\"weight 1\"), ax[1].set_ylabel(\"weight 2\"), ax[2].set_xlabel(\"weight 1\"), ax[2].set_ylabel(\"weight 2\"), ax[3].set_xlabel(\"weight 1\"), ax[3].set_ylabel(\"weight 2\")\nax[1].set_title(\"original labels\"), ax[2].set_title(\"predicted labels\"), ax[3].set_title(\"Classification map\")\nplt.gcf()","category":"page"},{"location":"examples/autoregressive/#examples-autoregressive","page":"Autoregressive Model","title":"Example: Autoregressive model","text":"","category":"section"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"In this example we are going to perform an automated Variational Bayesian Inference for autoregressive model that can be represented as following:","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"beginaligned\np(gamma) = mathrmGamma(gammaa b)\np(mathbftheta) = mathcalN(mathbfthetamathbfmu Sigma)\np(x_tmathbfx_t-1t-k) = mathcalN(x_tmathbftheta^Tmathbfx_t-1t-k gamma^-1)\np(y_tx_t) = mathcalN(y_tx_t tau^-1)\nendaligned","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"where x_t is a current state of our system, mathbfx_t-1t-k is a sequence of k previous states, k is an order of autoregression process, mathbftheta is a vector of transition coefficients, gamma is a precision of state transition process, y_k is a noisy observation of x_k with precision tau.","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"For a more rigorous introduction to Bayesian inference in Autoregressive models we refer to Albert Podusenko, Message Passing-Based Inference for Time-Varying Autoregressive Models.","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"We start with importing all needed packages:","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"using Rocket, ReactiveMP, GraphPPL\nusing Distributions, LinearAlgebra, Random, Plots, BenchmarkTools","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"Lets generate some synthetic dataset, we use a predefined set of coeffcients for k = 5:","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"# The following coefficients correspond to stable poles\ncoefs_ar_5 = [0.10699399235785655, -0.5237303489793305, 0.3068897071844715, -0.17232255282458891, 0.13323964347539288]\nnothing #hide","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"function generate_ar_data(rng, n, θ, γ, τ)\n    order        = length(θ)\n    states       = Vector{Vector{Float64}}(undef, n + 3order)\n    observations = Vector{Float64}(undef, n + 3order)\n    \n    γ_std = sqrt(inv(γ))\n    τ_std = sqrt(inv(γ))\n    \n    states[1] = randn(rng, order)\n    \n    for i in 2:(n + 3order)\n        states[i]       = vcat(rand(rng, Normal(dot(θ, states[i - 1]), γ_std)), states[i-1][1:end-1])\n        observations[i] = rand(rng, Normal(states[i][1], τ_std))\n    end\n    \n    return states[1+3order:end], observations[1+3order:end]\nend","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"# Seed for reproducibility\nseed = 123\nrng  = MersenneTwister(seed)\n\n# Number of observations in synthetic dataset\nn = 500\n\n# AR process parameters\nreal_γ = 5.0\nreal_τ = 5.0\nreal_θ = coefs_ar_5\n\nstates, observations = generate_ar_data(rng, n, real_θ, real_γ, real_τ)\nnothing #hide","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"Lets plot our synthetic dataset:","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"plot(first.(states), label = \"Hidden states\")\nscatter!(observations, label = \"Observations\")","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"Next step is to specify probabilistic model and run inference procedure with ReactiveMP. We use GraphPPL.jl package to specify probabilistic model and additional constraints for variational Bayesian Inference. We also specify two different models for Multivariate AR with order k > 1 and for Univariate AR (reduces to simple State-Space-Model) with order k = 1.","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"@model [ default_factorisation = MeanField() ] function lar_model(T::Type{ Multivariate }, n, order, c, stype, τ)\n    \n    # Parameter priors \n    γ  ~ GammaShapeRate(1.0, 1.0) \n    θ  ~ MvNormalMeanPrecision(zeros(order), diageye(order))\n    \n    # We create a sequence of random variables for hidden states\n    x = randomvar(n)\n    # As well a sequence of observartions\n    y = datavar(Float64, n)\n    \n    ct = constvar(c)\n    # We assume observation noise to be known\n    cτ = constvar(τ)\n    \n    # Prior for first state\n    x0 ~ MvNormalMeanPrecision(zeros(order), diageye(order))\n    \n    x_prev = x0\n    \n    # AR process requires extra meta information\n    meta = ARMeta(Multivariate, order, stype)\n    \n    for i in 1:n\n        # Autoregressive node uses structured factorisation assumption between states\n        x[i] ~ AR(x_prev, θ, γ) where { q = q(y, x)q(γ)q(θ), meta = meta }\n        y[i] ~ NormalMeanPrecision(dot(ct, x[i]), cτ)\n        x_prev = x[i]\n    end\n    \n    return x, y, θ, γ\nend","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"@model [ default_factorisation = MeanField() ] function lar_model(T::Type{Univariate}, n, order, c, stype, τ)\n    \n    # Parameter priors \n    γ  ~ GammaShapeRate(1.0, 1.0)\n    θ  ~ NormalMeanPrecision(0.0, 1.0)\n    \n    # We create a sequence of random variables for hidden states\n    x = randomvar(n)\n    # As well a sequence of observartions\n    y = datavar(Float64, n)\n    \n    ct = constvar(c)\n    # We assume observation noise to be known\n    cτ = constvar(τ) \n\n    # Prior for first state\n    x0 ~ NormalMeanPrecision(0.0, 1.0)\n    \n    x_prev = x0\n    \n    # AR process requires extra meta information\n    meta = ARMeta(Univariate, order, stype)\n    \n    for i in 1:n\n        x[i] ~ AR(x_prev, θ, γ) where { q = q(y, x)q(γ)q(θ), meta = meta }\n        y[i] ~ NormalMeanPrecision(ct * x[i], cτ)\n        x_prev = x[i]\n    end\n    \n    return x, y, θ, γ\nend","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"We will use different initial marginals depending on type of our AR process","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"function init_marginals!(::Type{ Multivariate }, order, γ, θ)\n    setmarginal!(γ, GammaShapeRate(1.0, 1.0))\n    setmarginal!(θ, MvNormalMeanPrecision(zeros(order), diageye(order)))\nend\n\nfunction init_marginals!(::Type{ Univariate }, order, γ, θ)\n    setmarginal!(γ, GammaShapeRate(1.0, 1.0))\n    setmarginal!(θ, NormalMeanPrecision(0.0, 1.0))\nend","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"function inference(data, order, artype, stype, niter, τ)\n    \n    # We build a full graph based on nber of observatios\n    n = length(data)\n    \n    # Depending on the order of AR process `c` is\n    # either a nber or a vector\n    c = ReactiveMP.ar_unit(artype, order)\n    \n    # Note that to run inference for huge model it might be necessary to pass extra \n    # options = (limit_stack_depth = 100,) to limit stack depth during recursive inference procedure\n    model, (x, y, θ, γ) = lar_model(artype, n, order, c, stype, τ)\n    \n    # We are going to keep `γ` and `θ` estimates for all VMP iterations\n    # But `buffer` only last posterior estimates for a sequence of hidden states `x`\n    # We also will keep Bethe Free Energy in `fe`\n    γ_buffer = keep(Marginal)\n    θ_buffer = keep(Marginal)\n    x_buffer = buffer(Marginal, n)\n    fe       = keep(Float64)\n    \n    γsub  = subscribe!(getmarginal(γ), γ_buffer)\n    θsub  = subscribe!(getmarginal(θ), θ_buffer)\n    xsub  = subscribe!(getmarginals(x), x_buffer)\n    fesub = subscribe!(score(Float64, BetheFreeEnergy(), model), fe)\n    \n    init_marginals!(artype, order, γ, θ)\n    \n    # We update data several times to perform several VMP iterations\n    for i in 1:niter\n        update!(y, data)\n    end\n    \n    # It is important to unsubscribe from running observables\n    unsubscribe!((γsub, θsub, xsub, fesub))\n    \n    return γ_buffer, θ_buffer, x_buffer, fe\nend","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"γ, θ, xs, fe = inference(observations, length(real_θ), Multivariate, ARsafe(), 15, real_τ)\nnothing #hide","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"p1 = plot(first.(states), label=\"Hidden state\")\np1 = scatter!(p1, observations, label=\"Observations\")\np1 = plot!(p1, first.(mean.(xs)), ribbon = sqrt.(first.(var.(xs))), label=\"Inferred states\", legend = :bottomright)\n\np2 = plot(mean.(γ), ribbon = std.(γ), label = \"Inferred transition precision\", legend = :bottomright)\np2 = plot!([ real_γ ], seriestype = :hline, label = \"Real transition precision\")\n\np3 = plot(getvalues(fe), label = \"Bethe Free Energy\")\n\nplot(p1, p2, p3, layout = @layout([ a; b c ]))","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"Lets also plot a subrange of our results:","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"subrange = div(n,5):(div(n, 5) + div(n, 5))\n\nplot(subrange, first.(states)[subrange], label=\"Hidden state\")\nscatter!(subrange, observations[subrange], label=\"Observations\")\nplot!(subrange, first.(mean.(xs))[subrange], ribbon = sqrt.(first.(var.(xs)))[subrange], label=\"Inferred states\", legend = :bottomright)","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"It is also interesting to see where our AR coefficients converge to:","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"let\n    pθ = plot()\n\n    θms = mean.(θ)\n    θvs = var.(θ)\n    \n    l = length(θms)\n\n    edim(e) = (a) -> map(r -> r[e], a)\n\n    for i in 1:length(first(θms))\n        pθ = plot!(pθ, θms |> edim(i), ribbon = θvs |> edim(i) .|> sqrt, label = \"Estimated θ[$i]\")\n    end\n    \n    for i in 1:length(real_θ)\n        pθ = plot!(pθ, [ real_θ[i] ], seriestype = :hline, label = \"Real θ[$i]\")\n    end\n    \n    plot(pθ, legend = :outertopright, size = (800, 300))\nend","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"println(\"$(length(real_θ))-order AR inference Bethe Free Energy: \", last(fe))\nnothing #hide","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"We can also run a 1-order AR inference on 5-order AR data:","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"γ, θ, xs, fe = inference(observations, 1, Univariate, ARsafe(), 15, real_τ)\nnothing #hide","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"println(\"1-order AR inference Bethe Free Energy: \", last(fe))\nnothing #hide","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"We can see that, according to final Bethe Free Energy value, in this example 5-order AR process can describe data better than 1-order AR.","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"We may be also interested in benchmarking our algorithm:","category":"page"},{"location":"examples/autoregressive/","page":"Autoregressive Model","title":"Autoregressive Model","text":"println(\"Benchmark for n = $n and AR-$(length(real_θ)) inference\");\n@benchmark inference($observations, length(real_θ), Multivariate, ARsafe(), 15, real_τ)","category":"page"},{"location":"extra/contributing/#Contribution-guidelines","page":"Contributing","title":"Contribution guidelines","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We welcome all possible contributors. This page details the some of the guidelines that should be followed when contributing to this package.","category":"page"},{"location":"extra/contributing/#Reporting-bugs","page":"Contributing","title":"Reporting bugs","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We track bugs using GitHub issues. We encourage you to write complete, specific, reproducible bug reports. Mention the versions of Julia and ReactiveMP for which you observe unexpected behavior. Please provide a concise description of the problem and complement it with code snippets, test cases, screenshots, tracebacks or any other information that you consider relevant. This will help us to replicate the problem and narrow the search space for solutions.","category":"page"},{"location":"extra/contributing/#Suggesting-features","page":"Contributing","title":"Suggesting features","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We welcome new feature proposals. However, before submitting a feature request, consider a few things:","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"Does the feature require changes in the core ReactiveMP.jl code? If it doesn't (for example, you would like to add a factor node for a particular application), you can add local extensions in your script/notebook or consider making a separate repository for your extensions.\nIf you would like to add an implementation of a feature that changes a lot in the core ReactiveMP.jl code, please open an issue on GitHub and describe your proposal first. This will allow us to discuss your proposal with you before you invest your time in implementing something that may be difficult to merge later on.","category":"page"},{"location":"extra/contributing/#Contributing-code","page":"Contributing","title":"Contributing code","text":"","category":"section"},{"location":"extra/contributing/#Installing-ReactiveMP","page":"Contributing","title":"Installing ReactiveMP","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We suggest that you use the dev command from the new Julia package manager to install ReactiveMP.jl for development purposes. To work on your fork of ReactiveMP.jl, use your fork's URL address in the dev command, for example:","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"] dev git@github.com:your_username/ReactiveMP.jl.git","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"The dev command clones ReactiveMP.jl to ~/.julia/dev/ReactiveMP. All local changes to ReactiveMP code will be reflected in imported code.","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"note: Note\nIt is also might be useful to install Revise.jl package as it allows you to modify code and use the changes without restarting Julia.","category":"page"},{"location":"extra/contributing/#Committing-code","page":"Contributing","title":"Committing code","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We use the standard GitHub Flow workflow where all contributions are added through pull requests. In order to contribute, first fork the repository, then commit your contributions to your fork, and then create a pull request on the master branch of the ReactiveMP.jl repository.","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"Before opening a pull request, please make sure that all tests pass without failing! All demos (can be found in /demo/ directory) and benchmarks (can be found in /benchmark/ directory) have to run without errors as well.","category":"page"},{"location":"extra/contributing/#Style-conventions","page":"Contributing","title":"Style conventions","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We use default Julia style guide. We list here a few important points and our modifications to the Julia style guide:","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"Use 4 spaces for indentation\nType names use UpperCamelCase. For example: AbstractFactorNode, RandomVariable, etc..\nFunction names are lowercase with underscores, when necessary. For example: activate!, randomvar, as_variable, etc..\nVariable names and function arguments use snake_case\nThe name of a method that modifies its argument(s) must end in !","category":"page"},{"location":"extra/contributing/#Unit-tests","page":"Contributing","title":"Unit tests","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We use the test-driven development (TDD) methodology for ReactiveMP.jl development. The test coverage should be as complete as possible. Please make sure that you write tests for each piece of code that you want to add.","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"All unit tests are located in the /test/ directory. The /test/ directory follows the structure of the /src/ directory. Each test file should have following filename format: test_*.jl. Some tests are also present in jldoctest docs annotations directly in the source code. See Julia's documentation about doctests.","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"The tests can be evaluated by running following command in the Julia REPL:","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"] test ReactiveMP","category":"page"},{"location":"lib/nodes/flow/#lib-nodes-flow","page":"Flow","title":"Flow node","text":"","category":"section"},{"location":"lib/nodes/flow/","page":"Flow","title":"Flow","text":"See also Flow tutorial for a comprehensive guide on using flows in ReactiveMP.","category":"page"},{"location":"lib/nodes/flow/","page":"Flow","title":"Flow","text":"PlanarFlow\nRadialFlow\nFlowModel\nCompiledFlowModel\ncompile\nAdditiveCouplingLayer\nInputLayer\nPermutationLayer\nFlowMeta","category":"page"},{"location":"lib/nodes/flow/#ReactiveMP.PlanarFlow","page":"Flow","title":"ReactiveMP.PlanarFlow","text":"The PlanarFlow function is defined as\n\nf(bfx) = bfx + bfu tanh(bfw^top bfx + b)\n\nwith input and output dimension D. Here bfxin mathbbR^D represents the input of the function. Furthermore bfuin mathbbR^D, bfwin mathbbR^D and binmathbbR represent the parameters of the function. The function contracts and expands the input space. \n\nThis function has been introduced in:\n\nRezende, Danilo, and Shakir Mohamed. \"Variational inference with normalizing flows.\" International conference on machine learning. PMLR, 2015.\n\n\n\n\n\n","category":"type"},{"location":"lib/nodes/flow/#ReactiveMP.RadialFlow","page":"Flow","title":"ReactiveMP.RadialFlow","text":"The RadialFlow function is defined as\n\nf(bfx) = bfx + fracbeta(bfz - bfz_0)alpha + bfz - bfz_0\n\nwith input and output dimension D. Here bfxin mathbbR^D represents the input of the function. Furthermore bfz_0in mathbbR^D, alphain mathbbR and betainmathbbR represent the parameters of the function. The function contracts and expands the input space. \n\nThis function has been introduced in:\n\nRezende, Danilo, and Shakir Mohamed. \"Variational inference with normalizing flows.\" International conference on machine learning. PMLR, 2015.\n\n\n\n\n\n","category":"type"},{"location":"lib/nodes/flow/#ReactiveMP.FlowModel","page":"Flow","title":"ReactiveMP.FlowModel","text":"The FlowModel structure is the most generic type of Flow model, in which the layers are not constrained to be of a specific type. The FlowModel structure contains the input dimensionality and a tuple of layers and can be constructed as FlowModel( dim, (layer1, layer2, ...) ).\n\nNote: this model can be specialized by constraining the types of layers. This potentially allows for more efficient specialized methods that can deal with specifics of these layers, such as triangular jacobian matrices.\n\n\n\n\n\n","category":"type"},{"location":"lib/nodes/flow/#ReactiveMP.CompiledFlowModel","page":"Flow","title":"ReactiveMP.CompiledFlowModel","text":"The CompiledFlowModel structure is the most generic type of compiled Flow model, in which the layers are not constrained to be of a specific type. The FlowModel structure contains the input dimension and a tuple of compiled layers. Do not manually create a CompiledFlowModel! Instead create a FlowModel first and compile it with compile(model::FlowModel). This will make sure that all layers/mappings are configured with the proper dimensionality and with randomly sampled parameters. Alternatively, if you would like to pass your own parameters, call compile(model::FlowModel, params::Vector).\n\nNote: this model can be specialized by constraining the types of layers. This potentially allows for more efficient specialized methods that can deal with specifics of these layers, such as triangular jacobian matrices.\n\n\n\n\n\n","category":"type"},{"location":"lib/nodes/flow/#ReactiveMP.compile","page":"Flow","title":"ReactiveMP.compile","text":"compile() compiles a model by setting its parameters. It randomly sets parameter values in the layers and flows such that inference in the model can be obtained.\n\nInput arguments\n\nmodel::FlowModel - a model of which the dimensionality of its layers/flows has been initialized, but its parameters have not been set.\n\nReturn arguments\n\n::CompiledFlowModel - a compiled model with set parameters, such that it can be used for processing data.\n\n\n\n\n\ncompile(model::FlowModel, params::Vector) lets you initialize a model model with a vector of parameters params.\n\nInput arguments\n\nmodel::FlowModel - a model of which the dimensionality of its layers/flows has been initialized, but its parameters have not been set.\nparams::Vector   - a vector of parameters with which the model should be compiled.\n\nReturn arguments\n\n::CompiledFlowModel - a compiled model with set parameters, such that it can be used for processing data.\n\n\n\n\n\n","category":"function"},{"location":"lib/nodes/flow/#ReactiveMP.AdditiveCouplingLayer","page":"Flow","title":"ReactiveMP.AdditiveCouplingLayer","text":"The additive coupling layer specifies an invertible function bfy = g(bfx) following the specific structure (for the mapping g mathbbR^2 rightarrow mathbbR^2):\n\n    beginalign\n        y_1 = x_1 \n        y_2 = x_2 + f(x_1)\n    endalign\n\nwhere f(cdot) denotes an arbitrary function with mapping f mathbbR rightarrow mathbbR. This function can be chosen arbitrarily complex. Non-linear functions (neural networks) are often chosen to model complex relationships. From the definition of the model, invertibility can be easily achieved as\n\n    beginalign\n        x_1 = y_1 \n        x_2 = y_2 - f(y_1)\n    endalign\n\nThe current implementation only allows for the mapping g mathbbR^2 rightarrow mathbbR^2, although this layer can be generalized for arbitrary input dimensions.\n\nAdditiveCouplingLayer(f <: AbstractCouplingFlow) creates the layer structure with function f.\n\nExample\n\nf = PlanarFlow()\nlayer = AdditiveCouplingLayer(f)\n\nThis layer structure has been introduced in:\n\nDinh, Laurent, David Krueger, and Yoshua Bengio. \"Nice: Non-linear independent components estimation.\" arXiv preprint arXiv:1410.8516 (2014).\n\n\n\n\n\n","category":"type"},{"location":"lib/nodes/flow/#ReactiveMP.InputLayer","page":"Flow","title":"ReactiveMP.InputLayer","text":"The input layer specifies the input dimension to a flow model.\n\nlayer = InputLayer(3)\n\n\n\n\n\n","category":"type"},{"location":"lib/nodes/flow/#ReactiveMP.PermutationLayer","page":"Flow","title":"ReactiveMP.PermutationLayer","text":"The permutation layer specifies an invertible mapping bfy = g(bfx) = Pbfx where P is a permutation matrix.\n\n\n\n\n\n","category":"type"},{"location":"lib/nodes/flow/#ReactiveMP.FlowMeta","page":"Flow","title":"ReactiveMP.FlowMeta","text":"The FlowMeta structure contains the meta data of the Flow factor node. More specifically, it contains the model of the Flow factor node. The FlowMeta structure can be constructed as FlowMeta(model). Make sure that the flow model has been compiled.\n\nThe FlowMeta structure is required for the Flow factor node and can be included with the Flow node as: y ~ Flow(x) where { meta = FlowMeta(...) }\n\n\n\n\n\n","category":"type"},{"location":"man/inference-execution/#user-guide-inference-execution","page":"Inference execution","title":"Inference execution","text":"","category":"section"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"This section explains how to use ReactiveMP reactive API for running inference on probabilistic models that were created with GraphPPL package as explained in Model Specification section.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"The ReactiveMP inference API supports different types of message-passing algorithms (including hybrid algorithms combining several different types):","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"Belief Propagation\nVariational Message Passing","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"Whereas belief propagation computes exact inference for the random variables of interest, the variational message passing (VMP) in an approximation method that can be applied to a larger range of models.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"The ReactiveMP engine itself isn't aware of different algorithm types and simply does message passing between nodes, however during model specification stage user may specifiy different factorisation constraints around factor nodes by using where { q = ... } syntax. Different factorisation constraints lead to a different message passing update rules.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"Inference with ReactiveMP usually consists of the same simple building blocks and designed in such a way to support both static and real-time infinite datasets:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"Create a model with @model macro and get a references to random variables and data inputs\nSubscribe to random variable posterior marginal updates \nSubscribe to Bethe Free Energy updates (optional)\nFeed model with observations \nUnsubscribe from posterior marginal updates (optional)","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"It is worth to note that Step 5 is optional and in case where observations come from an infinite real-time data stream (e.g. from the internet) it may be justified to never unsubscribe and perform real-time Bayesian inference in a reactive manner as soon as data arrives.","category":"page"},{"location":"man/inference-execution/#user-guide-inference-execution-model-creation","page":"Inference execution","title":"Model creation","text":"","category":"section"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"During model specification stage user decides on variables of interesets in a model and returns them using a return ... statement. As an example consider that we have a simple hierarchical model in which the mean of a Normal distribution is represented by another Normal distribution whose mean is modelled by another Normal distribution.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random\n\n@model function my_model()\n    m2 ~ NormalMeanVariance(0.0, 1.0)\n    m1 ~ NormalMeanVariance(m2, 1.0)\n\n    y = datavar(Float64)\n    y ~ NormalMeanVariance(m1, 1.0)\n\n    # Return variables of interests\n    return m1, y\nend","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"And later on we may create our model and obtain references for variables of interests:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"model, (m1, y) = my_model()\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"On the other hand, if we were interested in the posterior distributions of both m1 and m2 we would then need to return both of them from a model specification, i.e.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"@model function my_model()\n    ...\n    return m1, m2, y\nend\n\nmodel, (m1, m2, y) = my_model()","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"@model macro also return a reference for a factor graph as its first return value. Factor graph object (named model in previous example) contains all information about all factor nodes in a model as well as random variables and data inputs.","category":"page"},{"location":"man/inference-execution/#user-guide-inference-execution-marginal-updates","page":"Inference execution","title":"Posterior marginal updates","text":"","category":"section"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"The ReactiveMP package has a reactive API and operates in terms of Observables and Actors. For detailed information about these concepts we refer to Rocket.jl documentation.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"We use getmarginal function to get a posterior marginal updates observable:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"m1_posterior_updates = getmarginal(m1)\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"After that we can subscribe on new updates and perform some actions based on new values:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"m1_posterior_subscription = subscribe!(m1_posterior_updates, (new_posterior) -> begin\n    println(\"New posterior for m1: \", new_posterior)\nend)\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"Sometimes it is usefull to return an array of random variables from model specification, in this case we may use getmarginals() function that transform an array of observables to an observable of arrays.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"@model function my_model()\n    ...\n    m_n = randomvar(n)\n    ...\n    return m_n, ...\nend\n\nmodel, (m_n, ...) = my_model()\n\nm_n_updates = getmarginals(m_n)","category":"page"},{"location":"man/inference-execution/#user-guide-inference-execution-observations","page":"Inference execution","title":"Feeding observations","text":"","category":"section"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"By default (without any extra factorisation constraints) model specification implies Belief Propagation message passing update rules. In case of BP algorithm ReactiveMP package computes an exact Bayesian posteriors with a single message passing iteration. To enforce Belief Propagation message passing update rule for some specific factor node user may use where { q = FullFactorisation() } option. Read more in Model Specification section. To perform a message passing iteration we need to pass some data to all our data inputs that were created with datavar function during model specification.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"To feed an observation for a specific data input we use update! function:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"update!(y, 0.0)\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"As you can see after we passed a single value to our data input we got a posterior marginal update from our subscription and printed it with println function. In case of BP if observations do not change it should not affect posterior marginal results:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"update!(y, 0.0) # Observation didn't change, should result in the same posterior\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"If y is an array of data inputs it is possible to pass an array of observation to update! function:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"for i in 1:length(data)\n    update!(y[i], data[i])\nend\n# is an equivalent of\nupdate!(y, data)","category":"page"},{"location":"man/inference-execution/#user-guide-inference-vmp","page":"Inference execution","title":"Variational Message Passing","text":"","category":"section"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"Variational message passing (VMP) algorithms are generated much in the same way as the belief propagation algorithm we saw in the previous section. There is a major difference though: for VMP algorithm generation we need to define the factorization properties of our approximate distribution. A common approach is to assume that all random variables of the model factorize with respect to each other. This is known as the mean field assumption. In ReactiveMP, the specification of such factorization properties is defined during model specification stage using the where { q = ... } syntax. Let's take a look at a simple example to see how it is used. In this model we want to learn the mean and precision of a Normal distribution, where the former is modelled with a Normal distribution and the latter with a Gamma.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"real_mean      = -4.0\nreal_precision = 0.2\nrng            = MersenneTwister(1234)\n\nn    = 100\ndata = rand(rng, Normal(real_mean, sqrt(inv(real_precision))), n)\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"@model function normal_estimation(n)\n    m ~ NormalMeanVariance(0.0, 10.0)\n    w ~ Gamma(0.1, 10.0)\n\n    y = datavar(Float64, n)\n\n    for i in 1:n\n        y[i] ~ NormalMeanPrecision(m, w) where { q = MeanField() }\n    end\n\n    return m, w, y\nend","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"We create our model as usual, however in order to start VMP inference procedure we need to set initial posterior marginals for all random variables in the model:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"model, (m, w, y) = normal_estimation(n)\n\n# We use vague initial marginals\nsetmarginal!(m, vague(NormalMeanVariance)) \nsetmarginal!(w, vague(Gamma))\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"To perform a single VMP iteration it is enough to feed all data inputs with some values. To perform multiple VMP iterations we should feed our all data inputs with the same values multiple times:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"m_marginals = []\nw_marginals = []\n\nsubscriptions = subscribe!([\n    (getmarginal(m), (marginal) -> push!(m_marginals, marginal)),\n    (getmarginal(w), (marginal) -> push!(w_marginals, marginal)),\n])\n\nvmp_iterations = 10\n\nfor _ in 1:vmp_iterations\n    update!(y, data)\nend\n\nunsubscribe!(subscriptions)\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"As we process more VMP iterations, our beliefs about the possible values of m and w converge and become more confident.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"using Plots\n\np1    = plot(title = \"'Mean' posterior marginals\")\ngrid1 = -6.0:0.01:4.0\n\nfor iter in [ 1, 2, 10 ]\n\n    estimated = Normal(mean(m_marginals[iter]), std(m_marginals[iter]))\n    e_pdf     = (x) -> pdf(estimated, x)\n\n    plot!(p1, grid1, e_pdf, fill = true, opacity = 0.3, label = \"Estimated mean after $iter VMP iterations\")\n\nend\n\nplot!(p1, [ real_mean ], seriestype = :vline, label = \"Real mean\", color = :red4, opacity = 0.7)","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"p2    = plot(title = \"'Precision' posterior marginals\")\ngrid2 = 0.01:0.001:0.35\n\nfor iter in [ 2, 3, 10 ]\n\n    estimated = Gamma(shape(w_marginals[iter]), scale(w_marginals[iter]))\n    e_pdf     = (x) -> pdf(estimated, x)\n\n    plot!(p2, grid2, e_pdf, fill = true, opacity = 0.3, label = \"Estimated precision after $iter VMP iterations\")\n\nend\n\nplot!(p2, [ real_precision ], seriestype = :vline, label = \"Real precision\", color = :red4, opacity = 0.7)","category":"page"},{"location":"man/inference-execution/#user-guide-inference-vmp-bfe","page":"Inference execution","title":"Computing Bethe Free Energy","text":"","category":"section"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"VMP inference boils down to finding the member of a family of tractable probability distributions that is closest in KL divergence to an intractable posterior distribution. This is achieved by minimizing a quantity known as Variational Free Energy. ReactiveMP uses Bethe Free Energy approximation to the real Variational Free Energy. Free energy is particularly useful to test for convergence of the VMP iterative procedure.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"The ReactiveMP package exports score function for an observable of free energy values:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"fe_observable = score(BetheFreeEnergy(), model)\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"# Reset posterior marginals for `m` and `w`\nsetmarginal!(m, vague(NormalMeanVariance))\nsetmarginal!(w, vague(Gamma))\n\nfe_values = []\n\nfe_subscription = subscribe!(fe_observable, (v) -> push!(fe_values, v))\n\nvmp_iterations = 10\n\nfor _ in 1:vmp_iterations\n    update!(y, data)\nend\n\nunsubscribe!(fe_subscription)","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"plot(fe_values, label = \"Bethe Free Energy\", xlabel = \"Iteration #\")","category":"page"},{"location":"#ReactiveMP.jl","page":"Introduction","title":"ReactiveMP.jl","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Julia package for automatic Bayesian inference on a factor graph with reactive message passing.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Given a probabilistic model, ReactiveMP allows for an efficient message-passing based Bayesian inference. It uses the model structure to generate an algorithm that consists of a sequence of local computations on a Forney-style factor graph (FFG) representation of the model.","category":"page"},{"location":"#Package-Features","page":"Introduction","title":"Package Features","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"User friendly syntax for specification of probabilistic models.\nAutomatic generation of message passing algorithms including\nBelief propagation\nVariational message passing\nExpectation maximization\nSupport for hybrid models combining discrete and continuous latent variables.\nSupport for hybrid distinct message passing inference algorithm under a unified paradigm.\nEvaluation of Bethe free energy as a model performance measure.\nSchedule-free reactive message passing API.\nHigh performance.\nScalability for large models with millions of parameters and observations.\nInference procedure is differentiable.\nEasy to extend with custom nodes.","category":"page"},{"location":"#Resources","page":"Introduction","title":"Resources","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"For an introduction to message passing and FFGs, see The Factor Graph Approach to Model-Based Signal Processing by Loeliger et al. (2007).","category":"page"},{"location":"#How-to-get-started?","page":"Introduction","title":"How to get started?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Head to the Getting started section to get up and running with ForneyLab. Alternatively, explore various examples in the documentation.","category":"page"},{"location":"#Table-of-Contents","page":"Introduction","title":"Table of Contents","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Pages = [\n  \"man/getting-started.md\",\n  \"man/fundamentals.md\",\n  \"man/model-specification.md\",\n  \"examples/overview.md\",\n  \"lib/message.md\",\n  \"lib/node.md\",\n  \"lib/math.md\",\n  \"extra/contributing.md\"\n]\nDepth = 2","category":"page"},{"location":"#Index","page":"Introduction","title":"Index","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"lib/helpers/#lib-helpers","page":"Helper utils","title":"Helper utilities","text":"","category":"section"},{"location":"lib/helpers/#lib-one-div-n-vector","page":"Helper utils","title":"OneDivNVector","text":"","category":"section"},{"location":"lib/helpers/","page":"Helper utils","title":"Helper utils","text":"Helper utilities implement OneDivNVector structure that is allocation free equivalent of fill(1 / N, N) collection. Mostly used in SampleList implementation.","category":"page"},{"location":"lib/helpers/","page":"Helper utils","title":"Helper utils","text":"DocTestSetup = quote\n    using ReactiveMP\n    import ReactiveMP: OneDivNVector\nend","category":"page"},{"location":"lib/helpers/","page":"Helper utils","title":"Helper utils","text":"ReactiveMP.OneDivNVector","category":"page"},{"location":"lib/helpers/#ReactiveMP.OneDivNVector","page":"Helper utils","title":"ReactiveMP.OneDivNVector","text":"OneDivNVector(N::Int)\nOneDivNVector(::Type{T}, N::Int) where T\n\nAllocation-free version of fill(one(T) / N, N) vector.\n\nArguments\n\n::Type{T}: type of elements, optional, Float64 by default, should be a subtype of Number\nN::Int: number of elements in a container, should be greater than zero\n\nExamples\n\njulia> iter = ReactiveMP.OneDivNVector(3)\n3-element ReactiveMP.OneDivNVector{3, Float64}:\n 0.3333333333333333\n 0.3333333333333333\n 0.3333333333333333\n\njulia> length(iter)\n3\n\njulia> eltype(iter)\nFloat64\n\njulia> collect(iter)\n3-element Vector{Float64}:\n 0.3333333333333333\n 0.3333333333333333\n 0.3333333333333333\n\njulia> iter = ReactiveMP.OneDivNVector(Float32, 3)\n3-element ReactiveMP.OneDivNVector{3, Float32}:\n 0.33333334\n 0.33333334\n 0.33333334\n\njulia> collect(iter)\n3-element Vector{Float32}:\n 0.33333334\n 0.33333334\n 0.33333334\n\nSee also: SampleList\n\n\n\n\n\n","category":"type"},{"location":"lib/helpers/","page":"Helper utils","title":"Helper utils","text":"DocTestSetup = nothing","category":"page"}]
}
