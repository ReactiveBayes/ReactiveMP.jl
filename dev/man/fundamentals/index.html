<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Fundamentals · ReactiveMP.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">ReactiveMP.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Introduction</a></li><li><span class="tocitem">User guide</span><ul><li><a class="tocitem" href="../getting-started/">Getting Started</a></li><li class="is-active"><a class="tocitem" href>Fundamentals</a><ul class="internal"><li><a class="tocitem" href="#user-guide-fundamentals-model-syntax"><span>General model specification syntax</span></a></li><li><a class="tocitem" href="#user-guide-fundamentals-inference"><span>Probabilistic inference in ReactiveMP.jl</span></a></li><li><a class="tocitem" href="#user-guide-fundamentals-reactive-inference"><span>Reactive inference in ReactiveMP.jl</span></a></li><li><a class="tocitem" href="#user-guide-fundamentals-vmp-inference"><span>Variational inference in ReactiveMP.jl</span></a></li><li><a class="tocitem" href="#user-guide-fundamentals-custom-nodes-rules"><span>Creating custom nodes and message computation rules</span></a></li><li><a class="tocitem" href="#user-guide-fundamentals-pipeline"><span>Customizing messages computational pipeline</span></a></li></ul></li><li><a class="tocitem" href="../model-specification/">Model Specification</a></li><li><a class="tocitem" href="../inference-execution/">Inference execution</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../../examples/overview/">Overview</a></li><li><a class="tocitem" href="../../examples/linear_gaussian_state_space_model/">Linear Gaussian Dynamical System</a></li><li><a class="tocitem" href="../../examples/hierarchical_gaussian_filter/">Hierarchical Gaussian Filter</a></li><li><a class="tocitem" href="../../examples/autoregressive/">Autoregressive Model</a></li></ul></li><li><span class="tocitem">Library</span><ul><li><a class="tocitem" href="../../lib/message/">Messages</a></li><li><a class="tocitem" href="../../lib/node/">Factor nodes</a></li><li><a class="tocitem" href="../../lib/math/">Math utils</a></li><li><a class="tocitem" href="../../lib/helpers/">Helper utils</a></li></ul></li><li><a class="tocitem" href="../../extra/contributing/">Contributing</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">User guide</a></li><li class="is-active"><a href>Fundamentals</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Fundamentals</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/biaslab/ReactiveMP.jl/blob/master/docs/src/man/fundamentals.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="user-guide-fundamentals"><a class="docs-heading-anchor" href="#user-guide-fundamentals">Fundamentals</a><a id="user-guide-fundamentals-1"></a><a class="docs-heading-anchor-permalink" href="#user-guide-fundamentals" title="Permalink"></a></h1><p>This tutorials covers the fundamentals of the ReactiveMP.jl package. For a more advanced usage we refer the interested reader to the other sections of the documentation.</p><p>This tutorial also exists in the form of a Jupyter notebook in <a href="https://github.com/biaslab/ReactiveMP.jl/tree/master/demo">demo/</a> folder at GitHub repository.</p><p>First lets setup our environment by importing all needed packages:</p><pre><code class="language-julia hljs">using Rocket, GraphPPL, ReactiveMP, Distributions, Random</code></pre><h2 id="user-guide-fundamentals-model-syntax"><a class="docs-heading-anchor" href="#user-guide-fundamentals-model-syntax">General model specification syntax</a><a id="user-guide-fundamentals-model-syntax-1"></a><a class="docs-heading-anchor-permalink" href="#user-guide-fundamentals-model-syntax" title="Permalink"></a></h2><p>We use the <code>@model</code> macro from the <code>GraphPPL.jl</code> package to create a probabilistic model <span>$p(s, y)$</span> and to specify extra constraints on the variational family of distributions <span>$\mathcal{Q}$</span>, used for approximating intractable posterior distributions. Below there is a simple example of the general syntax for model specification. In this tutorial we do not cover all possible ways to create models or advanced features of <code>GraphPPL.jl</code>.  Instead we refer the interested reader to the <a href="../model-specification/#user-guide-model-specification">Model specification</a> section for a more rigorous explanation and illustrative examples.</p><pre><code class="language-julia hljs"># the `@model` macro accepts a regular Julia function
@model function test_model1(s_mean, s_precision)

    # We use the `randomvar` function to create
    # a random variable in our model
    s = randomvar()

    # the `tilde` operator creates a functional dependency
    # between variables in our model and can be read as
    # `sampled from` or `is modeled by`
    s ~ GaussianMeanPrecision(s_mean, s_precision)

    # We use the `datavar` function to create
    # observed data variables in our models
    # We also need to specify the type of our data
    # In this example it is `Float64`
    y = datavar(Float64)

    y ~ GaussianMeanPrecision(s, 1.0)

    return s, y
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">test_model1 (generic function with 1 method)</code></pre><p>The <code>@model</code> macro creates a function with the same name and with the same set of input arguments as the original function (<code>test_model1(s_mean, s_precision)</code> in this example). However, the return value is modified in such a way to contain a reference to the model object as the first value and to the user specified variables in the form of a tuple as the second value.</p><pre><code class="language-julia hljs">model, (s, y) = test_model1(0.0, 1.0)</code></pre><p>Later on we can examine our model structure with the help of some utility functions such as: </p><ul><li><code>getnodes()</code>: returns an array of factor nodes in a correposning factor graph</li><li><code>getrandom()</code>: returns an array of random variable in the model</li><li><code>getdata()</code>: returns an array of data inputs in the model</li><li><code>getconstant()</code>: return an array of constant values in the model</li></ul><pre><code class="language-julia hljs">getnodes(model)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{ReactiveMP.AbstractFactorNode}:
 FactorNode:
 form            : NormalMeanPrecision
 sdtype          : Stochastic()
 interfaces      : (Interface(out, Marginalisation()), Interface(μ, Marginalisation()), Interface(τ, Marginalisation()))
 factorisation   : ((1, 2, 3),)
 local marginals : (:out_μ_τ,)
 metadata        : nothing
 pipeline        : FactorNodePipeline(functional_dependencies = DefaultFunctionalDependencies(), extra_stages = EmptyPipelineStage()

 FactorNode:
 form            : NormalMeanPrecision
 sdtype          : Stochastic()
 interfaces      : (Interface(out, Marginalisation()), Interface(μ, Marginalisation()), Interface(τ, Marginalisation()))
 factorisation   : ((1, 2, 3),)
 local marginals : (:out_μ_τ,)
 metadata        : nothing
 pipeline        : FactorNodePipeline(functional_dependencies = DefaultFunctionalDependencies(), extra_stages = EmptyPipelineStage()
</code></pre><pre><code class="language-julia hljs">getrandom(model) .|&gt; name</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1-element Vector{Symbol}:
 :s</code></pre><pre><code class="language-julia hljs">getdata(model) .|&gt; name</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1-element Vector{Symbol}:
 :y</code></pre><pre><code class="language-julia hljs">getconstant(model) .|&gt; getconst</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3-element Vector{Float64}:
 0.0
 1.0
 1.0</code></pre><p>It is also possible to use control flow statements such as <code>if</code> or <code>for</code> blocks in the model specification function. In general, any valid snippet of Julia code can be used inside the <code>@model</code> block. As an example consider the following (valid!) model:</p><pre><code class="language-julia hljs">@model function test_model2(n)

    if n &lt;= 1
        error(&quot;`n` argument must be greater than one.&quot;)
    end

    # `randomvar(n)` creates a dense sequence of
    # random variables
    s = randomvar(n)

    # `datavar(Float64, n)` creates a dense sequence of
    # observed data variables of type `Float64`
    y = datavar(Float64, n)

    s[1] ~ GaussianMeanPrecision(0.0, 0.1)
    y[1] ~ GaussianMeanPrecision(s[1], 1.0)

    for i in 2:n
        s[i] ~ GaussianMeanPrecision(s[i - 1], 1.0)
        y[i] ~ GaussianMeanPrecision(s[i], 1.0)
    end

    return s, y
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">test_model2 (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">model, (s, y) = test_model2(10)</code></pre><pre><code class="language-julia hljs"># An amount of factor nodes in generated Factor Graph
getnodes(model) |&gt; length</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">20</code></pre><pre><code class="language-julia hljs"># An amount of random variables
getrandom(model) |&gt; length</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10</code></pre><pre><code class="language-julia hljs"># An amount of data inputs
getdata(model) |&gt; length</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10</code></pre><pre><code class="language-julia hljs"># An amount of constant values
getconstant(model) |&gt; length</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">21</code></pre><p>It is also possible to use complex expression inside the functional dependency expressions</p><pre><code class="language-julia hljs">y ~ NormalMeanPrecision(2.0 * (s + 1.0), 1.0)</code></pre><p>The <code>~</code> operator automatically creates a random variable if none was created before with the same name and throws an error if this name already exists</p><pre><code class="language-julia hljs"># s = randomvar() here is optional
# `~` creates random variables automatically
s ~ NormalMeanPrecision(0.0, 1.0)</code></pre><p>An example model which will throw an error:</p><pre><code class="language-julia hljs">@model function error_model1()
    s = 1.0
    s ~ NormalMeanPrecision(0.0, 1.0)
end</code></pre><pre><code class="nohighlight hljs">LoadError: Invalid name &#39;s&#39; for new random variable. &#39;s&#39; was already initialized with &#39;=&#39; operator before.</code></pre><p>By default the <code>GraphPPL.jl</code> package creates new references for constants (literals like <code>0.0</code> or <code>1.0</code>) in a model. In some situations this may not be efficient, especially when these constants represent large matrices. <code>GraphPPL.jl</code> will by default create new copies of some constant (e.g. matrix) in a model every time it uses it. However it is possible to use <code>constvar()</code> function to create and reuse similar constants in the model specification syntax as</p><pre><code class="language-julia hljs"># Creates constant reference in a model with a prespecified value
c = constvar(0.0)</code></pre><p>An example:</p><pre><code class="language-julia hljs">@model function test_model5(dim::Int, n::Int, A::Matrix, P::Matrix, Q::Matrix)

    s = randomvar(n)

    y = datavar(Vector{Float64}, n)

    # Here we create constant references
    # for constant matrices in our model
    # to make inference more memory efficient
    cA = constvar(A)
    cP = constvar(P)
    cQ = constvar(Q)

    s[1] ~ MvGaussianMeanCovariance(zeros(dim), cP)
    y[1] ~ MvGaussianMeanCovariance(s[1], cQ)

    for i in 2:n
        s[i] ~ MvGaussianMeanCovariance(cA * s[i - 1], cP)
        y[i] ~ MvGaussianMeanCovariance(s[i], cQ)
    end

    return s, y
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">test_model5 (generic function with 1 method)</code></pre><p>The <code>~</code> expression can also return a reference to a newly created node in a corresponding factor graph for convenience in later usage:</p><pre><code class="language-julia hljs">@model function test_model()

    # In this example `ynode` refers to the corresponding 
    # `GaussianMeanVariance` node created in the factor graph
    ynode, y ~ GaussianMeanVariance(0.0, 1.0)
    
    return ynode, y
end</code></pre><h2 id="user-guide-fundamentals-inference"><a class="docs-heading-anchor" href="#user-guide-fundamentals-inference">Probabilistic inference in ReactiveMP.jl</a><a id="user-guide-fundamentals-inference-1"></a><a class="docs-heading-anchor-permalink" href="#user-guide-fundamentals-inference" title="Permalink"></a></h2><p><code>ReactiveMP.jl</code> uses the <code>Rocket.jl</code> package API for inference routines. <code>Rocket.jl</code> is a reactive programming extension for Julia that is higly inspired by <code>RxJS</code> and similar libraries from the <code>Rx</code> ecosystem. It consists of <strong>observables</strong>, <strong>actors</strong>, <strong>subscriptions</strong> and <strong>operators</strong>. For more infromation and rigorous examples see <a href="https://github.com/biaslab/Rocket.jl">Rocket.jl github page</a>.</p><h3 id="Observables"><a class="docs-heading-anchor" href="#Observables">Observables</a><a id="Observables-1"></a><a class="docs-heading-anchor-permalink" href="#Observables" title="Permalink"></a></h3><p>Observables are lazy push-based collections and they deliver their values over time.</p><pre><code class="language-julia hljs"># Timer that emits a new value every second and has an initial one second delay 
observable = timer(1000, 1000)</code></pre><p>A subscription allows us to subscribe on future values of some observable, and actors specify what to do with these new values:</p><pre><code class="language-julia hljs">actor = (value) -&gt; println(value)
subscription1 = subscribe!(observable, actor)</code></pre><pre><code class="nohighlight hljs">0
1
2
3
...</code></pre><pre><code class="language-julia hljs"># We always need to unsubscribe from some observables
unsubscribe!(subscription1)</code></pre><pre><code class="language-julia hljs"># We can modify our observables
modified = observable |&gt; filter(d -&gt; rem(d, 2) === 1) |&gt; map(Int, d -&gt; d ^ 2)</code></pre><pre><code class="language-julia hljs">subscription2 = subscribe!(modified, (value) -&gt; println(value))</code></pre><pre><code class="nohighlight hljs">1
9
25
...</code></pre><pre><code class="language-julia hljs">unsubscribe!(subscription2)</code></pre><p>The <code>ReactiveMP.jl</code> package returns posterior marginal distributions in our specified model in the form of an observable. It is possible to subscribe on its future updates, but for convenience <code>ReactiveMP.jl</code> only caches the last obtained values of all marginals in a model. To get a reference for the posterior marginal of some random variable in a model <code>ReactiveMP.jl</code> exports two functions: </p><ul><li><code>getmarginal(x)</code>: for a single random variable <code>x</code></li><li><code>getmarginals(xs)</code>: for a dense sequence of random variables <code>sx</code></li></ul><p>Lets see how it works in practice. Here we create a simple coin toss model. We assume that observations are governed by the <code>Bernoulli</code> distribution with unknown bias parameter <code>θ</code>. To have a fully Bayesian treatment of this problem we endow <code>θ</code> with the <code>Beta</code> prior.</p><pre><code class="language-julia hljs">@model function coin_toss_model(n)

    # `datavar` creates data &#39;inputs&#39; in our model
    # We will pass data later on to these inputs
    # In this example we create a sequence of inputs that accepts Float64
    y = datavar(Float64, n)

    # We endow θ parameter of our model with some prior
    θ ~ Beta(2.0, 7.0)

    # We assume that the outcome of each coin flip
    # is modeled by a Bernoulli distribution
    for i in 1:n
        y[i] ~ Bernoulli(θ)
    end

    # We return references to our data inputs and θ parameter
    # We will use these references later on during the inference step
    return y, θ
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">coin_toss_model (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">_, (y, θ) = coin_toss_model(500)</code></pre><pre><code class="language-julia hljs"># As soon as we have a new value for the marginal posterior over the `θ` variable
# we simply print the first two statistics of it
θ_subscription = subscribe!(getmarginal(θ), (marginal) -&gt; println(&quot;New update: mean(θ) = &quot;, mean(marginal), &quot;, std(θ) = &quot;, std(marginal)));</code></pre><p>Next, lets define our dataset:</p><pre><code class="language-julia hljs">p       = 0.75 # Bias of a coin
dataset = float.(rand(Bernoulli(p), 500));</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">500-element Vector{Float64}:
 1.0
 0.0
 1.0
 0.0
 1.0
 1.0
 1.0
 0.0
 1.0
 0.0
 ⋮
 0.0
 1.0
 1.0
 1.0
 1.0
 1.0
 0.0
 1.0
 1.0</code></pre><p>To pass data to our model we use <code>update!</code> function</p><pre><code class="language-julia hljs">update!(y, dataset)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">New update: mean(θ) = 0.7583497053045186, std(θ) = 0.018955853241049647</code></pre><pre><code class="language-julia hljs"># It is necessary to always unsubscribe from running observables
unsubscribe!(θ_subscription)</code></pre><pre><code class="language-julia hljs"># The ReactiveMP.jl inference backend is lazy and does not compute posterior marginals if no-one is listening for them
# At this moment we have already unsubscribed from the new posterior updates so this `update!` does nothing
update!(y, dataset)</code></pre><p><code>Rocket.jl</code> provides some useful built-in actors for obtaining posterior marginals especially with static datasets.</p><pre><code class="language-julia hljs"># the `keep` actor simply keeps all incoming updates in an internal storage, ordered
θvalues = keep(Marginal)</code></pre><pre><code class="language-julia hljs"># `getmarginal` always emits last cached value as its first value
subscribe!(getmarginal(θ) |&gt; take(1), θvalues)</code></pre><pre><code class="language-julia hljs">getvalues(θvalues)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1-element Vector{Marginal}:
 Marginal(Beta{Float64}(α=386.0, β=123.0))</code></pre><pre><code class="language-julia hljs"># `getmarginal` always emits last cached value as its first value
subscribe!(getmarginal(θ) |&gt; take(1), θvalues)</code></pre><pre><code class="language-julia hljs">getvalues(θvalues)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Marginal}:
 Marginal(Beta{Float64}(α=386.0, β=123.0))
 Marginal(Beta{Float64}(α=386.0, β=123.0))</code></pre><pre><code class="language-julia hljs"># the `buffer` actor keeps very last incoming update in an internal storage and can also store
# an array of updates for a sequence of random variables
θbuffer = buffer(Marginal, 1)</code></pre><pre><code class="language-julia hljs">subscribe!(getmarginals([ θ ]) |&gt; take(1), θbuffer);</code></pre><pre><code class="language-julia hljs">getvalues(θbuffer)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1-element Vector{Marginal}:
 Marginal(Beta{Float64}(α=386.0, β=123.0))</code></pre><pre><code class="language-julia hljs">subscribe!(getmarginals([ θ ]) |&gt; take(1), θbuffer);</code></pre><pre><code class="language-julia hljs">getvalues(θbuffer)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1-element Vector{Marginal}:
 Marginal(Beta{Float64}(α=386.0, β=123.0))</code></pre><h2 id="user-guide-fundamentals-reactive-inference"><a class="docs-heading-anchor" href="#user-guide-fundamentals-reactive-inference">Reactive inference in ReactiveMP.jl</a><a id="user-guide-fundamentals-reactive-inference-1"></a><a class="docs-heading-anchor-permalink" href="#user-guide-fundamentals-reactive-inference" title="Permalink"></a></h2><p>ReactiveMP.jl naturally supports reactive streams of data and it is possible to run reactive inference with some external datasource.</p><pre><code class="language-julia hljs">@model function online_coin_toss_model()

    # We create datavars for the prior
    # over `θ` variable
    θ_a = datavar(Float64)
    θ_b = datavar(Float64)

    θ ~ Beta(θ_a, θ_b)

    y = datavar(Float64)
    y ~ Bernoulli(θ)

    return θ_a, θ_b, θ, y
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">online_coin_toss_model (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">_, (θ_a, θ_b, θ, y) = online_coin_toss_model()</code></pre><pre><code class="language-julia hljs"># In this example we subscribe on posterior marginal of θ variable and use it as a prior for our next observation
# We also print into stdout for convenience
θ_subscription = subscribe!(getmarginal(θ), (m) -&gt; begin
    m_a, m_b = params(m)
    update!(θ_a, m_a)
    update!(θ_b, m_b)
    println(&quot;New posterior for θ: mean = &quot;, mean(m), &quot;, std = &quot;, std(m))
end)</code></pre><pre><code class="language-julia hljs"># Initial priors
update!(θ_a, 10.0 * rand())
update!(θ_b, 10.0 * rand())</code></pre><pre><code class="language-julia hljs">data_source = timer(500, 500) |&gt; map(Float64, (_) -&gt; float(rand(Bernoulli(0.75)))) |&gt; tap((v) -&gt; println(&quot;New observation: &quot;, v))</code></pre><pre><code class="language-julia hljs">data_subscription = subscribe!(data_source |&gt; take(5), (data) -&gt; update!(y, data))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">New observation: 1.0
New posterior for θ: mean = 0.3463562558038911, std = 0.12808463111780527
New observation: 0.0
New posterior for θ: mean = 0.32125745714750537, std = 0.12138149313175396
New observation: 1.0
New posterior for θ: mean = 0.3671193244663678, std = 0.12126624240929154
New observation: 1.0
New posterior for θ: mean = 0.40717578722393166, std = 0.11986792524619594
New observation: 0.0
New posterior for θ: mean = 0.3829387213396894, std = 0.1152185142129341</code></pre><pre><code class="language-julia hljs"># It is important to unsubscribe from running observables to release computer resources
unsubscribe!(data_subscription)
unsubscribe!(θ_subscription)</code></pre><p>That was an example of exact Bayesian inference with Sum-Product (or Belief Propagation) algorithm. However, <code>ReactiveMP.jl</code> is not limited to only the sum-product algorithm but it also supports variational message passing with <a href="https://www.mdpi.com/1099-4300/23/7/807">Constrained Bethe Free Energy Minimisation</a>.</p><h2 id="user-guide-fundamentals-vmp-inference"><a class="docs-heading-anchor" href="#user-guide-fundamentals-vmp-inference">Variational inference in ReactiveMP.jl</a><a id="user-guide-fundamentals-vmp-inference-1"></a><a class="docs-heading-anchor-permalink" href="#user-guide-fundamentals-vmp-inference" title="Permalink"></a></h2><h3 id="Factorisation-constraints"><a class="docs-heading-anchor" href="#Factorisation-constraints">Factorisation constraints</a><a id="Factorisation-constraints-1"></a><a class="docs-heading-anchor-permalink" href="#Factorisation-constraints" title="Permalink"></a></h3><p>On a very high-level, ReactiveMP.jl is aimed to solve the Constrained Bethe Free Energy minimisation problem. For this task we approximate our exact posterior marginal distribution by some family of distributions <span>$q \in \mathcal{Q}$</span>. Often this involves assuming some factorization over <span>$q$</span>. For this purpose the <code>@model</code> macro supports optional <code>where { ... }</code> clauses for every <code>~</code> expression in a model specification.</p><pre><code class="language-julia hljs">@model function test_model6(n)
    τ ~ GammaShapeRate(1.0, 1.0)
    μ ~ NormalMeanVariance(0.0, 100.0)

    y = datavar(Float64, n)

    for i in 1:n
        # Here we assume a mean-field assumption on our
        # variational family of distributions locally for the current node
        y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) }
    end

    return μ, τ, y
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">test_model6 (generic function with 1 method)</code></pre><p>In this example we specified an extra constraints for <span>$q_a$</span> for Bethe factorisation:</p><p class="math-container">\[q(s) = \prod_{a \in \mathcal{V}} q_a(s_a) \prod_{i \in \mathcal{E}} q_i^{-1}(s_i)\]</p><p>There are several options to specify the mean-field factorisation constraint. </p><pre><code class="language-julia hljs">y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) } # With names from model specification
y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(out)q(mean)q(precision) } # With names from node specification
y[i] ~ NormalMeanPrecision(μ, τ) where { q = MeanField() } # With alias name</code></pre><p>It is also possible to use local structured factorisation:</p><pre><code class="language-julia hljs">y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i], μ)q(τ) } # With names from model specification
y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(out, mean)q(precision) } # With names from node specification</code></pre><p>As an option the <code>@model</code> macro accepts optional arguments for model specification, one of which is <code>default_factorisation</code> that accepts <code>MeanField()</code> as its argument for better convenience</p><pre><code class="language-julia hljs">@model [ default_factorisation = MeanField() ] function test_model(...)
    ...
end</code></pre><p>This will autatically impose a mean field factorization constraint over all marginal distributions in our model.</p><p>To run inference in this model we again need to create a synthetic dataset:</p><pre><code class="language-julia hljs">real_mean = -3.0
real_prec = 5.0
n         = 1000

dataset = rand(Normal(real_mean, inv(sqrt(real_prec))), n)</code></pre><pre><code class="language-julia hljs">model, (μ, τ, y) = test_model6(length(dataset))</code></pre><p>For variational inference we also usually need to set initial marginals for our inference procedure. For that purpose <code>ReactiveMP.jl</code> export the <code>setmarginal!</code> function:</p><pre><code class="language-julia hljs">setmarginal!(μ, vague(NormalMeanPrecision))
setmarginal!(τ, vague(GammaShapeRate))</code></pre><pre><code class="language-julia hljs">μ_values = keep(Marginal)
τ_values = keep(Marginal)

μ_subscription = subscribe!(getmarginal(μ), μ_values)
τ_subscription = subscribe!(getmarginal(τ), τ_values)

for i in 1:10
    update!(y, dataset)
end</code></pre><pre><code class="language-julia hljs">getvalues(μ_values)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10-element Vector{Marginal}:
 Marginal(NormalWeightedMeanPrecision{Float64}(xi=-3.0131627604260024e-9, w=0.010000001002000566))
 Marginal(NormalWeightedMeanPrecision{Float64}(xi=-27.581681272320832, w=9.182038430045631))
 Marginal(NormalWeightedMeanPrecision{Float64}(xi=-9686.723179749162, w=3221.2421065080966))
 Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14883.304740963575, w=4949.318263785573))
 Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14891.277397718337, w=4951.969498618706))
 Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14891.285358698933, w=4951.972145970727))
 Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14891.285366643979, w=4951.972148612796))
 Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14891.285366651906, w=4951.972148615468))
 Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14891.28536665192, w=4951.972148615468))
 Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14891.28536665191, w=4951.972148615468))</code></pre><pre><code class="language-julia hljs">getvalues(τ_values)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10-element Vector{Marginal}:
 Marginal(GammaShapeRate{Float64}(a=501.0, b=5.000000000046217e14))
 Marginal(GammaShapeRate{Float64}(a=501.0, b=54622.53607211518))
 Marginal(GammaShapeRate{Float64}(a=501.0, b=155.530549626581))
 Marginal(GammaShapeRate{Float64}(a=501.0, b=101.22626704540556))
 Marginal(GammaShapeRate{Float64}(a=501.0, b=101.1720714072366))
 Marginal(GammaShapeRate{Float64}(a=501.0, b=101.17201731997174))
 Marginal(GammaShapeRate{Float64}(a=501.0, b=101.17201726599272))
 Marginal(GammaShapeRate{Float64}(a=501.0, b=101.1720172659389))
 Marginal(GammaShapeRate{Float64}(a=501.0, b=101.1720172659388))
 Marginal(GammaShapeRate{Float64}(a=501.0, b=101.17201726593885))</code></pre><pre><code class="language-julia hljs">println(&quot;μ: mean = &quot;, mean(last(μ_values)), &quot;, std = &quot;, std(last(μ_values)))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">μ: mean = -3.0071423908988253, std = 0.014210550532122095</code></pre><pre><code class="language-julia hljs">println(&quot;τ: mean = &quot;, mean(last(τ_values)), &quot;, std = &quot;, std(last(τ_values)))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">τ: mean = 4.95196214861547, std = 0.22123735288151647</code></pre><h3 id="Form-constraints"><a class="docs-heading-anchor" href="#Form-constraints">Form constraints</a><a id="Form-constraints-1"></a><a class="docs-heading-anchor-permalink" href="#Form-constraints" title="Permalink"></a></h3><p>In order to support form constraints, the <code>randomvar()</code> function also supports a <code>where { ... }</code> clause with some optional arguments. One of these arguments is <code>form_constraint</code> that allows us to specify a form constraint to the random variables in our model. Another one is <code>prod_constraint</code> that allows to specify an additional constraints during computation of product of two colliding messages. For example we can perform the EM algorithm if we assign a point mass contraint on some variables in our model.</p><pre><code class="language-julia hljs">@model function test_model7(n)
    τ ~ GammaShapeRate(1.0, 1.0)

    # In case of form constraints `randomvar()` call is necessary
    μ = randomvar() where { form_constraint = PointMassFormConstraint() }
    μ ~ NormalMeanVariance(0.0, 100.0)

    y = datavar(Float64, n)

    for i in 1:n
        y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) }
    end

    return μ, τ, y
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">test_model7 (generic function with 1 method)</code></pre><p>In this example we specified an extra constraints for <span>$q_i$</span> for Bethe factorisation:</p><p class="math-container">\[q(s) = \prod_{a \in \mathcal{V}} q_a(s_a) \prod_{i \in \mathcal{E}} q_i^{-1}(s_i)\]</p><pre><code class="language-julia hljs">real_mean = -3.0
real_prec = 5.0
n         = 1000

dataset = rand(Normal(real_mean, inv(sqrt(real_prec))), n)</code></pre><pre><code class="language-julia hljs">model, (μ, τ, y) = test_model7(length(dataset))</code></pre><pre><code class="language-julia hljs">setmarginal!(μ, vague(NormalMeanPrecision))
setmarginal!(τ, PointMass(1.0))

μ_values = keep(Marginal)
τ_values = keep(Marginal)

μ_subscription = subscribe!(getmarginal(μ), μ_values)
τ_subscription = subscribe!(getmarginal(τ), τ_values)

for i in 1:10
    update!(y, dataset)
end</code></pre><pre><code class="language-julia hljs">getvalues(μ_values)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10-element Vector{Marginal}:
 Marginal(PointMass{Float64}(0.0))
 Marginal(PointMass{Float64}(-3.01616706508534))
 Marginal(PointMass{Float64}(-3.0164409562824446))
 Marginal(PointMass{Float64}(-3.0164409562847183))
 Marginal(PointMass{Float64}(-3.0164409562847134))
 Marginal(PointMass{Float64}(-3.016440956284716))
 Marginal(PointMass{Float64}(-3.0164409562847156))
 Marginal(PointMass{Float64}(-3.0164409562847134))
 Marginal(PointMass{Float64}(-3.016440956284716))
 Marginal(PointMass{Float64}(-3.0164409562847156))</code></pre><h3 id="Product-constraints"><a class="docs-heading-anchor" href="#Product-constraints">Product constraints</a><a id="Product-constraints-1"></a><a class="docs-heading-anchor-permalink" href="#Product-constraints" title="Permalink"></a></h3><p>By default <code>ReactiveMP.jl</code> tries to compute an analytical product of two colliding messages and throws an error if no analytical solution is known. However, it is possible to fall back to a generic product that does not require an analytical solution to be known. In this case the inference backend will simply propagate the product of two message in a form of a tuple. It is not possible to use such a tuple-product during an inference and in this case it is mandatory to use some form constraint to approximate this product.</p><pre><code class="language-julia hljs">μ = randomvar() where { 
    prod_constraint = ProdGeneric(),
    form_constraint = SampleListFormConstraint() 
}</code></pre><p>Sometimes it is useful to preserve a specific parametrisation of the resulting product later on in an inference procedure. <code>ReactiveMP.jl</code> exports a special <code>prod_constraint</code> called <code>ProdPreserveType</code> especially for that purpose:</p><pre><code class="language-julia hljs">μ = randomvar() where { prod_constraint = ProdPreserveType(NormalWeightedMeanPrecision) }</code></pre><h3 id="Free-Energy-Computation"><a class="docs-heading-anchor" href="#Free-Energy-Computation">Free Energy Computation</a><a id="Free-Energy-Computation-1"></a><a class="docs-heading-anchor-permalink" href="#Free-Energy-Computation" title="Permalink"></a></h3><p>During variational inference <code>ReactiveMP.jl</code> optimises a special functional called the Bethe Free Energy functional. It is possible to obtain its values for all VMP iterations with the <code>score</code> function.</p><pre><code class="language-julia hljs">model, (μ, τ, y) = test_model6(length(dataset))</code></pre><pre><code class="language-julia hljs">bfe_observable   = score(BetheFreeEnergy(), model)
bfe_subscription = subscribe!(bfe_observable, (fe) -&gt; println(&quot;Current BFE value: &quot;, fe))</code></pre><pre><code class="language-julia hljs"># Reset the model with vague marginals
setmarginal!(μ, vague(NormalMeanPrecision))
setmarginal!(τ, vague(GammaShapeRate))

for i in 1:10
    update!(y, dataset)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Current BFE value: 14763.268311193417
Current BFE value: 3276.000591850433
Current BFE value: 667.0358670479854
Current BFE value: 626.7376447739694
Current BFE value: 626.7375728723914
Current BFE value: 626.7375728723287
Current BFE value: 626.7375728723241
Current BFE value: 626.7375728723259
Current BFE value: 626.7375728723246
Current BFE value: 626.7375728723264</code></pre><pre><code class="language-julia hljs"># It always necessary to unsubscribe and release computer resources
unsubscribe!([ μ_subscription, τ_subscription, bfe_subscription ])</code></pre><h3 id="Meta-data-specification"><a class="docs-heading-anchor" href="#Meta-data-specification">Meta data specification</a><a id="Meta-data-specification-1"></a><a class="docs-heading-anchor-permalink" href="#Meta-data-specification" title="Permalink"></a></h3><p>During model specification some functional dependencies may accept an optional <code>meta</code> object in the <code>where { ... }</code> clause. The purpose of the <code>meta</code> object is to adjust, modify or supply some extra information to the inference backend during the computations of the messages. The <code>meta</code> object for example may contain an approximation method that needs to be used during various approximations or it may specify the tradeoff between accuracy and performance:</p><pre><code class="language-julia hljs"># In this example the `meta` object for the autoregressive `AR` node specifies the variate type of 
# the autoregressive process and its order. In addition it specifies that the message computation rules should
# respect accuracy over speed with the `ARsafe()` strategy. In contrast, `ARunsafe()` strategy tries to speedup computations
# by cost of possible numerical instabilities during an inference procedure
s[i] ~ AR(s[i - 1], θ, γ) where { q = q(s[i - 1], s[i])q(θ)q(γ), meta = ARMeta(Multivariate, order, ARsafe()) }
...
s[i] ~ AR(s[i - 1], θ, γ) where { q = q(s[i - 1], s[i])q(θ)q(γ), meta = ARMeta(Univariate, order, ARunsafe()) }</code></pre><p>Another example with <code>GaussianControlledVariance</code>, or simply <code>GCV</code> [see Hierarchical Gaussian Filter], node:</p><pre><code class="language-julia hljs"># In this example we specify structured factorisation and flag meta with `GaussHermiteCubature` 
# method with `21` sigma points for approximation of non-lineariety between hierarchy layers
xt ~ GCV(xt_min, zt, real_k, real_w) where { q = q(xt, xt_min)q(zt)q(κ)q(ω), meta = GCVMetadata(GaussHermiteCubature(21)) }</code></pre><p>The Meta object is useful to pass any extra information to a node that is not a random variable or constant model variable. It may include extra approximation methods, differentiation methods, optional non-linear functions, extra inference parameters etc.</p><h2 id="user-guide-fundamentals-custom-nodes-rules"><a class="docs-heading-anchor" href="#user-guide-fundamentals-custom-nodes-rules">Creating custom nodes and message computation rules</a><a id="user-guide-fundamentals-custom-nodes-rules-1"></a><a class="docs-heading-anchor-permalink" href="#user-guide-fundamentals-custom-nodes-rules" title="Permalink"></a></h2><h3 id="Custom-nodes"><a class="docs-heading-anchor" href="#Custom-nodes">Custom nodes</a><a id="Custom-nodes-1"></a><a class="docs-heading-anchor-permalink" href="#Custom-nodes" title="Permalink"></a></h3><p>To create a custom functional form and to make it available during model specification <code>ReactiveMP.jl</code> exports the <code>@node</code> macro:</p><pre><code class="language-julia hljs"># `@node` macro accepts a name of the functional form, its type, either `Stochastic` or `Deterministic` and an array of interfaces:
@node NormalMeanVariance Stochastic [ out, μ, v ]

# Interfaces may have aliases for their names that might be convenient for factorisation constraints specification
@node NormalMeanVariance Stochastic [ out, (μ, aliases = [ mean ]), (v, aliases = [ var ]) ]

# `NormalMeanVariance` structure declaration must exist, otherwise `@node` macro will throw an error
struct NormalMeanVariance end 

@node NormalMeanVariance Stochastic [ out, μ, v ]

# It is also possible to use function objects as a node functional form
function dot end

# Syntax for functions is a bit differet, as it is necesssary to use `typeof(...)` function for them 
# out = dot(x, a)
@node typeof(dot) Deterministic [ out, x, a ]</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Deterministic nodes do not support factorisation constraints with the <code>where { q = ... }</code> clause.</p></div></div><p>After that it is possible to use the newly created node during model specification:</p><pre><code class="language-julia hljs">@model function test_model()
    ...
    y ~ dot(x, a)
    ...
end</code></pre><h3 id="Custom-messages-computation-rules"><a class="docs-heading-anchor" href="#Custom-messages-computation-rules">Custom messages computation rules</a><a id="Custom-messages-computation-rules-1"></a><a class="docs-heading-anchor-permalink" href="#Custom-messages-computation-rules" title="Permalink"></a></h3><p><code>ReactiveMP.jl</code> exports the <code>@rule</code> macro to create custom message computation rules. For example let us create a simple <code>+</code> node to be available for usage in the model specification usage. We refer to <em>A Factor Graph Approach to Signal Modelling , System Identification and Filtering</em> [ Sascha Korl, 2005, page 32 ] for a rigorous explanation of the <code>+</code> node in factor graphs. According to Korl, assuming that inputs are Gaussian Sum-Product message computation rule for <code>+</code> node is the following:</p><p class="math-container">\[\mu_z = \mu_x + \mu_y \\
V_z = V_x + V_y\]</p><p>To specify this in <code>ReactiveMP.jl</code> we use the <code>@node</code> and <code>@rule</code> macros:</p><pre><code class="language-julia hljs">@node typeof(+) Deterministic  [ z, x, y ]

@rule typeof(+)(:z, Marginalisation) (m_x::UnivariateNormalDistributionsFamily, m_y::UnivariateNormalDistributionsFamily) = begin
    x_mean, x_var = mean_var(m_x)
    y_mean, y_var = mean_var(m_y)
    return NormalMeanVariance(x_mean + y_mean, x_var + y_var)
end</code></pre><p>In this example, for the <code>@rule</code> macro, we specify a type of our functional form: <code>typeof(+)</code>. Next, we specify an edge we are going to compute an outbound message for. <code>Marginalisation</code> indicates that the corresponding message respects the marginalisation constraint for posterior over corresponding edge:</p><p class="math-container">\[q(z) = \int q(z, x, y) \mathrm{d}x\mathrm{d}y\]</p><p>If we look on difference between sum-product rules and variational rules with mean-field assumption we notice that they require different local information to compute an outgoing message:</p><p class="math-container">\[\mu(z) = \int f(x, y, z)\mu(x)\mu(y)\mathrm{d}x\mathrm{d}y\]</p><p class="math-container">\[\nu(z) = \exp{ \int \log f(x, y, z)q(x)q(y)\mathrm{d}x\mathrm{d}y }\]</p><p>The <code>@rule</code> macro supports both cases with special prefixes during rule specification:</p><ul><li><code>m_</code> prefix corresponds to the incoming message on a specific edge</li><li><code>q_</code> prefix corresponds to the posterior marginal of a specific edge</li></ul><p>Example of a Sum-Product rule with <code>m_</code> messages used:</p><pre><code class="language-julia hljs">@rule NormalMeanPrecision(:μ, Marginalisation) (m_out::UnivariateNormalDistributionsFamily, m_τ::PointMass) = begin 
    m_out_mean, m_out_cov = mean_cov(m_out)
    return NormalMeanPrecision(m_out_mean, inv(m_out_cov + inv(mean(m_τ))))
end</code></pre><p>Example of a Variational rule with Mean-Field assumption with <code>q_</code> posteriors used:</p><pre><code class="language-julia hljs">@rule NormalMeanPrecision(:μ, Marginalisation) (q_out::Any, q_τ::Any) = begin 
    return NormalMeanPrecision(mean(q_out), mean(q_τ))
end</code></pre><p><code>ReactiveMP.jl</code> also supports structured rules. It is possible to obtain joint marginal over a set of edges:</p><pre><code class="language-julia hljs">@rule NormalMeanPrecision(:τ, Marginalisation) (q_out_μ::Any, ) = begin
    m, V = mean_cov(q_out_μ)
    θ = 2 / (V[1,1] - V[1,2] - V[2,1] + V[2,2] + abs2(m[1] - m[2]))
    α = convert(typeof(θ), 1.5)
    return Gamma(α, θ)
end</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>In the <code>@rule</code> specification the messages or marginals arguments <strong>must</strong> be in order with interfaces specification from <code>@node</code> macro:</p></div></div><pre><code class="language-julia hljs"># Inference backend expects arguments in `@rule` macro to be in the same order
@node NormalMeanPrecision Stochastic [ out, μ, τ ]</code></pre><p>Any rule always has access to the meta information with hidden the <code>meta::Any</code> variable:</p><pre><code class="language-julia hljs">@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any) = begin 
    ...
    println(meta)
    ...
end</code></pre><p>It is also possible to dispatch on a specific type of a meta object:</p><pre><code class="language-julia hljs">@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::LaplaceApproximation) = begin 
    ...
end</code></pre><p>or</p><pre><code class="language-julia hljs">@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::GaussHermiteCubature) = begin 
    ...
end</code></pre><h2 id="user-guide-fundamentals-pipeline"><a class="docs-heading-anchor" href="#user-guide-fundamentals-pipeline">Customizing messages computational pipeline</a><a id="user-guide-fundamentals-pipeline-1"></a><a class="docs-heading-anchor-permalink" href="#user-guide-fundamentals-pipeline" title="Permalink"></a></h2><p>In certain situations it might be convenient to customize the default message computational pipeline. <code>GrahpPPL.jl</code> supports the <code>pipeline</code> keyword in the <code>where { ... }</code> clause to add some extra steps after a message has been computed. A use case might be an extra approximation method to preserve conjugacy in the model, debugging or simple printing.</p><pre><code class="language-julia hljs"># Logs all outbound messages
y[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = LoggerPipelineStage() }
# Initialise messages to be vague
y[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = InitVaguePipelineStage() }
# In principle, it is possible to approximate outbound messages with Laplace Approximation
y[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = LaplaceApproximation() }</code></pre><p>Let us return to the coin toss model, but this time we want to print flowing messages:</p><pre><code class="language-julia hljs">@model function coin_toss_model_log(n)

    y = datavar(Float64, n)

    θ ~ Beta(2.0, 7.0) where { pipeline = LoggerPipelineStage(&quot;θ&quot;) }

    for i in 1:n
        y[i] ~ Bernoulli(θ)  where { pipeline = LoggerPipelineStage(&quot;y[$i]&quot;) }
    end

    return y, θ
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">coin_toss_model_log (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">_, (y, θ) = coin_toss_model_log(5)</code></pre><pre><code class="language-julia hljs">θ_subscription = subscribe!(getmarginal(θ), (value) -&gt; println(&quot;New posterior marginal for θ: &quot;, value))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">[θ][Beta][out]: Message(Beta{Float64}(α=2.0, β=7.0))</code></pre><pre><code class="language-julia hljs">coinflips = float.(rand(Bernoulli(0.5), 5))</code></pre><pre><code class="language-julia hljs">update!(y, coinflips)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">[y[1]][Bernoulli][p]: Message(Beta{Float64}(α=1.0, β=2.0))
[y[2]][Bernoulli][p]: Message(Beta{Float64}(α=2.0, β=1.0))
[y[3]][Bernoulli][p]: Message(Beta{Float64}(α=2.0, β=1.0))
[y[4]][Bernoulli][p]: Message(Beta{Float64}(α=1.0, β=2.0))
[y[5]][Bernoulli][p]: Message(Beta{Float64}(α=1.0, β=2.0))
New posterior marginal for θ: Marginal(Beta{Float64}(α=4.0, β=10.0))</code></pre><pre><code class="language-julia hljs">unsubscribe!(θ_subscription)</code></pre><pre><code class="language-julia hljs"># Inference is lazy and does not send messages if no one is listening for them
update!(y, coinflips)</code></pre><p>This tutorials covered the fundamentals of the ReactiveMP.jl package. For a more advanced usage we refer the interested reader to the other sections of the documentation.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../getting-started/">« Getting Started</a><a class="docs-footer-nextpage" href="../model-specification/">Model Specification »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.10 on <span class="colophon-date" title="Wednesday 27 October 2021 14:46">Wednesday 27 October 2021</span>. Using Julia version 1.6.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
